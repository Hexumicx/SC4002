{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3T3QwMxwPhv",
    "outputId": "84061c71-066a-45cf-ac64-2851f6ef9643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (0.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jjwx\\anaconda3\\envs\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install nltk\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6McSeWSrwI43"
   },
   "source": [
    "# Part 0. Data Prepraration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kc6eHQ8IwI45",
    "outputId": "b4da5896-49dc-45f9-d143-100dc35c6f0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset ['train']\n",
    "validation_dataset = dataset ['validation']\n",
    "test_dataset = dataset ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8530"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhTvsb7swI46"
   },
   "source": [
    "# Part 1. Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BfyWI60wI47"
   },
   "source": [
    "#### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffYmg5t5wI47",
    "outputId": "eedb6b74-1d27-4c90-b5cd-0184cfb0b70b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JJWX\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "vocab = set()\n",
    "for text in train_dataset['text']:\n",
    "    ls = nltk.word_tokenize(text)\n",
    "    for word in ls:\n",
    "        vocab.add(word)\n",
    "#Size of the vocabulary: 15812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEaufFA1wI47",
    "outputId": "9c58e5ef-a3c7-4103-a665-cb946d93d8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 18030\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OUr03AqwI48"
   },
   "source": [
    "#### (b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_MnWtslwI48",
    "outputId": "7633267e-8f95-48f0-87a1-987c734b35cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "for key in api.info()['models'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "7ZEvG2TawI48"
   },
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujb7pnArwI49",
    "outputId": "58e8d33a-d52b-4c0a-be6f-7962c8427b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 1867\n"
     ]
    }
   ],
   "source": [
    "oov_words = set()\n",
    "for word in vocab:\n",
    "    if word not in model:\n",
    "        oov_words.add(word)\n",
    "\n",
    "print(\"Number of OOV words:\", len(oov_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSy_BD90wI49"
   },
   "source": [
    "#### (c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove). Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you think is the best strategy to mitigate such limitation? Implement your solution in your source code. Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "bO_10iKqwI49"
   },
   "outputs": [],
   "source": [
    "# Group any words that are not in the model into a single token\n",
    "def wordtovec(word):\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ6rLQa-wI49"
   },
   "source": [
    "# Part 2. Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "aJZRcXbpwI49"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTMLPug0wI49"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(model.index_to_key) + 1\n",
    "embedding_dim = model.vector_size\n",
    "word_index = {word: index+1 for index, word in enumerate(model.index_to_key)} # index 0 is reserved for padding\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "biNlG_4VwI4-"
   },
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    if word in model:\n",
    "        embedding_matrix[idx] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTACwi_SwI4-"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, word_index):\n",
    "    ls = nltk.word_tokenize(text)\n",
    "    return [word_index[word] for word in ls if word in word_index]\n",
    "\n",
    "X_train = [tokenize(text, word_index) for text in train_dataset['text']]\n",
    "X_val = [tokenize(text, word_index) for text in validation_dataset['text']]\n",
    "X_test = [tokenize(text, word_index) for text in test_dataset['text']]\n",
    "max_length = max(len(seq) for seq in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "2deriXYKwI4-"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='pre', truncating='pre')\n",
    "X_val = pad_sequences(X_val, maxlen=max_length, padding='pre', truncating='pre')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia8WYob7z0qP"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_dataset['label'])\n",
    "y_val = np.array(validation_dataset['label'])\n",
    "y_test = np.array(test_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  1, ..., -1,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  0,  0,  0]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJT5G8794ZXC"
   },
   "source": [
    "Model Training - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "best_accuracy = {}\n",
    "class CustomCallback(Callback):\n",
    "    accuracy = 0\n",
    "    cur_key = \"\"\n",
    "    epochs = 0\n",
    "    optimizer = \"\"\n",
    "    batch_size = 0\n",
    "    best_model = None\n",
    "    lr = 0\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.accuracy = 0\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        global best_accuracy\n",
    "        if self.accuracy > best_accuracy.get(\"accuracy\", 0):\n",
    "            best_accuracy = {\n",
    "                \"accuracy\": self.accuracy,\n",
    "                \"epoch\": self.epochs,\n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"lr\": self.lr\n",
    "            }\n",
    "            print(\"Saved best accuracy for current run:\", self.accuracy, \"at epoch\", self.epochs)\n",
    "            self.best_model.save(filepath=\"best_model.keras\")\n",
    "        print(\"Run completed on:\")\n",
    "        print(self.cur_key)\n",
    "        print(\"Best accuracy for current run:\", self.accuracy, \"at epoch\", self.epochs)\n",
    "        print(\"Training ended\")\n",
    "\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_accuracy = logs['val_accuracy']\n",
    "        if val_accuracy > self.accuracy:\n",
    "            self.accuracy = val_accuracy\n",
    "            self.epochs = epoch\n",
    "            self.best_model = self.model\n",
    "\n",
    "    def set_key(self, optimizer, batch_size, lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.cur_key = f\"optimizer: {optimizer}, batch_size: {batch_size}, lr: {lr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "pvFnR88r6XZJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    # early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    #     monitor='val_accuracy',\n",
    "    #     patience=3,\n",
    "    #     restore_best_weights=True\n",
    "    # )\n",
    "    custom_callback = CustomCallback()\n",
    "    custom_callback.set_key(optimizer, batch_size, lr)\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False),  \n",
    "        SimpleRNN(32, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[custom_callback],\n",
    "        shuffle=True\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEFnPNmg7N-Z",
    "outputId": "2545a360-7231-482f-9598-81a616752eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"simple_rnn\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 62, 100, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m optimizer \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madagrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m         train_model(optimizer, \u001b[38;5;241m100\u001b[39m, batch_size, lr)\n",
      "Cell \u001b[1;32mIn[80], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(optimizer, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     28\u001b[0m     X_train, y_train,\n\u001b[0;32m     29\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m     30\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     32\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[custom_callback],\n\u001b[0;32m     33\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"simple_rnn\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 62, 100, 100)"
     ]
    }
   ],
   "source": [
    "for batch_size in [16, 32, 64, 128]:\n",
    "    for lr in [0.005, 0.01, 0.05, 0.1]:\n",
    "        for optimizer in ['adam', 'sgd', 'rmsprop', 'adagrad']:\n",
    "            train_model(optimizer, 100, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7016885280609131,\n",
       " 'epoch': 35,\n",
       " 'optimizer': 'adagrad',\n",
       " 'batch_size': 64,\n",
       " 'lr': 0.01}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model is trained with Optimizer: adagrad, Epoch: 35, Batch_size: 64, Learning_rate: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "cQjhQD_JwI4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m  2/134\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4062 - loss: 0.7197    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m train_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madagrad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[89], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(optimizer, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     28\u001b[0m     X_train, y_train,\n\u001b[0;32m     29\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[0;32m     30\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     32\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[custom_callback],\n\u001b[0;32m     33\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\JJWX\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 40, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "37gu7KI9_eWG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.5822\n",
      "Test accuracy: 0.501876175403595\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"best_model.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AveragePooling1D, GlobalAveragePooling1D\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_mean.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False),  # Embedding layer is frozen\n",
    "        SimpleRNN(16, return_sequences=True),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4691 - loss: 0.6987\n",
      "Epoch 1: val_accuracy improved from -inf to 0.47842, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.4691 - loss: 0.6987 - val_accuracy: 0.4784 - val_loss: 0.6947\n",
      "Epoch 2/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4769 - loss: 0.6949\n",
      "Epoch 2: val_accuracy improved from 0.47842 to 0.49719, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4770 - loss: 0.6949 - val_accuracy: 0.4972 - val_loss: 0.6932\n",
      "Epoch 3/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4916 - loss: 0.6935\n",
      "Epoch 3: val_accuracy improved from 0.49719 to 0.51501, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4920 - loss: 0.6935 - val_accuracy: 0.5150 - val_loss: 0.6921\n",
      "Epoch 4/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5066 - loss: 0.6924\n",
      "Epoch 4: val_accuracy improved from 0.51501 to 0.52439, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5069 - loss: 0.6924 - val_accuracy: 0.5244 - val_loss: 0.6911\n",
      "Epoch 5/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5197 - loss: 0.6914\n",
      "Epoch 5: val_accuracy improved from 0.52439 to 0.53283, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5197 - loss: 0.6914 - val_accuracy: 0.5328 - val_loss: 0.6902\n",
      "Epoch 6/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5328 - loss: 0.6905\n",
      "Epoch 6: val_accuracy improved from 0.53283 to 0.53752, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5328 - loss: 0.6905 - val_accuracy: 0.5375 - val_loss: 0.6892\n",
      "Epoch 7/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5489 - loss: 0.6895\n",
      "Epoch 7: val_accuracy improved from 0.53752 to 0.55816, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5490 - loss: 0.6895 - val_accuracy: 0.5582 - val_loss: 0.6882\n",
      "Epoch 8/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5591 - loss: 0.6885\n",
      "Epoch 8: val_accuracy improved from 0.55816 to 0.56754, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5597 - loss: 0.6885 - val_accuracy: 0.5675 - val_loss: 0.6871\n",
      "Epoch 9/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5716 - loss: 0.6874\n",
      "Epoch 9: val_accuracy improved from 0.56754 to 0.57505, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5719 - loss: 0.6874 - val_accuracy: 0.5750 - val_loss: 0.6859\n",
      "Epoch 10/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5770 - loss: 0.6862\n",
      "Epoch 10: val_accuracy improved from 0.57505 to 0.59381, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5780 - loss: 0.6861 - val_accuracy: 0.5938 - val_loss: 0.6843\n",
      "Epoch 11/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5898 - loss: 0.6845\n",
      "Epoch 11: val_accuracy did not improve from 0.59381\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5904 - loss: 0.6845 - val_accuracy: 0.5919 - val_loss: 0.6812\n",
      "Epoch 12/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5818 - loss: 0.6783\n",
      "Epoch 12: val_accuracy did not improve from 0.59381\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5820 - loss: 0.6781 - val_accuracy: 0.5938 - val_loss: 0.6653\n",
      "Epoch 13/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5866 - loss: 0.6651\n",
      "Epoch 13: val_accuracy did not improve from 0.59381\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5868 - loss: 0.6650 - val_accuracy: 0.5347 - val_loss: 0.6921\n",
      "Epoch 14/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6033 - loss: 0.6580\n",
      "Epoch 14: val_accuracy improved from 0.59381 to 0.62570, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6034 - loss: 0.6579 - val_accuracy: 0.6257 - val_loss: 0.6461\n",
      "Epoch 15/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6231 - loss: 0.6466\n",
      "Epoch 15: val_accuracy improved from 0.62570 to 0.62664, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6232 - loss: 0.6466 - val_accuracy: 0.6266 - val_loss: 0.6482\n",
      "Epoch 16/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6314 - loss: 0.6411\n",
      "Epoch 16: val_accuracy improved from 0.62664 to 0.63321, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6318 - loss: 0.6409 - val_accuracy: 0.6332 - val_loss: 0.6413\n",
      "Epoch 17/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6485 - loss: 0.6302\n",
      "Epoch 17: val_accuracy improved from 0.63321 to 0.63602, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6487 - loss: 0.6301 - val_accuracy: 0.6360 - val_loss: 0.6376\n",
      "Epoch 18/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6172 - loss: 0.6483\n",
      "Epoch 18: val_accuracy improved from 0.63602 to 0.64540, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6175 - loss: 0.6481 - val_accuracy: 0.6454 - val_loss: 0.6343\n",
      "Epoch 19/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6472 - loss: 0.6291\n",
      "Epoch 19: val_accuracy improved from 0.64540 to 0.66135, saving model to model_mean.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.6474 - loss: 0.6290 - val_accuracy: 0.6614 - val_loss: 0.6270\n",
      "Epoch 20/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6558 - loss: 0.6211\n",
      "Epoch 20: val_accuracy did not improve from 0.66135\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6562 - loss: 0.6209 - val_accuracy: 0.6520 - val_loss: 0.6280\n",
      "Epoch 21/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6531 - loss: 0.6252\n",
      "Epoch 21: val_accuracy did not improve from 0.66135\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6531 - loss: 0.6253 - val_accuracy: 0.6445 - val_loss: 0.6323\n",
      "Epoch 22/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6480 - loss: 0.6242\n",
      "Epoch 22: val_accuracy did not improve from 0.66135\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6482 - loss: 0.6241 - val_accuracy: 0.6210 - val_loss: 0.6615\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6120 - loss: 0.6263\n",
      "Test accuracy: 0.6454033851623535\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_mean.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPooling1D\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_max.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False),  # Embedding layer is frozen\n",
    "        SimpleRNN(16, return_sequences=True),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5012 - loss: 0.7078\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53189, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5016 - loss: 0.7073 - val_accuracy: 0.5319 - val_loss: 0.6873\n",
      "Epoch 2/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5218 - loss: 0.6920\n",
      "Epoch 2: val_accuracy improved from 0.53189 to 0.55347, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5222 - loss: 0.6919 - val_accuracy: 0.5535 - val_loss: 0.6841\n",
      "Epoch 3/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5456 - loss: 0.6887\n",
      "Epoch 3: val_accuracy improved from 0.55347 to 0.56754, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5457 - loss: 0.6887 - val_accuracy: 0.5675 - val_loss: 0.6809\n",
      "Epoch 4/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5539 - loss: 0.6849\n",
      "Epoch 4: val_accuracy improved from 0.56754 to 0.58630, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5540 - loss: 0.6849 - val_accuracy: 0.5863 - val_loss: 0.6772\n",
      "Epoch 5/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5696 - loss: 0.6798\n",
      "Epoch 5: val_accuracy improved from 0.58630 to 0.59381, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5697 - loss: 0.6798 - val_accuracy: 0.5938 - val_loss: 0.6732\n",
      "Epoch 6/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5923 - loss: 0.6732\n",
      "Epoch 6: val_accuracy improved from 0.59381 to 0.60413, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5925 - loss: 0.6731 - val_accuracy: 0.6041 - val_loss: 0.6679\n",
      "Epoch 7/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6122 - loss: 0.6657\n",
      "Epoch 7: val_accuracy improved from 0.60413 to 0.62289, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6126 - loss: 0.6656 - val_accuracy: 0.6229 - val_loss: 0.6611\n",
      "Epoch 8/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6257 - loss: 0.6572\n",
      "Epoch 8: val_accuracy improved from 0.62289 to 0.63039, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6259 - loss: 0.6571 - val_accuracy: 0.6304 - val_loss: 0.6533\n",
      "Epoch 9/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6392 - loss: 0.6485\n",
      "Epoch 9: val_accuracy improved from 0.63039 to 0.64353, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6393 - loss: 0.6485 - val_accuracy: 0.6435 - val_loss: 0.6452\n",
      "Epoch 10/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6572 - loss: 0.6397\n",
      "Epoch 10: val_accuracy improved from 0.64353 to 0.65291, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6574 - loss: 0.6395 - val_accuracy: 0.6529 - val_loss: 0.6372\n",
      "Epoch 11/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6706 - loss: 0.6303\n",
      "Epoch 11: val_accuracy improved from 0.65291 to 0.65947, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6708 - loss: 0.6301 - val_accuracy: 0.6595 - val_loss: 0.6294\n",
      "Epoch 12/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6818 - loss: 0.6204\n",
      "Epoch 12: val_accuracy did not improve from 0.65947\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6818 - loss: 0.6204 - val_accuracy: 0.6595 - val_loss: 0.6216\n",
      "Epoch 13/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6930 - loss: 0.6111\n",
      "Epoch 13: val_accuracy improved from 0.65947 to 0.67355, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6930 - loss: 0.6110 - val_accuracy: 0.6735 - val_loss: 0.6134\n",
      "Epoch 14/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7045 - loss: 0.6020\n",
      "Epoch 14: val_accuracy improved from 0.67355 to 0.67730, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7044 - loss: 0.6018 - val_accuracy: 0.6773 - val_loss: 0.6049\n",
      "Epoch 15/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7113 - loss: 0.5933\n",
      "Epoch 15: val_accuracy improved from 0.67730 to 0.68949, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7113 - loss: 0.5932 - val_accuracy: 0.6895 - val_loss: 0.5972\n",
      "Epoch 16/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7143 - loss: 0.5854\n",
      "Epoch 16: val_accuracy improved from 0.68949 to 0.70075, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7142 - loss: 0.5853 - val_accuracy: 0.7008 - val_loss: 0.5895\n",
      "Epoch 17/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7169 - loss: 0.5779\n",
      "Epoch 17: val_accuracy improved from 0.70075 to 0.70544, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7169 - loss: 0.5778 - val_accuracy: 0.7054 - val_loss: 0.5828\n",
      "Epoch 18/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7188 - loss: 0.5708\n",
      "Epoch 18: val_accuracy improved from 0.70544 to 0.71576, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7187 - loss: 0.5707 - val_accuracy: 0.7158 - val_loss: 0.5767\n",
      "Epoch 19/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7228 - loss: 0.5646\n",
      "Epoch 19: val_accuracy improved from 0.71576 to 0.71764, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7228 - loss: 0.5644 - val_accuracy: 0.7176 - val_loss: 0.5712\n",
      "Epoch 20/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7218 - loss: 0.5589\n",
      "Epoch 20: val_accuracy improved from 0.71764 to 0.72326, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7220 - loss: 0.5587 - val_accuracy: 0.7233 - val_loss: 0.5658\n",
      "Epoch 21/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7260 - loss: 0.5532\n",
      "Epoch 21: val_accuracy improved from 0.72326 to 0.72702, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7260 - loss: 0.5532 - val_accuracy: 0.7270 - val_loss: 0.5611\n",
      "Epoch 22/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7274 - loss: 0.5482\n",
      "Epoch 22: val_accuracy did not improve from 0.72702\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7275 - loss: 0.5481 - val_accuracy: 0.7251 - val_loss: 0.5566\n",
      "Epoch 23/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7301 - loss: 0.5434\n",
      "Epoch 23: val_accuracy did not improve from 0.72702\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7302 - loss: 0.5434 - val_accuracy: 0.7251 - val_loss: 0.5527\n",
      "Epoch 24/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7318 - loss: 0.5391\n",
      "Epoch 24: val_accuracy improved from 0.72702 to 0.73171, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7319 - loss: 0.5390 - val_accuracy: 0.7317 - val_loss: 0.5495\n",
      "Epoch 25/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7319 - loss: 0.5351\n",
      "Epoch 25: val_accuracy did not improve from 0.73171\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7319 - loss: 0.5350 - val_accuracy: 0.7289 - val_loss: 0.5457\n",
      "Epoch 26/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7299 - loss: 0.5370\n",
      "Epoch 26: val_accuracy did not improve from 0.73171\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7300 - loss: 0.5370 - val_accuracy: 0.7242 - val_loss: 0.5468\n",
      "Epoch 27/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7368 - loss: 0.5297\n",
      "Epoch 27: val_accuracy did not improve from 0.73171\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7368 - loss: 0.5296 - val_accuracy: 0.7308 - val_loss: 0.5436\n",
      "Epoch 28/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7397 - loss: 0.5263\n",
      "Epoch 28: val_accuracy improved from 0.73171 to 0.73640, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7397 - loss: 0.5263 - val_accuracy: 0.7364 - val_loss: 0.5415\n",
      "Epoch 29/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7449 - loss: 0.5235\n",
      "Epoch 29: val_accuracy did not improve from 0.73640\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7449 - loss: 0.5235 - val_accuracy: 0.7364 - val_loss: 0.5397\n",
      "Epoch 30/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7461 - loss: 0.5208\n",
      "Epoch 30: val_accuracy improved from 0.73640 to 0.73827, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7461 - loss: 0.5208 - val_accuracy: 0.7383 - val_loss: 0.5379\n",
      "Epoch 31/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7476 - loss: 0.5182\n",
      "Epoch 31: val_accuracy improved from 0.73827 to 0.74296, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7476 - loss: 0.5182 - val_accuracy: 0.7430 - val_loss: 0.5364\n",
      "Epoch 32/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7487 - loss: 0.5159\n",
      "Epoch 32: val_accuracy improved from 0.74296 to 0.74484, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7487 - loss: 0.5159 - val_accuracy: 0.7448 - val_loss: 0.5350\n",
      "Epoch 33/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7501 - loss: 0.5138\n",
      "Epoch 33: val_accuracy improved from 0.74484 to 0.74672, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7501 - loss: 0.5138 - val_accuracy: 0.7467 - val_loss: 0.5334\n",
      "Epoch 34/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7507 - loss: 0.5117\n",
      "Epoch 34: val_accuracy did not improve from 0.74672\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7507 - loss: 0.5117 - val_accuracy: 0.7448 - val_loss: 0.5320\n",
      "Epoch 35/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7518 - loss: 0.5096\n",
      "Epoch 35: val_accuracy did not improve from 0.74672\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7518 - loss: 0.5096 - val_accuracy: 0.7467 - val_loss: 0.5308\n",
      "Epoch 36/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7527 - loss: 0.5085\n",
      "Epoch 36: val_accuracy did not improve from 0.74672\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7522 - loss: 0.5089 - val_accuracy: 0.7308 - val_loss: 0.5446\n",
      "Epoch 37/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7473 - loss: 0.5135\n",
      "Epoch 37: val_accuracy did not improve from 0.74672\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7473 - loss: 0.5134 - val_accuracy: 0.7439 - val_loss: 0.5286\n",
      "Epoch 38/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7558 - loss: 0.5060\n",
      "Epoch 38: val_accuracy improved from 0.74672 to 0.75047, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7558 - loss: 0.5059 - val_accuracy: 0.7505 - val_loss: 0.5276\n",
      "Epoch 39/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7598 - loss: 0.5039\n",
      "Epoch 39: val_accuracy improved from 0.75047 to 0.75328, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7598 - loss: 0.5038 - val_accuracy: 0.7533 - val_loss: 0.5267\n",
      "Epoch 40/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7616 - loss: 0.5022\n",
      "Epoch 40: val_accuracy did not improve from 0.75328\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7615 - loss: 0.5022 - val_accuracy: 0.7533 - val_loss: 0.5261\n",
      "Epoch 41/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7620 - loss: 0.5007\n",
      "Epoch 41: val_accuracy improved from 0.75328 to 0.75516, saving model to model_max.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7620 - loss: 0.5007 - val_accuracy: 0.7552 - val_loss: 0.5253\n",
      "Epoch 42/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7630 - loss: 0.4993\n",
      "Epoch 42: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7630 - loss: 0.4993 - val_accuracy: 0.7542 - val_loss: 0.5248\n",
      "Epoch 43/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7637 - loss: 0.4979\n",
      "Epoch 43: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7637 - loss: 0.4979 - val_accuracy: 0.7533 - val_loss: 0.5243\n",
      "Epoch 44/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7655 - loss: 0.4966\n",
      "Epoch 44: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7655 - loss: 0.4966 - val_accuracy: 0.7523 - val_loss: 0.5236\n",
      "Epoch 45/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7664 - loss: 0.4953\n",
      "Epoch 45: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7664 - loss: 0.4953 - val_accuracy: 0.7495 - val_loss: 0.5230\n",
      "Epoch 46/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7674 - loss: 0.4941\n",
      "Epoch 46: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7674 - loss: 0.4941 - val_accuracy: 0.7495 - val_loss: 0.5223\n",
      "Epoch 47/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7676 - loss: 0.4929\n",
      "Epoch 47: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7676 - loss: 0.4929 - val_accuracy: 0.7505 - val_loss: 0.5217\n",
      "Epoch 48/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7682 - loss: 0.4918\n",
      "Epoch 48: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7681 - loss: 0.4917 - val_accuracy: 0.7495 - val_loss: 0.5211\n",
      "Epoch 49/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7681 - loss: 0.4906\n",
      "Epoch 49: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7680 - loss: 0.4906 - val_accuracy: 0.7514 - val_loss: 0.5206\n",
      "Epoch 50/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7682 - loss: 0.4895\n",
      "Epoch 50: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7681 - loss: 0.4895 - val_accuracy: 0.7514 - val_loss: 0.5203\n",
      "Epoch 51/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7674 - loss: 0.4885\n",
      "Epoch 51: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7674 - loss: 0.4885 - val_accuracy: 0.7505 - val_loss: 0.5198\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7259 - loss: 0.5401\n",
      "Test accuracy: 0.7354596853256226\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_max.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_dense.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=True),  # Embedding layer is frozen\n",
    "        SimpleRNN(16, return_sequences=True),\n",
    "        Flatten(),\n",
    "        Dense(62, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.5358 - loss: 0.6983\n",
      "Epoch 1: val_accuracy improved from -inf to 0.58349, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 180ms/step - accuracy: 0.5360 - loss: 0.6983 - val_accuracy: 0.5835 - val_loss: 0.6724\n",
      "Epoch 2/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.6117 - loss: 0.6588\n",
      "Epoch 2: val_accuracy improved from 0.58349 to 0.65009, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 208ms/step - accuracy: 0.6118 - loss: 0.6587 - val_accuracy: 0.6501 - val_loss: 0.6379\n",
      "Epoch 3/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6471 - loss: 0.6261\n",
      "Epoch 3: val_accuracy improved from 0.65009 to 0.66886, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.6472 - loss: 0.6261 - val_accuracy: 0.6689 - val_loss: 0.6154\n",
      "Epoch 4/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6783 - loss: 0.6012\n",
      "Epoch 4: val_accuracy improved from 0.66886 to 0.68105, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.6784 - loss: 0.6012 - val_accuracy: 0.6811 - val_loss: 0.6013\n",
      "Epoch 5/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6932 - loss: 0.5815\n",
      "Epoch 5: val_accuracy did not improve from 0.68105\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 141ms/step - accuracy: 0.6932 - loss: 0.5815 - val_accuracy: 0.6679 - val_loss: 0.6042\n",
      "Epoch 6/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7085 - loss: 0.5701\n",
      "Epoch 6: val_accuracy improved from 0.68105 to 0.70169, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 146ms/step - accuracy: 0.7085 - loss: 0.5701 - val_accuracy: 0.7017 - val_loss: 0.5795\n",
      "Epoch 7/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7123 - loss: 0.5531\n",
      "Epoch 7: val_accuracy improved from 0.70169 to 0.70356, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.7123 - loss: 0.5531 - val_accuracy: 0.7036 - val_loss: 0.5668\n",
      "Epoch 8/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7223 - loss: 0.5431\n",
      "Epoch 8: val_accuracy improved from 0.70356 to 0.70919, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 150ms/step - accuracy: 0.7223 - loss: 0.5431 - val_accuracy: 0.7092 - val_loss: 0.5741\n",
      "Epoch 9/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7300 - loss: 0.5374\n",
      "Epoch 9: val_accuracy did not improve from 0.70919\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7301 - loss: 0.5374 - val_accuracy: 0.7092 - val_loss: 0.5589\n",
      "Epoch 10/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.7358 - loss: 0.5226\n",
      "Epoch 10: val_accuracy did not improve from 0.70919\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 148ms/step - accuracy: 0.7358 - loss: 0.5226 - val_accuracy: 0.7092 - val_loss: 0.5577\n",
      "Epoch 11/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7415 - loss: 0.5143\n",
      "Epoch 11: val_accuracy did not improve from 0.70919\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7415 - loss: 0.5143 - val_accuracy: 0.6942 - val_loss: 0.5795\n",
      "Epoch 12/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7323 - loss: 0.5266\n",
      "Epoch 12: val_accuracy improved from 0.70919 to 0.71201, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 146ms/step - accuracy: 0.7323 - loss: 0.5265 - val_accuracy: 0.7120 - val_loss: 0.5521\n",
      "Epoch 13/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7517 - loss: 0.5015\n",
      "Epoch 13: val_accuracy improved from 0.71201 to 0.71388, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.7517 - loss: 0.5015 - val_accuracy: 0.7139 - val_loss: 0.5438\n",
      "Epoch 14/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7579 - loss: 0.4936\n",
      "Epoch 14: val_accuracy did not improve from 0.71388\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7579 - loss: 0.4936 - val_accuracy: 0.7092 - val_loss: 0.5431\n",
      "Epoch 15/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7597 - loss: 0.4865\n",
      "Epoch 15: val_accuracy did not improve from 0.71388\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 160ms/step - accuracy: 0.7597 - loss: 0.4865 - val_accuracy: 0.7017 - val_loss: 0.5527\n",
      "Epoch 16/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7576 - loss: 0.4870\n",
      "Epoch 16: val_accuracy did not improve from 0.71388\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 154ms/step - accuracy: 0.7577 - loss: 0.4870 - val_accuracy: 0.7120 - val_loss: 0.5484\n",
      "Epoch 17/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7725 - loss: 0.4772\n",
      "Epoch 17: val_accuracy improved from 0.71388 to 0.71670, saving model to model_dense.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 160ms/step - accuracy: 0.7724 - loss: 0.4772 - val_accuracy: 0.7167 - val_loss: 0.5436\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7367 - loss: 0.5282\n",
      "Test accuracy: 0.7373358607292175\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_dense.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk92r_ZsrSpi"
   },
   "source": [
    "# Part 3. Enhancement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KqKwEpbsrdS"
   },
   "source": [
    "#### 1. Instead of keeping the word embeddings fixed, now update the word embeddings (the same way as model parameters) during the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K0Do_y_onlw"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=True),\n",
    "    SimpleRNN(16, return_sequences=False),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXX18PkzurVx"
   },
   "source": [
    "#### 2. As discussed in Question 1(c), apply your solution in mitigating the influence of OOV words and train your model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "QqnGXOHhvkMX"
   },
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "vocab_size = len(model.index_to_key) + 2 # 0 is reserved for padding, 1 is reserved for OOV\n",
    "embedding_dim = model.vector_size\n",
    "word_index = {word: index+2 for index, word in enumerate(model.index_to_key)} # index 0 is reserved for padding\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "QF-3ABACvkMZ"
   },
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    if word in model:\n",
    "        embedding_matrix[idx] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "43whGTSCvkMZ"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, word_index):\n",
    "    ls = nltk.word_tokenize(text)\n",
    "    return [word_index[word] if word in word_index else 1 for word in ls]\n",
    "\n",
    "X_train = [tokenize(text, word_index) for text in train_dataset['text']]\n",
    "X_val = [tokenize(text, word_index) for text in validation_dataset['text']]\n",
    "X_test = [tokenize(text, word_index) for text in test_dataset['text']]\n",
    "max_length = max(len(seq) for seq in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "tdWGgBiavkMZ"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_val = pad_sequences(X_val, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "yUXkRq0gvkMa"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_dataset['label'])\n",
    "y_val = np.array(validation_dataset['label'])\n",
    "y_test = np.array(test_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_oov.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=False),  # Embedding layer is frozen\n",
    "        SimpleRNN(16, return_sequences=False),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5133 - loss: 0.6945\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52439, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.5132 - loss: 0.6945 - val_accuracy: 0.5244 - val_loss: 0.6933\n",
      "Epoch 2/100\n",
      "\u001b[1m126/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5323 - loss: 0.6905\n",
      "Epoch 2: val_accuracy did not improve from 0.52439\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5322 - loss: 0.6906 - val_accuracy: 0.5206 - val_loss: 0.6940\n",
      "Epoch 3/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5591 - loss: 0.6857\n",
      "Epoch 3: val_accuracy improved from 0.52439 to 0.53377, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5589 - loss: 0.6858 - val_accuracy: 0.5338 - val_loss: 0.6915\n",
      "Epoch 4/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5766 - loss: 0.6794\n",
      "Epoch 4: val_accuracy improved from 0.53377 to 0.55253, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5764 - loss: 0.6794 - val_accuracy: 0.5525 - val_loss: 0.6824\n",
      "Epoch 5/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5938 - loss: 0.6719\n",
      "Epoch 5: val_accuracy improved from 0.55253 to 0.58818, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5934 - loss: 0.6720 - val_accuracy: 0.5882 - val_loss: 0.6698\n",
      "Epoch 6/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6113 - loss: 0.6640\n",
      "Epoch 6: val_accuracy improved from 0.58818 to 0.60694, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6109 - loss: 0.6641 - val_accuracy: 0.6069 - val_loss: 0.6609\n",
      "Epoch 7/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6183 - loss: 0.6566\n",
      "Epoch 7: val_accuracy improved from 0.60694 to 0.60882, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6182 - loss: 0.6566 - val_accuracy: 0.6088 - val_loss: 0.6562\n",
      "Epoch 8/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6291 - loss: 0.6482\n",
      "Epoch 8: val_accuracy improved from 0.60882 to 0.61726, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6291 - loss: 0.6482 - val_accuracy: 0.6173 - val_loss: 0.6514\n",
      "Epoch 9/100\n",
      "\u001b[1m126/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6422 - loss: 0.6411\n",
      "Epoch 9: val_accuracy improved from 0.61726 to 0.62383, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6417 - loss: 0.6414 - val_accuracy: 0.6238 - val_loss: 0.6479\n",
      "Epoch 10/100\n",
      "\u001b[1m133/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6507 - loss: 0.6354\n",
      "Epoch 10: val_accuracy did not improve from 0.62383\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6506 - loss: 0.6355 - val_accuracy: 0.6210 - val_loss: 0.6461\n",
      "Epoch 11/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6563 - loss: 0.6312\n",
      "Epoch 11: val_accuracy improved from 0.62383 to 0.63790, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6557 - loss: 0.6315 - val_accuracy: 0.6379 - val_loss: 0.6402\n",
      "Epoch 12/100\n",
      "\u001b[1m128/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6669 - loss: 0.6242\n",
      "Epoch 12: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6664 - loss: 0.6244 - val_accuracy: 0.6295 - val_loss: 0.6460\n",
      "Epoch 13/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6681 - loss: 0.6201\n",
      "Epoch 13: val_accuracy improved from 0.63790 to 0.63977, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6680 - loss: 0.6201 - val_accuracy: 0.6398 - val_loss: 0.6427\n",
      "Epoch 14/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6757 - loss: 0.6158\n",
      "Epoch 14: val_accuracy improved from 0.63977 to 0.65197, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6750 - loss: 0.6160 - val_accuracy: 0.6520 - val_loss: 0.6400\n",
      "Epoch 15/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6806 - loss: 0.6089\n",
      "Epoch 15: val_accuracy improved from 0.65197 to 0.65291, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6803 - loss: 0.6090 - val_accuracy: 0.6529 - val_loss: 0.6430\n",
      "Epoch 16/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6820 - loss: 0.6047\n",
      "Epoch 16: val_accuracy improved from 0.65291 to 0.65666, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6814 - loss: 0.6049 - val_accuracy: 0.6567 - val_loss: 0.6440\n",
      "Epoch 17/100\n",
      "\u001b[1m123/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6911 - loss: 0.6031\n",
      "Epoch 17: val_accuracy improved from 0.65666 to 0.65947, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6901 - loss: 0.6035 - val_accuracy: 0.6595 - val_loss: 0.6381\n",
      "Epoch 18/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6894 - loss: 0.5999\n",
      "Epoch 18: val_accuracy improved from 0.65947 to 0.66229, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6890 - loss: 0.6002 - val_accuracy: 0.6623 - val_loss: 0.6203\n",
      "Epoch 19/100\n",
      "\u001b[1m126/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6959 - loss: 0.5931\n",
      "Epoch 19: val_accuracy improved from 0.66229 to 0.66510, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.6952 - loss: 0.5936 - val_accuracy: 0.6651 - val_loss: 0.6311\n",
      "Epoch 20/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6984 - loss: 0.5930\n",
      "Epoch 20: val_accuracy did not improve from 0.66510\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6981 - loss: 0.5932 - val_accuracy: 0.6642 - val_loss: 0.6184\n",
      "Epoch 21/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6975 - loss: 0.5905\n",
      "Epoch 21: val_accuracy improved from 0.66510 to 0.67448, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6969 - loss: 0.5910 - val_accuracy: 0.6745 - val_loss: 0.6217\n",
      "Epoch 22/100\n",
      "\u001b[1m126/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7046 - loss: 0.5868\n",
      "Epoch 22: val_accuracy improved from 0.67448 to 0.67730, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7037 - loss: 0.5873 - val_accuracy: 0.6773 - val_loss: 0.6204\n",
      "Epoch 23/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7010 - loss: 0.5847\n",
      "Epoch 23: val_accuracy did not improve from 0.67730\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7004 - loss: 0.5852 - val_accuracy: 0.6642 - val_loss: 0.6130\n",
      "Epoch 24/100\n",
      "\u001b[1m124/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7090 - loss: 0.5809\n",
      "Epoch 24: val_accuracy did not improve from 0.67730\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7080 - loss: 0.5814 - val_accuracy: 0.6754 - val_loss: 0.6204\n",
      "Epoch 25/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7126 - loss: 0.5800\n",
      "Epoch 25: val_accuracy improved from 0.67730 to 0.67824, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7116 - loss: 0.5806 - val_accuracy: 0.6782 - val_loss: 0.6156\n",
      "Epoch 26/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7158 - loss: 0.5750\n",
      "Epoch 26: val_accuracy did not improve from 0.67824\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7152 - loss: 0.5754 - val_accuracy: 0.6679 - val_loss: 0.6088\n",
      "Epoch 27/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7152 - loss: 0.5718\n",
      "Epoch 27: val_accuracy improved from 0.67824 to 0.68293, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7148 - loss: 0.5722 - val_accuracy: 0.6829 - val_loss: 0.6085\n",
      "Epoch 28/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7172 - loss: 0.5685\n",
      "Epoch 28: val_accuracy did not improve from 0.68293\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7169 - loss: 0.5688 - val_accuracy: 0.6764 - val_loss: 0.6108\n",
      "Epoch 29/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7165 - loss: 0.5701\n",
      "Epoch 29: val_accuracy did not improve from 0.68293\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7160 - loss: 0.5705 - val_accuracy: 0.6811 - val_loss: 0.6106\n",
      "Epoch 30/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7190 - loss: 0.5685\n",
      "Epoch 30: val_accuracy did not improve from 0.68293\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7185 - loss: 0.5689 - val_accuracy: 0.6820 - val_loss: 0.6030\n",
      "Epoch 31/100\n",
      "\u001b[1m132/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7182 - loss: 0.5660\n",
      "Epoch 31: val_accuracy did not improve from 0.68293\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7179 - loss: 0.5662 - val_accuracy: 0.6811 - val_loss: 0.6029\n",
      "Epoch 32/100\n",
      "\u001b[1m130/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7201 - loss: 0.5618\n",
      "Epoch 32: val_accuracy did not improve from 0.68293\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7197 - loss: 0.5623 - val_accuracy: 0.6829 - val_loss: 0.6012\n",
      "Epoch 33/100\n",
      "\u001b[1m129/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7202 - loss: 0.5610\n",
      "Epoch 33: val_accuracy improved from 0.68293 to 0.68574, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7197 - loss: 0.5615 - val_accuracy: 0.6857 - val_loss: 0.6001\n",
      "Epoch 34/100\n",
      "\u001b[1m131/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7171 - loss: 0.5628\n",
      "Epoch 34: val_accuracy improved from 0.68574 to 0.68762, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7167 - loss: 0.5631 - val_accuracy: 0.6876 - val_loss: 0.5970\n",
      "Epoch 35/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7255 - loss: 0.5604\n",
      "Epoch 35: val_accuracy improved from 0.68762 to 0.69137, saving model to model_oov.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7254 - loss: 0.5605 - val_accuracy: 0.6914 - val_loss: 0.6025\n",
      "Epoch 36/100\n",
      "\u001b[1m125/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7249 - loss: 0.5557\n",
      "Epoch 36: val_accuracy did not improve from 0.69137\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7241 - loss: 0.5565 - val_accuracy: 0.6914 - val_loss: 0.5990\n",
      "Epoch 37/100\n",
      "\u001b[1m127/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7255 - loss: 0.5535\n",
      "Epoch 37: val_accuracy did not improve from 0.69137\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7249 - loss: 0.5542 - val_accuracy: 0.6904 - val_loss: 0.6028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Sequential name=sequential_1, built=True>,\n",
       " <keras.src.callbacks.history.History at 0x1d5b9a1ec90>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6786 - loss: 0.5732\n",
      "Test accuracy: 0.6763602495193481\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_oov.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a biLSTM model and a biGRU model, incorporating recurrent computations in both directions and stacking multiple layers if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_combined.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=True),  # Embedding layer is frozen\n",
    "        Bidirectional(LSTM(16, return_sequences=True)),\n",
    "        Bidirectional(LSTM(16, return_sequences=True)),\n",
    "        Bidirectional(LSTM(16, return_sequences=False)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.5006 - loss: 0.6935\n",
      "Epoch 1: val_accuracy improved from -inf to 0.58443, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 202ms/step - accuracy: 0.5006 - loss: 0.6935 - val_accuracy: 0.5844 - val_loss: 0.6909\n",
      "Epoch 2/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.5479 - loss: 0.6905\n",
      "Epoch 2: val_accuracy improved from 0.58443 to 0.60038, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.5479 - loss: 0.6905 - val_accuracy: 0.6004 - val_loss: 0.6876\n",
      "Epoch 3/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.5880 - loss: 0.6862\n",
      "Epoch 3: val_accuracy improved from 0.60038 to 0.61632, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.5880 - loss: 0.6862 - val_accuracy: 0.6163 - val_loss: 0.6805\n",
      "Epoch 4/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.6149 - loss: 0.6756\n",
      "Epoch 4: val_accuracy did not improve from 0.61632\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.6149 - loss: 0.6756 - val_accuracy: 0.6154 - val_loss: 0.6669\n",
      "Epoch 5/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6357 - loss: 0.6518\n",
      "Epoch 5: val_accuracy improved from 0.61632 to 0.62101, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.6357 - loss: 0.6517 - val_accuracy: 0.6210 - val_loss: 0.6515\n",
      "Epoch 6/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.6672 - loss: 0.6234\n",
      "Epoch 6: val_accuracy improved from 0.62101 to 0.63790, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.6672 - loss: 0.6234 - val_accuracy: 0.6379 - val_loss: 0.6416\n",
      "Epoch 7/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6888 - loss: 0.6003\n",
      "Epoch 7: val_accuracy improved from 0.63790 to 0.65478, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.6888 - loss: 0.6003 - val_accuracy: 0.6548 - val_loss: 0.6303\n",
      "Epoch 8/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.7067 - loss: 0.5825\n",
      "Epoch 8: val_accuracy improved from 0.65478 to 0.65760, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.7067 - loss: 0.5825 - val_accuracy: 0.6576 - val_loss: 0.6221\n",
      "Epoch 9/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7140 - loss: 0.5678\n",
      "Epoch 9: val_accuracy improved from 0.65760 to 0.66886, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7140 - loss: 0.5678 - val_accuracy: 0.6689 - val_loss: 0.6127\n",
      "Epoch 10/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.7229 - loss: 0.5550\n",
      "Epoch 10: val_accuracy improved from 0.66886 to 0.67636, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.7229 - loss: 0.5550 - val_accuracy: 0.6764 - val_loss: 0.6017\n",
      "Epoch 11/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7308 - loss: 0.5439\n",
      "Epoch 11: val_accuracy improved from 0.67636 to 0.68574, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7308 - loss: 0.5439 - val_accuracy: 0.6857 - val_loss: 0.5911\n",
      "Epoch 12/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.7386 - loss: 0.5343\n",
      "Epoch 12: val_accuracy improved from 0.68574 to 0.69794, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 193ms/step - accuracy: 0.7386 - loss: 0.5343 - val_accuracy: 0.6979 - val_loss: 0.5811\n",
      "Epoch 13/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.7440 - loss: 0.5260\n",
      "Epoch 13: val_accuracy improved from 0.69794 to 0.69887, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7441 - loss: 0.5261 - val_accuracy: 0.6989 - val_loss: 0.5715\n",
      "Epoch 14/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.7512 - loss: 0.5187\n",
      "Epoch 14: val_accuracy improved from 0.69887 to 0.70263, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 190ms/step - accuracy: 0.7512 - loss: 0.5187 - val_accuracy: 0.7026 - val_loss: 0.5623\n",
      "Epoch 15/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7545 - loss: 0.5121\n",
      "Epoch 15: val_accuracy improved from 0.70263 to 0.70919, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 197ms/step - accuracy: 0.7545 - loss: 0.5121 - val_accuracy: 0.7092 - val_loss: 0.5539\n",
      "Epoch 16/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7582 - loss: 0.5060\n",
      "Epoch 16: val_accuracy improved from 0.70919 to 0.71576, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.7582 - loss: 0.5060 - val_accuracy: 0.7158 - val_loss: 0.5464\n",
      "Epoch 17/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7619 - loss: 0.5004\n",
      "Epoch 17: val_accuracy improved from 0.71576 to 0.71670, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.7619 - loss: 0.5005 - val_accuracy: 0.7167 - val_loss: 0.5398\n",
      "Epoch 18/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7658 - loss: 0.4953\n",
      "Epoch 18: val_accuracy improved from 0.71670 to 0.71857, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.7658 - loss: 0.4953 - val_accuracy: 0.7186 - val_loss: 0.5342\n",
      "Epoch 19/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.7678 - loss: 0.4905\n",
      "Epoch 19: val_accuracy improved from 0.71857 to 0.72889, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 193ms/step - accuracy: 0.7678 - loss: 0.4905 - val_accuracy: 0.7289 - val_loss: 0.5293\n",
      "Epoch 20/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7680 - loss: 0.4860\n",
      "Epoch 20: val_accuracy improved from 0.72889 to 0.73546, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 195ms/step - accuracy: 0.7680 - loss: 0.4860 - val_accuracy: 0.7355 - val_loss: 0.5252\n",
      "Epoch 21/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.7707 - loss: 0.4817\n",
      "Epoch 21: val_accuracy improved from 0.73546 to 0.74015, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 214ms/step - accuracy: 0.7707 - loss: 0.4818 - val_accuracy: 0.7402 - val_loss: 0.5217\n",
      "Epoch 22/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.7735 - loss: 0.4777\n",
      "Epoch 22: val_accuracy did not improve from 0.74015\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.7735 - loss: 0.4777 - val_accuracy: 0.7392 - val_loss: 0.5187\n",
      "Epoch 23/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7756 - loss: 0.4738\n",
      "Epoch 23: val_accuracy did not improve from 0.74015\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.7755 - loss: 0.4738 - val_accuracy: 0.7392 - val_loss: 0.5161\n",
      "Epoch 24/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.7773 - loss: 0.4701\n",
      "Epoch 24: val_accuracy improved from 0.74015 to 0.74203, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 199ms/step - accuracy: 0.7773 - loss: 0.4701 - val_accuracy: 0.7420 - val_loss: 0.5139\n",
      "Epoch 25/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7807 - loss: 0.4664\n",
      "Epoch 25: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.7807 - loss: 0.4665 - val_accuracy: 0.7420 - val_loss: 0.5120\n",
      "Epoch 26/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.7835 - loss: 0.4629\n",
      "Epoch 26: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.7835 - loss: 0.4629 - val_accuracy: 0.7411 - val_loss: 0.5104\n",
      "Epoch 27/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7872 - loss: 0.4594\n",
      "Epoch 27: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 202ms/step - accuracy: 0.7871 - loss: 0.4594 - val_accuracy: 0.7411 - val_loss: 0.5090\n",
      "Epoch 28/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.7898 - loss: 0.4560\n",
      "Epoch 28: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.7897 - loss: 0.4560 - val_accuracy: 0.7411 - val_loss: 0.5079\n",
      "Epoch 29/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.7921 - loss: 0.4527\n",
      "Epoch 29: val_accuracy improved from 0.74203 to 0.74484, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 200ms/step - accuracy: 0.7921 - loss: 0.4527 - val_accuracy: 0.7448 - val_loss: 0.5070\n",
      "Epoch 30/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.7949 - loss: 0.4494\n",
      "Epoch 30: val_accuracy improved from 0.74484 to 0.74672, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 189ms/step - accuracy: 0.7949 - loss: 0.4494 - val_accuracy: 0.7467 - val_loss: 0.5063\n",
      "Epoch 31/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.7983 - loss: 0.4462\n",
      "Epoch 31: val_accuracy improved from 0.74672 to 0.74765, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7982 - loss: 0.4462 - val_accuracy: 0.7477 - val_loss: 0.5058\n",
      "Epoch 32/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.7990 - loss: 0.4431\n",
      "Epoch 32: val_accuracy did not improve from 0.74765\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.7990 - loss: 0.4431 - val_accuracy: 0.7477 - val_loss: 0.5055\n",
      "Epoch 33/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8010 - loss: 0.4400\n",
      "Epoch 33: val_accuracy improved from 0.74765 to 0.74953, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 195ms/step - accuracy: 0.8010 - loss: 0.4401 - val_accuracy: 0.7495 - val_loss: 0.5053\n",
      "Epoch 34/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8036 - loss: 0.4370\n",
      "Epoch 34: val_accuracy did not improve from 0.74953\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.8036 - loss: 0.4370 - val_accuracy: 0.7467 - val_loss: 0.5051\n",
      "Epoch 35/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8066 - loss: 0.4340\n",
      "Epoch 35: val_accuracy did not improve from 0.74953\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.8065 - loss: 0.4340 - val_accuracy: 0.7486 - val_loss: 0.5051\n",
      "Epoch 36/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8086 - loss: 0.4310\n",
      "Epoch 36: val_accuracy did not improve from 0.74953\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8086 - loss: 0.4310 - val_accuracy: 0.7495 - val_loss: 0.5050\n",
      "Epoch 37/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8086 - loss: 0.4280\n",
      "Epoch 37: val_accuracy improved from 0.74953 to 0.75141, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 195ms/step - accuracy: 0.8085 - loss: 0.4280 - val_accuracy: 0.7514 - val_loss: 0.5050\n",
      "Epoch 38/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8117 - loss: 0.4250\n",
      "Epoch 38: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8116 - loss: 0.4251 - val_accuracy: 0.7514 - val_loss: 0.5051\n",
      "Epoch 39/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8134 - loss: 0.4220\n",
      "Epoch 39: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.8134 - loss: 0.4221 - val_accuracy: 0.7505 - val_loss: 0.5051\n",
      "Epoch 40/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8149 - loss: 0.4191\n",
      "Epoch 40: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.8149 - loss: 0.4191 - val_accuracy: 0.7514 - val_loss: 0.5051\n",
      "Epoch 41/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8161 - loss: 0.4161\n",
      "Epoch 41: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.8160 - loss: 0.4161 - val_accuracy: 0.7495 - val_loss: 0.5051\n",
      "Epoch 42/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8191 - loss: 0.4131\n",
      "Epoch 42: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8191 - loss: 0.4131 - val_accuracy: 0.7495 - val_loss: 0.5051\n",
      "Epoch 43/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8196 - loss: 0.4101\n",
      "Epoch 43: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8196 - loss: 0.4102 - val_accuracy: 0.7486 - val_loss: 0.5051\n",
      "Epoch 44/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8235 - loss: 0.4072\n",
      "Epoch 44: val_accuracy did not improve from 0.75141\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8234 - loss: 0.4072 - val_accuracy: 0.7505 - val_loss: 0.5052\n",
      "Epoch 45/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8251 - loss: 0.4043\n",
      "Epoch 45: val_accuracy improved from 0.75141 to 0.75422, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.8250 - loss: 0.4043 - val_accuracy: 0.7542 - val_loss: 0.5053\n",
      "Epoch 46/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8263 - loss: 0.4013\n",
      "Epoch 46: val_accuracy improved from 0.75422 to 0.75516, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.8262 - loss: 0.4013 - val_accuracy: 0.7552 - val_loss: 0.5055\n",
      "Epoch 47/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8276 - loss: 0.3982\n",
      "Epoch 47: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 221ms/step - accuracy: 0.8276 - loss: 0.3983 - val_accuracy: 0.7542 - val_loss: 0.5060\n",
      "Epoch 48/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.8285 - loss: 0.3952\n",
      "Epoch 48: val_accuracy did not improve from 0.75516\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 244ms/step - accuracy: 0.8284 - loss: 0.3952 - val_accuracy: 0.7552 - val_loss: 0.5069\n",
      "Epoch 49/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8304 - loss: 0.3922\n",
      "Epoch 49: val_accuracy improved from 0.75516 to 0.75610, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 209ms/step - accuracy: 0.8304 - loss: 0.3922 - val_accuracy: 0.7561 - val_loss: 0.5079\n",
      "Epoch 50/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.8318 - loss: 0.3893\n",
      "Epoch 50: val_accuracy improved from 0.75610 to 0.75704, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 207ms/step - accuracy: 0.8318 - loss: 0.3893 - val_accuracy: 0.7570 - val_loss: 0.5091\n",
      "Epoch 51/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8335 - loss: 0.3864\n",
      "Epoch 51: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.8335 - loss: 0.3864 - val_accuracy: 0.7552 - val_loss: 0.5105\n",
      "Epoch 52/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8355 - loss: 0.3835\n",
      "Epoch 52: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 181ms/step - accuracy: 0.8355 - loss: 0.3836 - val_accuracy: 0.7570 - val_loss: 0.5123\n",
      "Epoch 53/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8372 - loss: 0.3805\n",
      "Epoch 53: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.8371 - loss: 0.3806 - val_accuracy: 0.7570 - val_loss: 0.5136\n",
      "Epoch 54/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.8383 - loss: 0.3776\n",
      "Epoch 54: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 177ms/step - accuracy: 0.8382 - loss: 0.3776 - val_accuracy: 0.7552 - val_loss: 0.5155\n",
      "Epoch 55/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8414 - loss: 0.3750\n",
      "Epoch 55: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 190ms/step - accuracy: 0.8414 - loss: 0.3751 - val_accuracy: 0.7552 - val_loss: 0.5161\n",
      "Epoch 56/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8411 - loss: 0.3718\n",
      "Epoch 56: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 194ms/step - accuracy: 0.8410 - loss: 0.3718 - val_accuracy: 0.7533 - val_loss: 0.5193\n",
      "Epoch 57/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.8453 - loss: 0.3693\n",
      "Epoch 57: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.8453 - loss: 0.3694 - val_accuracy: 0.7570 - val_loss: 0.5185\n",
      "Epoch 58/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8457 - loss: 0.3661\n",
      "Epoch 58: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.8457 - loss: 0.3661 - val_accuracy: 0.7552 - val_loss: 0.5213\n",
      "Epoch 59/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8498 - loss: 0.3629\n",
      "Epoch 59: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.8498 - loss: 0.3630 - val_accuracy: 0.7542 - val_loss: 0.5223\n",
      "Epoch 60/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8512 - loss: 0.3601\n",
      "Epoch 60: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.8511 - loss: 0.3602 - val_accuracy: 0.7495 - val_loss: 0.5225\n",
      "Epoch 61/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8545 - loss: 0.3563\n",
      "Epoch 61: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 176ms/step - accuracy: 0.8545 - loss: 0.3564 - val_accuracy: 0.7523 - val_loss: 0.5225\n",
      "Epoch 62/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8545 - loss: 0.3546\n",
      "Epoch 62: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 204ms/step - accuracy: 0.8545 - loss: 0.3546 - val_accuracy: 0.7486 - val_loss: 0.5241\n",
      "Epoch 63/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8562 - loss: 0.3526\n",
      "Epoch 63: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.8562 - loss: 0.3526 - val_accuracy: 0.7467 - val_loss: 0.5251\n",
      "Epoch 64/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8600 - loss: 0.3530\n",
      "Epoch 64: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.8600 - loss: 0.3530 - val_accuracy: 0.7477 - val_loss: 0.5249\n",
      "Epoch 65/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8590 - loss: 0.3467\n",
      "Epoch 65: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.8589 - loss: 0.3468 - val_accuracy: 0.7430 - val_loss: 0.5232\n",
      "Epoch 66/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8618 - loss: 0.3459\n",
      "Epoch 66: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8618 - loss: 0.3460 - val_accuracy: 0.7552 - val_loss: 0.5247\n",
      "Epoch 67/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8633 - loss: 0.3401\n",
      "Epoch 67: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.8633 - loss: 0.3401 - val_accuracy: 0.7552 - val_loss: 0.5282\n",
      "Epoch 68/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8650 - loss: 0.3402\n",
      "Epoch 68: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.8649 - loss: 0.3402 - val_accuracy: 0.7486 - val_loss: 0.5282\n",
      "Epoch 69/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8652 - loss: 0.3366\n",
      "Epoch 69: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.8651 - loss: 0.3367 - val_accuracy: 0.7486 - val_loss: 0.5288\n",
      "Epoch 70/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.8692 - loss: 0.3319\n",
      "Epoch 70: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.8692 - loss: 0.3320 - val_accuracy: 0.7486 - val_loss: 0.5312\n",
      "Epoch 71/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8633 - loss: 0.3387\n",
      "Epoch 71: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8633 - loss: 0.3388 - val_accuracy: 0.7477 - val_loss: 0.5345\n",
      "Epoch 72/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8661 - loss: 0.3334\n",
      "Epoch 72: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 181ms/step - accuracy: 0.8661 - loss: 0.3335 - val_accuracy: 0.7477 - val_loss: 0.5310\n",
      "Epoch 73/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8702 - loss: 0.3305\n",
      "Epoch 73: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8701 - loss: 0.3305 - val_accuracy: 0.7542 - val_loss: 0.5363\n",
      "Epoch 74/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8697 - loss: 0.3303\n",
      "Epoch 74: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8697 - loss: 0.3303 - val_accuracy: 0.7505 - val_loss: 0.5369\n",
      "Epoch 75/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8767 - loss: 0.3216\n",
      "Epoch 75: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8767 - loss: 0.3216 - val_accuracy: 0.7523 - val_loss: 0.5394\n",
      "Epoch 76/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8788 - loss: 0.3165\n",
      "Epoch 76: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.8788 - loss: 0.3166 - val_accuracy: 0.7523 - val_loss: 0.5442\n",
      "Epoch 77/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8697 - loss: 0.3292\n",
      "Epoch 77: val_accuracy improved from 0.75704 to 0.75797, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 185ms/step - accuracy: 0.8697 - loss: 0.3292 - val_accuracy: 0.7580 - val_loss: 0.5431\n",
      "Epoch 78/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8844 - loss: 0.3103\n",
      "Epoch 78: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 190ms/step - accuracy: 0.8843 - loss: 0.3103 - val_accuracy: 0.7561 - val_loss: 0.5418\n",
      "Epoch 79/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8828 - loss: 0.3104\n",
      "Epoch 79: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.8828 - loss: 0.3104 - val_accuracy: 0.7523 - val_loss: 0.5501\n",
      "Epoch 80/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8866 - loss: 0.3058\n",
      "Epoch 80: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8866 - loss: 0.3059 - val_accuracy: 0.7486 - val_loss: 0.5578\n",
      "Epoch 81/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8867 - loss: 0.3023\n",
      "Epoch 81: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8866 - loss: 0.3024 - val_accuracy: 0.7561 - val_loss: 0.5493\n",
      "Epoch 82/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8856 - loss: 0.3059\n",
      "Epoch 82: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.8856 - loss: 0.3059 - val_accuracy: 0.7542 - val_loss: 0.5591\n",
      "Epoch 83/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8888 - loss: 0.2997\n",
      "Epoch 83: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8887 - loss: 0.2998 - val_accuracy: 0.7448 - val_loss: 0.5535\n",
      "Epoch 84/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8874 - loss: 0.3033\n",
      "Epoch 84: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8873 - loss: 0.3034 - val_accuracy: 0.7542 - val_loss: 0.5583\n",
      "Epoch 85/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8919 - loss: 0.2961\n",
      "Epoch 85: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8919 - loss: 0.2961 - val_accuracy: 0.7542 - val_loss: 0.5648\n",
      "Epoch 86/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8949 - loss: 0.2903\n",
      "Epoch 86: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 183ms/step - accuracy: 0.8948 - loss: 0.2903 - val_accuracy: 0.7477 - val_loss: 0.5666\n",
      "Epoch 87/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.8905 - loss: 0.2996\n",
      "Epoch 87: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 266ms/step - accuracy: 0.8905 - loss: 0.2996 - val_accuracy: 0.7505 - val_loss: 0.5643\n",
      "Epoch 88/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8855 - loss: 0.3034\n",
      "Epoch 88: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.8855 - loss: 0.3034 - val_accuracy: 0.7561 - val_loss: 0.5760\n",
      "Epoch 89/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8957 - loss: 0.2874\n",
      "Epoch 89: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.8956 - loss: 0.2875 - val_accuracy: 0.7467 - val_loss: 0.5710\n",
      "Epoch 90/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8932 - loss: 0.2920\n",
      "Epoch 90: val_accuracy did not improve from 0.75797\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8932 - loss: 0.2920 - val_accuracy: 0.7561 - val_loss: 0.5749\n",
      "Epoch 91/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8859 - loss: 0.3041\n",
      "Epoch 91: val_accuracy improved from 0.75797 to 0.75891, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 176ms/step - accuracy: 0.8859 - loss: 0.3041 - val_accuracy: 0.7589 - val_loss: 0.5830\n",
      "Epoch 92/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8739 - loss: 0.3187\n",
      "Epoch 92: val_accuracy improved from 0.75891 to 0.76173, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.8740 - loss: 0.3186 - val_accuracy: 0.7617 - val_loss: 0.5845\n",
      "Epoch 93/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9019 - loss: 0.2772\n",
      "Epoch 93: val_accuracy did not improve from 0.76173\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.9018 - loss: 0.2773 - val_accuracy: 0.7561 - val_loss: 0.5743\n",
      "Epoch 94/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9000 - loss: 0.2793\n",
      "Epoch 94: val_accuracy did not improve from 0.76173\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.9000 - loss: 0.2793 - val_accuracy: 0.7608 - val_loss: 0.5930\n",
      "Epoch 95/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.9070 - loss: 0.2697\n",
      "Epoch 95: val_accuracy did not improve from 0.76173\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.9069 - loss: 0.2698 - val_accuracy: 0.7617 - val_loss: 0.5892\n",
      "Epoch 96/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9069 - loss: 0.2672\n",
      "Epoch 96: val_accuracy did not improve from 0.76173\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.9069 - loss: 0.2672 - val_accuracy: 0.7589 - val_loss: 0.5949\n",
      "Epoch 97/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9001 - loss: 0.2817\n",
      "Epoch 97: val_accuracy did not improve from 0.76173\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8999 - loss: 0.2819 - val_accuracy: 0.7580 - val_loss: 0.5537\n",
      "Epoch 98/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8942 - loss: 0.2918\n",
      "Epoch 98: val_accuracy improved from 0.76173 to 0.76266, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8942 - loss: 0.2917 - val_accuracy: 0.7627 - val_loss: 0.5956\n",
      "Epoch 99/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9047 - loss: 0.2718\n",
      "Epoch 99: val_accuracy improved from 0.76266 to 0.76454, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.9047 - loss: 0.2718 - val_accuracy: 0.7645 - val_loss: 0.5895\n",
      "Epoch 100/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9092 - loss: 0.2642\n",
      "Epoch 100: val_accuracy improved from 0.76454 to 0.76735, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.9092 - loss: 0.2642 - val_accuracy: 0.7674 - val_loss: 0.5979\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_11                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,976</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_12                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_13                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │    \u001b[38;5;34m40,000,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_11                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m14,976\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_12                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_13                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m6,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,055,508</span> (305.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,055,508\u001b[0m (305.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,027,753</span> (152.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,027,753\u001b[0m (152.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,027,755</span> (152.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m40,027,755\u001b[0m (152.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6798 - loss: 0.5810\n",
      "Test accuracy: 0.7429643273353577\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_combined.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, GRU\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_combined.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=True),  # Embedding layer is frozen\n",
    "        Bidirectional(GRU(16, return_sequences=False)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5007 - loss: 0.7018\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50750, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 176ms/step - accuracy: 0.5007 - loss: 0.7017 - val_accuracy: 0.5075 - val_loss: 0.6927\n",
      "Epoch 2/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5439 - loss: 0.6876\n",
      "Epoch 2: val_accuracy improved from 0.50750 to 0.55159, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.5440 - loss: 0.6876 - val_accuracy: 0.5516 - val_loss: 0.6857\n",
      "Epoch 3/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.5801 - loss: 0.6800\n",
      "Epoch 3: val_accuracy improved from 0.55159 to 0.55910, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.5801 - loss: 0.6800 - val_accuracy: 0.5591 - val_loss: 0.6803\n",
      "Epoch 4/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.5972 - loss: 0.6738\n",
      "Epoch 4: val_accuracy improved from 0.55910 to 0.57317, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.5971 - loss: 0.6738 - val_accuracy: 0.5732 - val_loss: 0.6753\n",
      "Epoch 5/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.6081 - loss: 0.6678\n",
      "Epoch 5: val_accuracy improved from 0.57317 to 0.57786, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.6081 - loss: 0.6678 - val_accuracy: 0.5779 - val_loss: 0.6699\n",
      "Epoch 6/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.6204 - loss: 0.6615\n",
      "Epoch 6: val_accuracy improved from 0.57786 to 0.59099, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.6204 - loss: 0.6615 - val_accuracy: 0.5910 - val_loss: 0.6640\n",
      "Epoch 7/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.6267 - loss: 0.6543\n",
      "Epoch 7: val_accuracy improved from 0.59099 to 0.61257, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.6266 - loss: 0.6543 - val_accuracy: 0.6126 - val_loss: 0.6571\n",
      "Epoch 8/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6416 - loss: 0.6461\n",
      "Epoch 8: val_accuracy improved from 0.61257 to 0.61820, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - accuracy: 0.6416 - loss: 0.6461 - val_accuracy: 0.6182 - val_loss: 0.6491\n",
      "Epoch 9/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.6579 - loss: 0.6363\n",
      "Epoch 9: val_accuracy improved from 0.61820 to 0.63415, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.6578 - loss: 0.6363 - val_accuracy: 0.6341 - val_loss: 0.6397\n",
      "Epoch 10/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.6688 - loss: 0.6248\n",
      "Epoch 10: val_accuracy improved from 0.63415 to 0.65197, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.6688 - loss: 0.6247 - val_accuracy: 0.6520 - val_loss: 0.6289\n",
      "Epoch 11/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6781 - loss: 0.6111\n",
      "Epoch 11: val_accuracy improved from 0.65197 to 0.67261, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.6781 - loss: 0.6111 - val_accuracy: 0.6726 - val_loss: 0.6167\n",
      "Epoch 12/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.6844 - loss: 0.5950\n",
      "Epoch 12: val_accuracy improved from 0.67261 to 0.68574, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.6844 - loss: 0.5950 - val_accuracy: 0.6857 - val_loss: 0.6034\n",
      "Epoch 13/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.6999 - loss: 0.5768\n",
      "Epoch 13: val_accuracy improved from 0.68574 to 0.68668, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 154ms/step - accuracy: 0.6999 - loss: 0.5767 - val_accuracy: 0.6867 - val_loss: 0.5903\n",
      "Epoch 14/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7117 - loss: 0.5579\n",
      "Epoch 14: val_accuracy improved from 0.68668 to 0.69231, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.7117 - loss: 0.5579 - val_accuracy: 0.6923 - val_loss: 0.5825\n",
      "Epoch 15/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7240 - loss: 0.5433\n",
      "Epoch 15: val_accuracy improved from 0.69231 to 0.69794, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 211ms/step - accuracy: 0.7240 - loss: 0.5434 - val_accuracy: 0.6979 - val_loss: 0.5798\n",
      "Epoch 16/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.7324 - loss: 0.5348\n",
      "Epoch 16: val_accuracy improved from 0.69794 to 0.70356, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 207ms/step - accuracy: 0.7324 - loss: 0.5348 - val_accuracy: 0.7036 - val_loss: 0.5729\n",
      "Epoch 17/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7374 - loss: 0.5284\n",
      "Epoch 17: val_accuracy improved from 0.70356 to 0.70544, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 225ms/step - accuracy: 0.7374 - loss: 0.5284 - val_accuracy: 0.7054 - val_loss: 0.5654\n",
      "Epoch 18/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.7419 - loss: 0.5229\n",
      "Epoch 18: val_accuracy improved from 0.70544 to 0.71201, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 206ms/step - accuracy: 0.7419 - loss: 0.5229 - val_accuracy: 0.7120 - val_loss: 0.5587\n",
      "Epoch 19/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.7449 - loss: 0.5179\n",
      "Epoch 19: val_accuracy did not improve from 0.71201\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 213ms/step - accuracy: 0.7448 - loss: 0.5179 - val_accuracy: 0.7120 - val_loss: 0.5526\n",
      "Epoch 20/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7516 - loss: 0.5134\n",
      "Epoch 20: val_accuracy improved from 0.71201 to 0.71951, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 201ms/step - accuracy: 0.7516 - loss: 0.5134 - val_accuracy: 0.7195 - val_loss: 0.5471\n",
      "Epoch 21/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.7555 - loss: 0.5092\n",
      "Epoch 21: val_accuracy improved from 0.71951 to 0.72045, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 199ms/step - accuracy: 0.7554 - loss: 0.5092 - val_accuracy: 0.7205 - val_loss: 0.5421\n",
      "Epoch 22/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7570 - loss: 0.5053\n",
      "Epoch 22: val_accuracy improved from 0.72045 to 0.72983, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 202ms/step - accuracy: 0.7569 - loss: 0.5053 - val_accuracy: 0.7298 - val_loss: 0.5376\n",
      "Epoch 23/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.7612 - loss: 0.5016\n",
      "Epoch 23: val_accuracy improved from 0.72983 to 0.73358, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 223ms/step - accuracy: 0.7611 - loss: 0.5016 - val_accuracy: 0.7336 - val_loss: 0.5336\n",
      "Epoch 24/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7637 - loss: 0.4981\n",
      "Epoch 24: val_accuracy improved from 0.73358 to 0.73921, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 227ms/step - accuracy: 0.7637 - loss: 0.4981 - val_accuracy: 0.7392 - val_loss: 0.5299\n",
      "Epoch 25/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7647 - loss: 0.4948\n",
      "Epoch 25: val_accuracy did not improve from 0.73921\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 209ms/step - accuracy: 0.7646 - loss: 0.4948 - val_accuracy: 0.7392 - val_loss: 0.5265\n",
      "Epoch 26/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7662 - loss: 0.4916\n",
      "Epoch 26: val_accuracy improved from 0.73921 to 0.74390, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 220ms/step - accuracy: 0.7661 - loss: 0.4916 - val_accuracy: 0.7439 - val_loss: 0.5235\n",
      "Epoch 27/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7666 - loss: 0.4885\n",
      "Epoch 27: val_accuracy improved from 0.74390 to 0.74765, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 223ms/step - accuracy: 0.7665 - loss: 0.4886 - val_accuracy: 0.7477 - val_loss: 0.5208\n",
      "Epoch 28/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7676 - loss: 0.4856\n",
      "Epoch 28: val_accuracy improved from 0.74765 to 0.74859, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 226ms/step - accuracy: 0.7675 - loss: 0.4856 - val_accuracy: 0.7486 - val_loss: 0.5183\n",
      "Epoch 29/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.7693 - loss: 0.4827\n",
      "Epoch 29: val_accuracy did not improve from 0.74859\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 216ms/step - accuracy: 0.7692 - loss: 0.4828 - val_accuracy: 0.7486 - val_loss: 0.5160\n",
      "Epoch 30/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.7712 - loss: 0.4800\n",
      "Epoch 30: val_accuracy did not improve from 0.74859\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 212ms/step - accuracy: 0.7712 - loss: 0.4800 - val_accuracy: 0.7486 - val_loss: 0.5140\n",
      "Epoch 31/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7740 - loss: 0.4773\n",
      "Epoch 31: val_accuracy improved from 0.74859 to 0.74953, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 205ms/step - accuracy: 0.7740 - loss: 0.4773 - val_accuracy: 0.7495 - val_loss: 0.5121\n",
      "Epoch 32/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7740 - loss: 0.4747\n",
      "Epoch 32: val_accuracy improved from 0.74953 to 0.75141, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 215ms/step - accuracy: 0.7739 - loss: 0.4748 - val_accuracy: 0.7514 - val_loss: 0.5104\n",
      "Epoch 33/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7751 - loss: 0.4722\n",
      "Epoch 33: val_accuracy improved from 0.75141 to 0.75422, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 212ms/step - accuracy: 0.7751 - loss: 0.4722 - val_accuracy: 0.7542 - val_loss: 0.5088\n",
      "Epoch 34/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7766 - loss: 0.4697\n",
      "Epoch 34: val_accuracy improved from 0.75422 to 0.75704, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 220ms/step - accuracy: 0.7766 - loss: 0.4698 - val_accuracy: 0.7570 - val_loss: 0.5074\n",
      "Epoch 35/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7781 - loss: 0.4673\n",
      "Epoch 35: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 209ms/step - accuracy: 0.7781 - loss: 0.4673 - val_accuracy: 0.7542 - val_loss: 0.5060\n",
      "Epoch 36/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7808 - loss: 0.4649\n",
      "Epoch 36: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 206ms/step - accuracy: 0.7808 - loss: 0.4650 - val_accuracy: 0.7523 - val_loss: 0.5048\n",
      "Epoch 37/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7808 - loss: 0.4626\n",
      "Epoch 37: val_accuracy did not improve from 0.75704\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 207ms/step - accuracy: 0.7808 - loss: 0.4627 - val_accuracy: 0.7514 - val_loss: 0.5037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Sequential name=sequential_4, built=True>,\n",
       " <keras.src.callbacks.history.History at 0x1d5d1d63690>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7120 - loss: 0.5646\n",
      "Test accuracy: 0.7532833218574524\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_combined.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Keeping the above two adjustments, replace your simple RNN model in Part 2 with a Convolutional Neural Network (CNN) to produce sentence representations and perform sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution1D, Flatten\n",
    "\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_combined.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embedding_dim,\n",
    "                  weights=[embedding_matrix],\n",
    "                  trainable=True),  # Embedding layer is trainable\n",
    "        Convolution1D(16, kernel_size=3, activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, early_stopping]\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.4938 - loss: 0.7037\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53846, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 133ms/step - accuracy: 0.4939 - loss: 0.7036 - val_accuracy: 0.5385 - val_loss: 0.6903\n",
      "Epoch 2/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.5456 - loss: 0.6871\n",
      "Epoch 2: val_accuracy improved from 0.53846 to 0.58161, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.5457 - loss: 0.6870 - val_accuracy: 0.5816 - val_loss: 0.6789\n",
      "Epoch 3/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.5965 - loss: 0.6690\n",
      "Epoch 3: val_accuracy improved from 0.58161 to 0.60694, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.5966 - loss: 0.6690 - val_accuracy: 0.6069 - val_loss: 0.6650\n",
      "Epoch 4/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6333 - loss: 0.6457\n",
      "Epoch 4: val_accuracy improved from 0.60694 to 0.62946, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 138ms/step - accuracy: 0.6334 - loss: 0.6456 - val_accuracy: 0.6295 - val_loss: 0.6523\n",
      "Epoch 5/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.6642 - loss: 0.6217\n",
      "Epoch 5: val_accuracy improved from 0.62946 to 0.64540, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.6642 - loss: 0.6217 - val_accuracy: 0.6454 - val_loss: 0.6421\n",
      "Epoch 6/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.6931 - loss: 0.5992\n",
      "Epoch 6: val_accuracy improved from 0.64540 to 0.65854, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 190ms/step - accuracy: 0.6931 - loss: 0.5991 - val_accuracy: 0.6585 - val_loss: 0.6341\n",
      "Epoch 7/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7078 - loss: 0.5809\n",
      "Epoch 7: val_accuracy improved from 0.65854 to 0.67167, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 221ms/step - accuracy: 0.7078 - loss: 0.5809 - val_accuracy: 0.6717 - val_loss: 0.6266\n",
      "Epoch 8/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.7210 - loss: 0.5665\n",
      "Epoch 8: val_accuracy improved from 0.67167 to 0.67261, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.7210 - loss: 0.5665 - val_accuracy: 0.6726 - val_loss: 0.6178\n",
      "Epoch 9/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.7289 - loss: 0.5550\n",
      "Epoch 9: val_accuracy improved from 0.67261 to 0.67730, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.7288 - loss: 0.5550 - val_accuracy: 0.6773 - val_loss: 0.6100\n",
      "Epoch 10/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7341 - loss: 0.5457\n",
      "Epoch 10: val_accuracy improved from 0.67730 to 0.68199, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.7341 - loss: 0.5457 - val_accuracy: 0.6820 - val_loss: 0.6029\n",
      "Epoch 11/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7388 - loss: 0.5377\n",
      "Epoch 11: val_accuracy improved from 0.68199 to 0.69043, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 151ms/step - accuracy: 0.7388 - loss: 0.5377 - val_accuracy: 0.6904 - val_loss: 0.5969\n",
      "Epoch 12/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7410 - loss: 0.5306\n",
      "Epoch 12: val_accuracy improved from 0.69043 to 0.69794, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.7410 - loss: 0.5306 - val_accuracy: 0.6979 - val_loss: 0.5916\n",
      "Epoch 13/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7447 - loss: 0.5244\n",
      "Epoch 13: val_accuracy improved from 0.69794 to 0.70263, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 150ms/step - accuracy: 0.7447 - loss: 0.5244 - val_accuracy: 0.7026 - val_loss: 0.5867\n",
      "Epoch 14/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7485 - loss: 0.5187\n",
      "Epoch 14: val_accuracy did not improve from 0.70263\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 132ms/step - accuracy: 0.7485 - loss: 0.5187 - val_accuracy: 0.7026 - val_loss: 0.5825\n",
      "Epoch 15/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7519 - loss: 0.5134\n",
      "Epoch 15: val_accuracy did not improve from 0.70263\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - accuracy: 0.7519 - loss: 0.5134 - val_accuracy: 0.7008 - val_loss: 0.5783\n",
      "Epoch 16/100\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7542 - loss: 0.5085\n",
      "Epoch 16: val_accuracy did not improve from 0.70263\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7542 - loss: 0.5085 - val_accuracy: 0.7026 - val_loss: 0.5749\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 100, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6092 - loss: 0.7034\n",
      "Test accuracy: 0.7101313471794128\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(\"model_combined.keras\")\n",
    "accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, Input, Bidirectional, LSTM, Dropout, Dense, Lambda, GlobalMaxPooling1D, Flatten, GlobalAveragePooling1D, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def train_model(optimizer, epochs, batch_size, lr):\n",
    "    tf.random.set_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_combined.keras\", \n",
    "        monitor='val_accuracy',            \n",
    "        save_best_only=True,           \n",
    "        mode='max',                 \n",
    "        save_weights_only=False,       \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    \n",
    "    gru_output = GRU(32, return_sequences=True)(embedding_layer)\n",
    "    gru_output = Dropout(0.5)(gru_output)\n",
    "    attention_output = MultiHeadAttention(num_heads=1, key_dim=16)(gru_output, gru_output)\n",
    "    gru_output = Bidirectional(GRU(16, return_sequences=True))(attention_output)\n",
    "    max_output = GlobalMaxPooling1D()(gru_output)\n",
    "    gru_output = gru_output[:, -1, :]\n",
    "    concat_output = tf.keras.layers.Concatenate(axis=1)([max_output, gru_output])\n",
    "    output_layer = Dense(1, activation='sigmoid')(concat_output) \n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',     # Monitor validation loss\n",
    "        factor=0.75,             # Factor to reduce the learning rate\n",
    "        patience=3,             # Number of epochs with no improvement to wait\n",
    "        min_lr=1e-6             # Minimum learning rate limit\n",
    "    )\n",
    "    # Set optimizer\n",
    "    if optimizer == 'adam': optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd': optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer == 'rmsprop': optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    else: optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[checkpoint_callback, reduce_lr]\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.4986 - loss: 0.6936\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50844, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 164ms/step - accuracy: 0.4986 - loss: 0.6936 - val_accuracy: 0.5084 - val_loss: 0.6930 - learning_rate: 0.0100\n",
      "Epoch 2/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5070 - loss: 0.6930\n",
      "Epoch 2: val_accuracy improved from 0.50844 to 0.52345, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.5070 - loss: 0.6930 - val_accuracy: 0.5235 - val_loss: 0.6927 - learning_rate: 0.0100\n",
      "Epoch 3/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5170 - loss: 0.6927\n",
      "Epoch 3: val_accuracy improved from 0.52345 to 0.53189, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.5170 - loss: 0.6927 - val_accuracy: 0.5319 - val_loss: 0.6924 - learning_rate: 0.0100\n",
      "Epoch 4/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.5214 - loss: 0.6924\n",
      "Epoch 4: val_accuracy improved from 0.53189 to 0.54597, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 177ms/step - accuracy: 0.5214 - loss: 0.6924 - val_accuracy: 0.5460 - val_loss: 0.6921 - learning_rate: 0.0100\n",
      "Epoch 5/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5435 - loss: 0.6917\n",
      "Epoch 5: val_accuracy improved from 0.54597 to 0.55535, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.5436 - loss: 0.6917 - val_accuracy: 0.5553 - val_loss: 0.6915 - learning_rate: 0.0100\n",
      "Epoch 6/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5482 - loss: 0.6913\n",
      "Epoch 6: val_accuracy improved from 0.55535 to 0.56848, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.5483 - loss: 0.6913 - val_accuracy: 0.5685 - val_loss: 0.6908 - learning_rate: 0.0100\n",
      "Epoch 7/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.5678 - loss: 0.6905\n",
      "Epoch 7: val_accuracy improved from 0.56848 to 0.58537, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.5678 - loss: 0.6905 - val_accuracy: 0.5854 - val_loss: 0.6900 - learning_rate: 0.0100\n",
      "Epoch 8/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5860 - loss: 0.6893\n",
      "Epoch 8: val_accuracy improved from 0.58537 to 0.60038, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.5860 - loss: 0.6893 - val_accuracy: 0.6004 - val_loss: 0.6888 - learning_rate: 0.0100\n",
      "Epoch 9/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.5904 - loss: 0.6881\n",
      "Epoch 9: val_accuracy improved from 0.60038 to 0.61069, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 177ms/step - accuracy: 0.5904 - loss: 0.6881 - val_accuracy: 0.6107 - val_loss: 0.6871 - learning_rate: 0.0100\n",
      "Epoch 10/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.5942 - loss: 0.6863\n",
      "Epoch 10: val_accuracy improved from 0.61069 to 0.61163, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.5943 - loss: 0.6863 - val_accuracy: 0.6116 - val_loss: 0.6847 - learning_rate: 0.0100\n",
      "Epoch 11/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.6045 - loss: 0.6835\n",
      "Epoch 11: val_accuracy improved from 0.61163 to 0.62008, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 200ms/step - accuracy: 0.6046 - loss: 0.6835 - val_accuracy: 0.6201 - val_loss: 0.6811 - learning_rate: 0.0100\n",
      "Epoch 12/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.6078 - loss: 0.6797\n",
      "Epoch 12: val_accuracy improved from 0.62008 to 0.62195, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 235ms/step - accuracy: 0.6079 - loss: 0.6797 - val_accuracy: 0.6220 - val_loss: 0.6759 - learning_rate: 0.0100\n",
      "Epoch 13/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.6220 - loss: 0.6733\n",
      "Epoch 13: val_accuracy improved from 0.62195 to 0.62664, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 210ms/step - accuracy: 0.6221 - loss: 0.6732 - val_accuracy: 0.6266 - val_loss: 0.6685 - learning_rate: 0.0100\n",
      "Epoch 14/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.6180 - loss: 0.6648\n",
      "Epoch 14: val_accuracy improved from 0.62664 to 0.64353, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 198ms/step - accuracy: 0.6181 - loss: 0.6647 - val_accuracy: 0.6435 - val_loss: 0.6588 - learning_rate: 0.0100\n",
      "Epoch 15/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.6276 - loss: 0.6541\n",
      "Epoch 15: val_accuracy improved from 0.64353 to 0.64916, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - accuracy: 0.6277 - loss: 0.6540 - val_accuracy: 0.6492 - val_loss: 0.6457 - learning_rate: 0.0100\n",
      "Epoch 16/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6397 - loss: 0.6415\n",
      "Epoch 16: val_accuracy improved from 0.64916 to 0.67167, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.6397 - loss: 0.6414 - val_accuracy: 0.6717 - val_loss: 0.6311 - learning_rate: 0.0100\n",
      "Epoch 17/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6477 - loss: 0.6314\n",
      "Epoch 17: val_accuracy did not improve from 0.67167\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.6478 - loss: 0.6313 - val_accuracy: 0.6642 - val_loss: 0.6179 - learning_rate: 0.0100\n",
      "Epoch 18/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.6584 - loss: 0.6198\n",
      "Epoch 18: val_accuracy did not improve from 0.67167\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.6585 - loss: 0.6197 - val_accuracy: 0.6604 - val_loss: 0.6120 - learning_rate: 0.0100\n",
      "Epoch 19/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.6716 - loss: 0.6085\n",
      "Epoch 19: val_accuracy improved from 0.67167 to 0.67636, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.6716 - loss: 0.6084 - val_accuracy: 0.6764 - val_loss: 0.5983 - learning_rate: 0.0100\n",
      "Epoch 20/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.6804 - loss: 0.5972\n",
      "Epoch 20: val_accuracy improved from 0.67636 to 0.67917, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 176ms/step - accuracy: 0.6804 - loss: 0.5971 - val_accuracy: 0.6792 - val_loss: 0.5957 - learning_rate: 0.0100\n",
      "Epoch 21/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.6811 - loss: 0.5944\n",
      "Epoch 21: val_accuracy improved from 0.67917 to 0.69606, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.6812 - loss: 0.5943 - val_accuracy: 0.6961 - val_loss: 0.5788 - learning_rate: 0.0100\n",
      "Epoch 22/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.6967 - loss: 0.5883\n",
      "Epoch 22: val_accuracy did not improve from 0.69606\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.6967 - loss: 0.5883 - val_accuracy: 0.6857 - val_loss: 0.5900 - learning_rate: 0.0100\n",
      "Epoch 23/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7034 - loss: 0.5795\n",
      "Epoch 23: val_accuracy did not improve from 0.69606\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - accuracy: 0.7034 - loss: 0.5794 - val_accuracy: 0.6914 - val_loss: 0.5853 - learning_rate: 0.0100\n",
      "Epoch 24/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7064 - loss: 0.5720\n",
      "Epoch 24: val_accuracy did not improve from 0.69606\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7064 - loss: 0.5719 - val_accuracy: 0.6923 - val_loss: 0.5859 - learning_rate: 0.0100\n",
      "Epoch 25/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.7163 - loss: 0.5631\n",
      "Epoch 25: val_accuracy improved from 0.69606 to 0.69700, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 186ms/step - accuracy: 0.7163 - loss: 0.5631 - val_accuracy: 0.6970 - val_loss: 0.5675 - learning_rate: 0.0075\n",
      "Epoch 26/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.7176 - loss: 0.5583\n",
      "Epoch 26: val_accuracy improved from 0.69700 to 0.70263, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.7177 - loss: 0.5583 - val_accuracy: 0.7026 - val_loss: 0.5605 - learning_rate: 0.0075\n",
      "Epoch 27/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.7183 - loss: 0.5569\n",
      "Epoch 27: val_accuracy improved from 0.70263 to 0.72045, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.7184 - loss: 0.5569 - val_accuracy: 0.7205 - val_loss: 0.5519 - learning_rate: 0.0075\n",
      "Epoch 28/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.7246 - loss: 0.5500\n",
      "Epoch 28: val_accuracy improved from 0.72045 to 0.72983, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.7246 - loss: 0.5499 - val_accuracy: 0.7298 - val_loss: 0.5478 - learning_rate: 0.0075\n",
      "Epoch 29/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7217 - loss: 0.5513\n",
      "Epoch 29: val_accuracy did not improve from 0.72983\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.7217 - loss: 0.5513 - val_accuracy: 0.7176 - val_loss: 0.5501 - learning_rate: 0.0075\n",
      "Epoch 30/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7203 - loss: 0.5502\n",
      "Epoch 30: val_accuracy did not improve from 0.72983\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7204 - loss: 0.5502 - val_accuracy: 0.7214 - val_loss: 0.5471 - learning_rate: 0.0075\n",
      "Epoch 31/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7233 - loss: 0.5452\n",
      "Epoch 31: val_accuracy improved from 0.72983 to 0.73452, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.7233 - loss: 0.5451 - val_accuracy: 0.7345 - val_loss: 0.5411 - learning_rate: 0.0075\n",
      "Epoch 32/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.7308 - loss: 0.5465\n",
      "Epoch 32: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.7308 - loss: 0.5465 - val_accuracy: 0.7214 - val_loss: 0.5443 - learning_rate: 0.0075\n",
      "Epoch 33/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7295 - loss: 0.5421\n",
      "Epoch 33: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.7295 - loss: 0.5421 - val_accuracy: 0.7176 - val_loss: 0.5453 - learning_rate: 0.0075\n",
      "Epoch 34/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.7312 - loss: 0.5383\n",
      "Epoch 34: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 219ms/step - accuracy: 0.7313 - loss: 0.5383 - val_accuracy: 0.7214 - val_loss: 0.5401 - learning_rate: 0.0075\n",
      "Epoch 35/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7383 - loss: 0.5345\n",
      "Epoch 35: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7383 - loss: 0.5344 - val_accuracy: 0.7261 - val_loss: 0.5354 - learning_rate: 0.0075\n",
      "Epoch 36/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7371 - loss: 0.5358\n",
      "Epoch 36: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7371 - loss: 0.5357 - val_accuracy: 0.7251 - val_loss: 0.5382 - learning_rate: 0.0075\n",
      "Epoch 37/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7400 - loss: 0.5322\n",
      "Epoch 37: val_accuracy did not improve from 0.73452\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7400 - loss: 0.5322 - val_accuracy: 0.7298 - val_loss: 0.5323 - learning_rate: 0.0075\n",
      "Epoch 38/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7380 - loss: 0.5302\n",
      "Epoch 38: val_accuracy improved from 0.73452 to 0.73546, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.7381 - loss: 0.5302 - val_accuracy: 0.7355 - val_loss: 0.5304 - learning_rate: 0.0075\n",
      "Epoch 39/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.7412 - loss: 0.5299\n",
      "Epoch 39: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 181ms/step - accuracy: 0.7412 - loss: 0.5298 - val_accuracy: 0.7355 - val_loss: 0.5290 - learning_rate: 0.0075\n",
      "Epoch 40/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7406 - loss: 0.5285\n",
      "Epoch 40: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.7407 - loss: 0.5284 - val_accuracy: 0.7308 - val_loss: 0.5283 - learning_rate: 0.0075\n",
      "Epoch 41/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.7461 - loss: 0.5227\n",
      "Epoch 41: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - accuracy: 0.7461 - loss: 0.5226 - val_accuracy: 0.7280 - val_loss: 0.5275 - learning_rate: 0.0075\n",
      "Epoch 42/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.7503 - loss: 0.5225\n",
      "Epoch 42: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 197ms/step - accuracy: 0.7503 - loss: 0.5224 - val_accuracy: 0.7326 - val_loss: 0.5255 - learning_rate: 0.0075\n",
      "Epoch 43/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.7446 - loss: 0.5226\n",
      "Epoch 43: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 205ms/step - accuracy: 0.7446 - loss: 0.5226 - val_accuracy: 0.7326 - val_loss: 0.5245 - learning_rate: 0.0075\n",
      "Epoch 44/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7464 - loss: 0.5216\n",
      "Epoch 44: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 201ms/step - accuracy: 0.7464 - loss: 0.5215 - val_accuracy: 0.7326 - val_loss: 0.5254 - learning_rate: 0.0075\n",
      "Epoch 45/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7488 - loss: 0.5181\n",
      "Epoch 45: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 194ms/step - accuracy: 0.7488 - loss: 0.5181 - val_accuracy: 0.7336 - val_loss: 0.5225 - learning_rate: 0.0075\n",
      "Epoch 46/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.7486 - loss: 0.5129\n",
      "Epoch 46: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.7486 - loss: 0.5129 - val_accuracy: 0.7336 - val_loss: 0.5223 - learning_rate: 0.0075\n",
      "Epoch 47/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.7508 - loss: 0.5134\n",
      "Epoch 47: val_accuracy did not improve from 0.73546\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.7508 - loss: 0.5133 - val_accuracy: 0.7336 - val_loss: 0.5202 - learning_rate: 0.0075\n",
      "Epoch 48/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.7504 - loss: 0.5101\n",
      "Epoch 48: val_accuracy improved from 0.73546 to 0.73921, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - accuracy: 0.7504 - loss: 0.5101 - val_accuracy: 0.7392 - val_loss: 0.5183 - learning_rate: 0.0075\n",
      "Epoch 49/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.7583 - loss: 0.5072\n",
      "Epoch 49: val_accuracy improved from 0.73921 to 0.74015, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 214ms/step - accuracy: 0.7583 - loss: 0.5071 - val_accuracy: 0.7402 - val_loss: 0.5171 - learning_rate: 0.0075\n",
      "Epoch 50/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7573 - loss: 0.5058\n",
      "Epoch 50: val_accuracy did not improve from 0.74015\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 205ms/step - accuracy: 0.7572 - loss: 0.5058 - val_accuracy: 0.7392 - val_loss: 0.5159 - learning_rate: 0.0075\n",
      "Epoch 51/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7560 - loss: 0.5042\n",
      "Epoch 51: val_accuracy improved from 0.74015 to 0.74203, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 249ms/step - accuracy: 0.7559 - loss: 0.5042 - val_accuracy: 0.7420 - val_loss: 0.5146 - learning_rate: 0.0075\n",
      "Epoch 52/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7551 - loss: 0.5053\n",
      "Epoch 52: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 241ms/step - accuracy: 0.7552 - loss: 0.5053 - val_accuracy: 0.7420 - val_loss: 0.5134 - learning_rate: 0.0075\n",
      "Epoch 53/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.7579 - loss: 0.5044\n",
      "Epoch 53: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 255ms/step - accuracy: 0.7579 - loss: 0.5043 - val_accuracy: 0.7402 - val_loss: 0.5128 - learning_rate: 0.0075\n",
      "Epoch 54/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7596 - loss: 0.4996\n",
      "Epoch 54: val_accuracy did not improve from 0.74203\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 192ms/step - accuracy: 0.7596 - loss: 0.4996 - val_accuracy: 0.7411 - val_loss: 0.5116 - learning_rate: 0.0075\n",
      "Epoch 55/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7604 - loss: 0.4941\n",
      "Epoch 55: val_accuracy improved from 0.74203 to 0.74296, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 232ms/step - accuracy: 0.7605 - loss: 0.4941 - val_accuracy: 0.7430 - val_loss: 0.5102 - learning_rate: 0.0075\n",
      "Epoch 56/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7587 - loss: 0.4970\n",
      "Epoch 56: val_accuracy did not improve from 0.74296\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 210ms/step - accuracy: 0.7587 - loss: 0.4969 - val_accuracy: 0.7411 - val_loss: 0.5104 - learning_rate: 0.0075\n",
      "Epoch 57/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7712 - loss: 0.4902\n",
      "Epoch 57: val_accuracy improved from 0.74296 to 0.74484, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 239ms/step - accuracy: 0.7712 - loss: 0.4902 - val_accuracy: 0.7448 - val_loss: 0.5080 - learning_rate: 0.0075\n",
      "Epoch 58/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7606 - loss: 0.4922\n",
      "Epoch 58: val_accuracy improved from 0.74484 to 0.74953, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 249ms/step - accuracy: 0.7607 - loss: 0.4922 - val_accuracy: 0.7495 - val_loss: 0.5066 - learning_rate: 0.0075\n",
      "Epoch 59/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7674 - loss: 0.4887\n",
      "Epoch 59: val_accuracy did not improve from 0.74953\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 211ms/step - accuracy: 0.7674 - loss: 0.4886 - val_accuracy: 0.7477 - val_loss: 0.5057 - learning_rate: 0.0075\n",
      "Epoch 60/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7667 - loss: 0.4922\n",
      "Epoch 60: val_accuracy did not improve from 0.74953\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 194ms/step - accuracy: 0.7667 - loss: 0.4922 - val_accuracy: 0.7486 - val_loss: 0.5044 - learning_rate: 0.0075\n",
      "Epoch 61/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.7651 - loss: 0.4895\n",
      "Epoch 61: val_accuracy improved from 0.74953 to 0.75235, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 206ms/step - accuracy: 0.7651 - loss: 0.4895 - val_accuracy: 0.7523 - val_loss: 0.5037 - learning_rate: 0.0075\n",
      "Epoch 62/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7685 - loss: 0.4840\n",
      "Epoch 62: val_accuracy improved from 0.75235 to 0.75328, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 226ms/step - accuracy: 0.7684 - loss: 0.4840 - val_accuracy: 0.7533 - val_loss: 0.5027 - learning_rate: 0.0075\n",
      "Epoch 63/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7666 - loss: 0.4855\n",
      "Epoch 63: val_accuracy did not improve from 0.75328\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.7667 - loss: 0.4854 - val_accuracy: 0.7523 - val_loss: 0.5018 - learning_rate: 0.0075\n",
      "Epoch 64/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7666 - loss: 0.4847\n",
      "Epoch 64: val_accuracy did not improve from 0.75328\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.7667 - loss: 0.4846 - val_accuracy: 0.7533 - val_loss: 0.5007 - learning_rate: 0.0075\n",
      "Epoch 65/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7708 - loss: 0.4802\n",
      "Epoch 65: val_accuracy improved from 0.75328 to 0.75516, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.7708 - loss: 0.4801 - val_accuracy: 0.7552 - val_loss: 0.4999 - learning_rate: 0.0075\n",
      "Epoch 66/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7770 - loss: 0.4736\n",
      "Epoch 66: val_accuracy improved from 0.75516 to 0.75891, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.7769 - loss: 0.4736 - val_accuracy: 0.7589 - val_loss: 0.4998 - learning_rate: 0.0075\n",
      "Epoch 67/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.7722 - loss: 0.4779\n",
      "Epoch 67: val_accuracy did not improve from 0.75891\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.7722 - loss: 0.4779 - val_accuracy: 0.7542 - val_loss: 0.4989 - learning_rate: 0.0075\n",
      "Epoch 68/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7716 - loss: 0.4775\n",
      "Epoch 68: val_accuracy did not improve from 0.75891\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.7716 - loss: 0.4774 - val_accuracy: 0.7589 - val_loss: 0.4977 - learning_rate: 0.0075\n",
      "Epoch 69/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7755 - loss: 0.4745\n",
      "Epoch 69: val_accuracy did not improve from 0.75891\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.7755 - loss: 0.4745 - val_accuracy: 0.7561 - val_loss: 0.4977 - learning_rate: 0.0075\n",
      "Epoch 70/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7735 - loss: 0.4738\n",
      "Epoch 70: val_accuracy improved from 0.75891 to 0.75985, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7735 - loss: 0.4737 - val_accuracy: 0.7598 - val_loss: 0.4961 - learning_rate: 0.0075\n",
      "Epoch 71/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7781 - loss: 0.4772\n",
      "Epoch 71: val_accuracy improved from 0.75985 to 0.76079, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.7781 - loss: 0.4771 - val_accuracy: 0.7608 - val_loss: 0.4961 - learning_rate: 0.0075\n",
      "Epoch 72/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7770 - loss: 0.4690\n",
      "Epoch 72: val_accuracy did not improve from 0.76079\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.7771 - loss: 0.4689 - val_accuracy: 0.7598 - val_loss: 0.4953 - learning_rate: 0.0075\n",
      "Epoch 73/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.7739 - loss: 0.4710\n",
      "Epoch 73: val_accuracy did not improve from 0.76079\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.7739 - loss: 0.4710 - val_accuracy: 0.7580 - val_loss: 0.4957 - learning_rate: 0.0075\n",
      "Epoch 74/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.7789 - loss: 0.4686\n",
      "Epoch 74: val_accuracy did not improve from 0.76079\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.7789 - loss: 0.4686 - val_accuracy: 0.7608 - val_loss: 0.4944 - learning_rate: 0.0075\n",
      "Epoch 75/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.7800 - loss: 0.4667\n",
      "Epoch 75: val_accuracy did not improve from 0.76079\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.7800 - loss: 0.4666 - val_accuracy: 0.7589 - val_loss: 0.4941 - learning_rate: 0.0075\n",
      "Epoch 76/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7770 - loss: 0.4695\n",
      "Epoch 76: val_accuracy improved from 0.76079 to 0.76173, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7771 - loss: 0.4695 - val_accuracy: 0.7617 - val_loss: 0.4928 - learning_rate: 0.0075\n",
      "Epoch 77/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7800 - loss: 0.4673\n",
      "Epoch 77: val_accuracy improved from 0.76173 to 0.76454, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.7800 - loss: 0.4673 - val_accuracy: 0.7645 - val_loss: 0.4920 - learning_rate: 0.0075\n",
      "Epoch 78/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7774 - loss: 0.4674\n",
      "Epoch 78: val_accuracy did not improve from 0.76454\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.7774 - loss: 0.4674 - val_accuracy: 0.7589 - val_loss: 0.4930 - learning_rate: 0.0075\n",
      "Epoch 79/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7862 - loss: 0.4623\n",
      "Epoch 79: val_accuracy did not improve from 0.76454\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.7862 - loss: 0.4623 - val_accuracy: 0.7598 - val_loss: 0.4917 - learning_rate: 0.0075\n",
      "Epoch 80/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7854 - loss: 0.4601\n",
      "Epoch 80: val_accuracy did not improve from 0.76454\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.7854 - loss: 0.4601 - val_accuracy: 0.7627 - val_loss: 0.4920 - learning_rate: 0.0075\n",
      "Epoch 81/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.7797 - loss: 0.4604\n",
      "Epoch 81: val_accuracy did not improve from 0.76454\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.7797 - loss: 0.4603 - val_accuracy: 0.7636 - val_loss: 0.4909 - learning_rate: 0.0075\n",
      "Epoch 82/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7831 - loss: 0.4624\n",
      "Epoch 82: val_accuracy improved from 0.76454 to 0.76735, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.7831 - loss: 0.4624 - val_accuracy: 0.7674 - val_loss: 0.4892 - learning_rate: 0.0075\n",
      "Epoch 83/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7871 - loss: 0.4534\n",
      "Epoch 83: val_accuracy did not improve from 0.76735\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.7871 - loss: 0.4533 - val_accuracy: 0.7664 - val_loss: 0.4888 - learning_rate: 0.0075\n",
      "Epoch 84/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7796 - loss: 0.4598\n",
      "Epoch 84: val_accuracy did not improve from 0.76735\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.7796 - loss: 0.4598 - val_accuracy: 0.7655 - val_loss: 0.4879 - learning_rate: 0.0075\n",
      "Epoch 85/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7859 - loss: 0.4564\n",
      "Epoch 85: val_accuracy did not improve from 0.76735\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.7859 - loss: 0.4564 - val_accuracy: 0.7645 - val_loss: 0.4883 - learning_rate: 0.0075\n",
      "Epoch 86/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7914 - loss: 0.4506\n",
      "Epoch 86: val_accuracy did not improve from 0.76735\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.7914 - loss: 0.4506 - val_accuracy: 0.7674 - val_loss: 0.4884 - learning_rate: 0.0075\n",
      "Epoch 87/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.7899 - loss: 0.4537\n",
      "Epoch 87: val_accuracy improved from 0.76735 to 0.76829, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 176ms/step - accuracy: 0.7898 - loss: 0.4536 - val_accuracy: 0.7683 - val_loss: 0.4871 - learning_rate: 0.0075\n",
      "Epoch 88/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7862 - loss: 0.4538\n",
      "Epoch 88: val_accuracy did not improve from 0.76829\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.7862 - loss: 0.4538 - val_accuracy: 0.7655 - val_loss: 0.4865 - learning_rate: 0.0075\n",
      "Epoch 89/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7852 - loss: 0.4541\n",
      "Epoch 89: val_accuracy did not improve from 0.76829\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 160ms/step - accuracy: 0.7852 - loss: 0.4540 - val_accuracy: 0.7683 - val_loss: 0.4860 - learning_rate: 0.0075\n",
      "Epoch 90/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7814 - loss: 0.4530\n",
      "Epoch 90: val_accuracy did not improve from 0.76829\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.7814 - loss: 0.4529 - val_accuracy: 0.7674 - val_loss: 0.4857 - learning_rate: 0.0075\n",
      "Epoch 91/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7887 - loss: 0.4504\n",
      "Epoch 91: val_accuracy did not improve from 0.76829\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.7887 - loss: 0.4504 - val_accuracy: 0.7674 - val_loss: 0.4852 - learning_rate: 0.0075\n",
      "Epoch 92/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.7896 - loss: 0.4486\n",
      "Epoch 92: val_accuracy improved from 0.76829 to 0.77017, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.7896 - loss: 0.4486 - val_accuracy: 0.7702 - val_loss: 0.4848 - learning_rate: 0.0075\n",
      "Epoch 93/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7879 - loss: 0.4484\n",
      "Epoch 93: val_accuracy improved from 0.77017 to 0.77205, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.7879 - loss: 0.4484 - val_accuracy: 0.7720 - val_loss: 0.4847 - learning_rate: 0.0075\n",
      "Epoch 94/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7943 - loss: 0.4447\n",
      "Epoch 94: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.7943 - loss: 0.4446 - val_accuracy: 0.7674 - val_loss: 0.4846 - learning_rate: 0.0075\n",
      "Epoch 95/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.7935 - loss: 0.4467\n",
      "Epoch 95: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - accuracy: 0.7935 - loss: 0.4467 - val_accuracy: 0.7664 - val_loss: 0.4846 - learning_rate: 0.0075\n",
      "Epoch 96/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7989 - loss: 0.4429\n",
      "Epoch 96: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.7989 - loss: 0.4429 - val_accuracy: 0.7702 - val_loss: 0.4839 - learning_rate: 0.0075\n",
      "Epoch 97/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7921 - loss: 0.4453\n",
      "Epoch 97: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.7921 - loss: 0.4453 - val_accuracy: 0.7692 - val_loss: 0.4834 - learning_rate: 0.0075\n",
      "Epoch 98/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8009 - loss: 0.4417\n",
      "Epoch 98: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8008 - loss: 0.4416 - val_accuracy: 0.7702 - val_loss: 0.4831 - learning_rate: 0.0075\n",
      "Epoch 99/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7964 - loss: 0.4422\n",
      "Epoch 99: val_accuracy did not improve from 0.77205\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.7964 - loss: 0.4422 - val_accuracy: 0.7702 - val_loss: 0.4830 - learning_rate: 0.0075\n",
      "Epoch 100/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7917 - loss: 0.4372\n",
      "Epoch 100: val_accuracy improved from 0.77205 to 0.77298, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.7917 - loss: 0.4371 - val_accuracy: 0.7730 - val_loss: 0.4824 - learning_rate: 0.0075\n",
      "Epoch 101/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8010 - loss: 0.4381\n",
      "Epoch 101: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.8009 - loss: 0.4381 - val_accuracy: 0.7664 - val_loss: 0.4834 - learning_rate: 0.0075\n",
      "Epoch 102/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.7942 - loss: 0.4357\n",
      "Epoch 102: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 154ms/step - accuracy: 0.7942 - loss: 0.4356 - val_accuracy: 0.7664 - val_loss: 0.4833 - learning_rate: 0.0075\n",
      "Epoch 103/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8000 - loss: 0.4378\n",
      "Epoch 103: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.8000 - loss: 0.4377 - val_accuracy: 0.7674 - val_loss: 0.4819 - learning_rate: 0.0075\n",
      "Epoch 104/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8018 - loss: 0.4332\n",
      "Epoch 104: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 160ms/step - accuracy: 0.8018 - loss: 0.4332 - val_accuracy: 0.7683 - val_loss: 0.4836 - learning_rate: 0.0075\n",
      "Epoch 105/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8023 - loss: 0.4326\n",
      "Epoch 105: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8023 - loss: 0.4326 - val_accuracy: 0.7664 - val_loss: 0.4832 - learning_rate: 0.0075\n",
      "Epoch 106/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7977 - loss: 0.4345\n",
      "Epoch 106: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.7977 - loss: 0.4344 - val_accuracy: 0.7674 - val_loss: 0.4809 - learning_rate: 0.0075\n",
      "Epoch 107/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7995 - loss: 0.4288\n",
      "Epoch 107: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.7996 - loss: 0.4288 - val_accuracy: 0.7702 - val_loss: 0.4810 - learning_rate: 0.0075\n",
      "Epoch 108/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.8003 - loss: 0.4311\n",
      "Epoch 108: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.8003 - loss: 0.4311 - val_accuracy: 0.7683 - val_loss: 0.4804 - learning_rate: 0.0075\n",
      "Epoch 109/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8001 - loss: 0.4281\n",
      "Epoch 109: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.8001 - loss: 0.4281 - val_accuracy: 0.7674 - val_loss: 0.4805 - learning_rate: 0.0075\n",
      "Epoch 110/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8041 - loss: 0.4256\n",
      "Epoch 110: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8041 - loss: 0.4256 - val_accuracy: 0.7720 - val_loss: 0.4801 - learning_rate: 0.0075\n",
      "Epoch 111/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8058 - loss: 0.4246\n",
      "Epoch 111: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8058 - loss: 0.4246 - val_accuracy: 0.7702 - val_loss: 0.4800 - learning_rate: 0.0075\n",
      "Epoch 112/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8032 - loss: 0.4237\n",
      "Epoch 112: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8032 - loss: 0.4237 - val_accuracy: 0.7702 - val_loss: 0.4802 - learning_rate: 0.0075\n",
      "Epoch 113/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8068 - loss: 0.4232\n",
      "Epoch 113: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.8068 - loss: 0.4231 - val_accuracy: 0.7720 - val_loss: 0.4799 - learning_rate: 0.0075\n",
      "Epoch 114/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8075 - loss: 0.4254\n",
      "Epoch 114: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.8075 - loss: 0.4253 - val_accuracy: 0.7720 - val_loss: 0.4804 - learning_rate: 0.0075\n",
      "Epoch 115/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8050 - loss: 0.4229\n",
      "Epoch 115: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8051 - loss: 0.4228 - val_accuracy: 0.7711 - val_loss: 0.4795 - learning_rate: 0.0075\n",
      "Epoch 116/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8109 - loss: 0.4195\n",
      "Epoch 116: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8109 - loss: 0.4195 - val_accuracy: 0.7692 - val_loss: 0.4792 - learning_rate: 0.0075\n",
      "Epoch 117/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8126 - loss: 0.4169\n",
      "Epoch 117: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - accuracy: 0.8126 - loss: 0.4168 - val_accuracy: 0.7720 - val_loss: 0.4789 - learning_rate: 0.0075\n",
      "Epoch 118/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8107 - loss: 0.4181\n",
      "Epoch 118: val_accuracy did not improve from 0.77298\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8107 - loss: 0.4181 - val_accuracy: 0.7692 - val_loss: 0.4786 - learning_rate: 0.0075\n",
      "Epoch 119/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8142 - loss: 0.4147\n",
      "Epoch 119: val_accuracy improved from 0.77298 to 0.77674, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8141 - loss: 0.4146 - val_accuracy: 0.7767 - val_loss: 0.4783 - learning_rate: 0.0075\n",
      "Epoch 120/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8071 - loss: 0.4133\n",
      "Epoch 120: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8071 - loss: 0.4133 - val_accuracy: 0.7739 - val_loss: 0.4780 - learning_rate: 0.0075\n",
      "Epoch 121/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8120 - loss: 0.4136\n",
      "Epoch 121: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8120 - loss: 0.4135 - val_accuracy: 0.7730 - val_loss: 0.4784 - learning_rate: 0.0075\n",
      "Epoch 122/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8089 - loss: 0.4141\n",
      "Epoch 122: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - accuracy: 0.8089 - loss: 0.4141 - val_accuracy: 0.7739 - val_loss: 0.4779 - learning_rate: 0.0075\n",
      "Epoch 123/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8140 - loss: 0.4108\n",
      "Epoch 123: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8140 - loss: 0.4108 - val_accuracy: 0.7730 - val_loss: 0.4781 - learning_rate: 0.0075\n",
      "Epoch 124/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8139 - loss: 0.4113\n",
      "Epoch 124: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8139 - loss: 0.4113 - val_accuracy: 0.7758 - val_loss: 0.4778 - learning_rate: 0.0056\n",
      "Epoch 125/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8155 - loss: 0.4073\n",
      "Epoch 125: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.8155 - loss: 0.4073 - val_accuracy: 0.7720 - val_loss: 0.4781 - learning_rate: 0.0056\n",
      "Epoch 126/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8116 - loss: 0.4105\n",
      "Epoch 126: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8116 - loss: 0.4105 - val_accuracy: 0.7720 - val_loss: 0.4777 - learning_rate: 0.0056\n",
      "Epoch 127/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8186 - loss: 0.4104\n",
      "Epoch 127: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8187 - loss: 0.4103 - val_accuracy: 0.7739 - val_loss: 0.4780 - learning_rate: 0.0056\n",
      "Epoch 128/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8180 - loss: 0.4023\n",
      "Epoch 128: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8180 - loss: 0.4023 - val_accuracy: 0.7730 - val_loss: 0.4782 - learning_rate: 0.0056\n",
      "Epoch 129/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8144 - loss: 0.4082\n",
      "Epoch 129: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8144 - loss: 0.4082 - val_accuracy: 0.7739 - val_loss: 0.4777 - learning_rate: 0.0056\n",
      "Epoch 130/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8180 - loss: 0.4013\n",
      "Epoch 130: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - accuracy: 0.8181 - loss: 0.4013 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 0.0042\n",
      "Epoch 131/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8204 - loss: 0.4009\n",
      "Epoch 131: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8203 - loss: 0.4009 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 0.0042\n",
      "Epoch 132/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8226 - loss: 0.3987\n",
      "Epoch 132: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.8226 - loss: 0.3987 - val_accuracy: 0.7739 - val_loss: 0.4778 - learning_rate: 0.0042\n",
      "Epoch 133/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8229 - loss: 0.4002\n",
      "Epoch 133: val_accuracy did not improve from 0.77674\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8229 - loss: 0.4002 - val_accuracy: 0.7767 - val_loss: 0.4779 - learning_rate: 0.0032\n",
      "Epoch 134/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8202 - loss: 0.4005\n",
      "Epoch 134: val_accuracy improved from 0.77674 to 0.77767, saving model to model_combined.keras\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.8202 - loss: 0.4005 - val_accuracy: 0.7777 - val_loss: 0.4784 - learning_rate: 0.0032\n",
      "Epoch 135/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8170 - loss: 0.3998\n",
      "Epoch 135: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8170 - loss: 0.3997 - val_accuracy: 0.7739 - val_loss: 0.4779 - learning_rate: 0.0032\n",
      "Epoch 136/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8166 - loss: 0.4011\n",
      "Epoch 136: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8166 - loss: 0.4011 - val_accuracy: 0.7777 - val_loss: 0.4780 - learning_rate: 0.0024\n",
      "Epoch 137/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8222 - loss: 0.3951\n",
      "Epoch 137: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 161ms/step - accuracy: 0.8222 - loss: 0.3951 - val_accuracy: 0.7758 - val_loss: 0.4780 - learning_rate: 0.0024\n",
      "Epoch 138/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8151 - loss: 0.4029\n",
      "Epoch 138: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8151 - loss: 0.4029 - val_accuracy: 0.7758 - val_loss: 0.4782 - learning_rate: 0.0024\n",
      "Epoch 139/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8228 - loss: 0.4006\n",
      "Epoch 139: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8228 - loss: 0.4005 - val_accuracy: 0.7758 - val_loss: 0.4781 - learning_rate: 0.0018\n",
      "Epoch 140/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8185 - loss: 0.3992\n",
      "Epoch 140: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8185 - loss: 0.3991 - val_accuracy: 0.7739 - val_loss: 0.4782 - learning_rate: 0.0018\n",
      "Epoch 141/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8225 - loss: 0.3997\n",
      "Epoch 141: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8225 - loss: 0.3997 - val_accuracy: 0.7777 - val_loss: 0.4779 - learning_rate: 0.0018\n",
      "Epoch 142/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8252 - loss: 0.3954\n",
      "Epoch 142: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 158ms/step - accuracy: 0.8252 - loss: 0.3953 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 0.0013\n",
      "Epoch 143/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8208 - loss: 0.3954\n",
      "Epoch 143: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8208 - loss: 0.3954 - val_accuracy: 0.7777 - val_loss: 0.4783 - learning_rate: 0.0013\n",
      "Epoch 144/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8235 - loss: 0.3961\n",
      "Epoch 144: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8234 - loss: 0.3961 - val_accuracy: 0.7749 - val_loss: 0.4780 - learning_rate: 0.0013\n",
      "Epoch 145/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8192 - loss: 0.3963\n",
      "Epoch 145: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8192 - loss: 0.3963 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 0.0010\n",
      "Epoch 146/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8260 - loss: 0.3918\n",
      "Epoch 146: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8260 - loss: 0.3918 - val_accuracy: 0.7777 - val_loss: 0.4783 - learning_rate: 0.0010\n",
      "Epoch 147/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8211 - loss: 0.4021\n",
      "Epoch 147: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8211 - loss: 0.4020 - val_accuracy: 0.7758 - val_loss: 0.4780 - learning_rate: 0.0010\n",
      "Epoch 148/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8242 - loss: 0.3980\n",
      "Epoch 148: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8242 - loss: 0.3980 - val_accuracy: 0.7767 - val_loss: 0.4783 - learning_rate: 7.5085e-04\n",
      "Epoch 149/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8252 - loss: 0.3961\n",
      "Epoch 149: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8252 - loss: 0.3960 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 7.5085e-04\n",
      "Epoch 150/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8244 - loss: 0.3943\n",
      "Epoch 150: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8244 - loss: 0.3943 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 7.5085e-04\n",
      "Epoch 151/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8171 - loss: 0.3987\n",
      "Epoch 151: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8171 - loss: 0.3986 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 5.6314e-04\n",
      "Epoch 152/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8200 - loss: 0.3974\n",
      "Epoch 152: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8200 - loss: 0.3973 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 5.6314e-04\n",
      "Epoch 153/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8173 - loss: 0.3994\n",
      "Epoch 153: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8174 - loss: 0.3994 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 5.6314e-04\n",
      "Epoch 154/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8212 - loss: 0.3931\n",
      "Epoch 154: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8212 - loss: 0.3931 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 4.2235e-04\n",
      "Epoch 155/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8261 - loss: 0.3977\n",
      "Epoch 155: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8261 - loss: 0.3976 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 4.2235e-04\n",
      "Epoch 156/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8255 - loss: 0.3927\n",
      "Epoch 156: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8255 - loss: 0.3927 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 4.2235e-04\n",
      "Epoch 157/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8221 - loss: 0.3946\n",
      "Epoch 157: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.8221 - loss: 0.3946 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 3.1676e-04\n",
      "Epoch 158/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8221 - loss: 0.3937\n",
      "Epoch 158: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8222 - loss: 0.3937 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 3.1676e-04\n",
      "Epoch 159/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8250 - loss: 0.4000\n",
      "Epoch 159: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 164ms/step - accuracy: 0.8250 - loss: 0.3999 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 3.1676e-04\n",
      "Epoch 160/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8212 - loss: 0.3951\n",
      "Epoch 160: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8212 - loss: 0.3950 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 2.3757e-04\n",
      "Epoch 161/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8235 - loss: 0.3945\n",
      "Epoch 161: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - accuracy: 0.8235 - loss: 0.3944 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 2.3757e-04\n",
      "Epoch 162/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8171 - loss: 0.3982\n",
      "Epoch 162: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8172 - loss: 0.3981 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 2.3757e-04\n",
      "Epoch 163/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8218 - loss: 0.3956\n",
      "Epoch 163: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8218 - loss: 0.3956 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 1.7818e-04\n",
      "Epoch 164/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8235 - loss: 0.3991\n",
      "Epoch 164: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8235 - loss: 0.3991 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 1.7818e-04\n",
      "Epoch 165/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8260 - loss: 0.3931\n",
      "Epoch 165: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8260 - loss: 0.3930 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 1.7818e-04\n",
      "Epoch 166/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8278 - loss: 0.3919\n",
      "Epoch 166: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8277 - loss: 0.3919 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 1.3363e-04\n",
      "Epoch 167/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8238 - loss: 0.3972\n",
      "Epoch 167: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8239 - loss: 0.3972 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 1.3363e-04\n",
      "Epoch 168/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8218 - loss: 0.3954\n",
      "Epoch 168: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8218 - loss: 0.3954 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 1.3363e-04\n",
      "Epoch 169/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8246 - loss: 0.3926\n",
      "Epoch 169: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8246 - loss: 0.3926 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 1.0023e-04\n",
      "Epoch 170/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8210 - loss: 0.3973\n",
      "Epoch 170: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - accuracy: 0.8210 - loss: 0.3972 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 1.0023e-04\n",
      "Epoch 171/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8224 - loss: 0.3935\n",
      "Epoch 171: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.8225 - loss: 0.3935 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 1.0023e-04\n",
      "Epoch 172/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8221 - loss: 0.3930\n",
      "Epoch 172: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 170ms/step - accuracy: 0.8221 - loss: 0.3930 - val_accuracy: 0.7777 - val_loss: 0.4781 - learning_rate: 7.5169e-05\n",
      "Epoch 173/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8223 - loss: 0.3975\n",
      "Epoch 173: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8224 - loss: 0.3975 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 7.5169e-05\n",
      "Epoch 174/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8144 - loss: 0.3960\n",
      "Epoch 174: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8144 - loss: 0.3960 - val_accuracy: 0.7767 - val_loss: 0.4781 - learning_rate: 7.5169e-05\n",
      "Epoch 175/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8279 - loss: 0.3938\n",
      "Epoch 175: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8279 - loss: 0.3937 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 5.6377e-05\n",
      "Epoch 176/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8252 - loss: 0.3967\n",
      "Epoch 176: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.8252 - loss: 0.3967 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 5.6377e-05\n",
      "Epoch 177/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8276 - loss: 0.3964\n",
      "Epoch 177: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 169ms/step - accuracy: 0.8276 - loss: 0.3963 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 5.6377e-05\n",
      "Epoch 178/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8270 - loss: 0.3970\n",
      "Epoch 178: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8270 - loss: 0.3970 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 4.2283e-05\n",
      "Epoch 179/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8264 - loss: 0.3930\n",
      "Epoch 179: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 162ms/step - accuracy: 0.8264 - loss: 0.3930 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 4.2283e-05\n",
      "Epoch 180/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8244 - loss: 0.3956\n",
      "Epoch 180: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 163ms/step - accuracy: 0.8244 - loss: 0.3956 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 4.2283e-05\n",
      "Epoch 181/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8185 - loss: 0.3927\n",
      "Epoch 181: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8185 - loss: 0.3927 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 3.1712e-05\n",
      "Epoch 182/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8254 - loss: 0.3922\n",
      "Epoch 182: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - accuracy: 0.8254 - loss: 0.3921 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 3.1712e-05\n",
      "Epoch 183/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.8225 - loss: 0.3948\n",
      "Epoch 183: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.8225 - loss: 0.3948 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 3.1712e-05\n",
      "Epoch 184/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.8197 - loss: 0.3996\n",
      "Epoch 184: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 182ms/step - accuracy: 0.8197 - loss: 0.3995 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 2.3784e-05\n",
      "Epoch 185/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8246 - loss: 0.3936\n",
      "Epoch 185: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8246 - loss: 0.3936 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 2.3784e-05\n",
      "Epoch 186/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8247 - loss: 0.3930\n",
      "Epoch 186: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - accuracy: 0.8247 - loss: 0.3930 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 2.3784e-05\n",
      "Epoch 187/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8232 - loss: 0.3949\n",
      "Epoch 187: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8232 - loss: 0.3949 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.7838e-05\n",
      "Epoch 188/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8246 - loss: 0.3961\n",
      "Epoch 188: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.8246 - loss: 0.3960 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.7838e-05\n",
      "Epoch 189/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.8183 - loss: 0.3954\n",
      "Epoch 189: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 177ms/step - accuracy: 0.8183 - loss: 0.3954 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.7838e-05\n",
      "Epoch 190/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8286 - loss: 0.3947\n",
      "Epoch 190: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.8285 - loss: 0.3947 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.3379e-05\n",
      "Epoch 191/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8205 - loss: 0.3939\n",
      "Epoch 191: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8205 - loss: 0.3939 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.3379e-05\n",
      "Epoch 192/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.8274 - loss: 0.3885\n",
      "Epoch 192: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - accuracy: 0.8274 - loss: 0.3885 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.3379e-05\n",
      "Epoch 193/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.8174 - loss: 0.4013\n",
      "Epoch 193: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8175 - loss: 0.4012 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.0034e-05\n",
      "Epoch 194/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.8281 - loss: 0.3965\n",
      "Epoch 194: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.8281 - loss: 0.3965 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.0034e-05\n",
      "Epoch 195/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8233 - loss: 0.3912\n",
      "Epoch 195: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.8233 - loss: 0.3912 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 1.0034e-05\n",
      "Epoch 196/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8202 - loss: 0.3977\n",
      "Epoch 196: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8203 - loss: 0.3977 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 7.5254e-06\n",
      "Epoch 197/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8215 - loss: 0.3977\n",
      "Epoch 197: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.8215 - loss: 0.3976 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 7.5254e-06\n",
      "Epoch 198/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8179 - loss: 0.3964\n",
      "Epoch 198: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - accuracy: 0.8179 - loss: 0.3964 - val_accuracy: 0.7777 - val_loss: 0.4782 - learning_rate: 7.5254e-06\n",
      "Epoch 199/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8241 - loss: 0.3979\n",
      "Epoch 199: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - accuracy: 0.8241 - loss: 0.3978 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 5.6441e-06\n",
      "Epoch 200/200\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8242 - loss: 0.3945\n",
      "Epoch 200: val_accuracy did not improve from 0.77767\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.8242 - loss: 0.3945 - val_accuracy: 0.7767 - val_loss: 0.4782 - learning_rate: 5.6441e-06\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\"adagrad\", 200, 64, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7717 - loss: 0.4819\n",
      "Test accuracy: 0.7823639512062073\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"model_combined.keras\")\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_59\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_59\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_43        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">40,000,100</span> │ input_layer_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,872</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,872</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">31,872</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_77          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_78          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_79          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_82          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_83          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,792</span> │ dropout_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ dropout_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,216</span> │ dropout_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dropout_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ dropout_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_55    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,872</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_56    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_55… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_17         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_55… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_56… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_18         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_56… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ get_item_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ get_item_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ concatenate_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_43        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │ \u001b[38;5;34m40,000,100\u001b[0m │ input_layer_42[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_41 (\u001b[38;5;33mGRU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m31,872\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_42 (\u001b[38;5;33mGRU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m31,872\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_43 (\u001b[38;5;33mGRU\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m31,872\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_29 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m42,240\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_30 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m42,240\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_31 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m42,240\u001b[0m │ embedding_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_77          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ gru_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_78          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ gru_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_79          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ gru_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_81          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_82          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_83          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m49,792\u001b[0m │ dropout_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ dropout_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m33,216\u001b[0m │ dropout_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dropout_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ dropout_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_55    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m7,872\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_56    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m10,368\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_55… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_17         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_55… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_56… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_18         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_56… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ get_item_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ get_item_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ concatenate_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,647,628</span> (307.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,647,628\u001b[0m (307.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,323,813</span> (153.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,323,813\u001b[0m (153.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,323,815</span> (153.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m40,323,815\u001b[0m (153.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       " 'effective but too-tepid biopic',\n",
       " 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       " \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\",\n",
       " 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .',\n",
       " 'offers that rare combination of entertainment and education .',\n",
       " 'perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .',\n",
       " \"steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\",\n",
       " 'take care of my cat offers a refreshingly different slice of asian cinema .',\n",
       " 'this is a film well worth seeing , talking and singing heads and all .',\n",
       " 'what really surprises about wisegirls is its low-key quality and genuine tenderness .',\n",
       " '( wendigo is ) why we go to the cinema : to be fed through the eye , the heart , the mind .',\n",
       " 'one of the greatest family-oriented , fantasy-adventure movies ever .',\n",
       " 'ultimately , it ponders the reasons we need stories so much .',\n",
       " \"an utterly compelling 'who wrote it' in which the reputation of the most famous author who ever lived comes into question .\",\n",
       " 'illuminating if overly talky documentary .',\n",
       " 'a masterpiece four years in the making .',\n",
       " \"the movie's ripe , enrapturing beauty will tempt those willing to probe its inscrutable mysteries .\",\n",
       " 'offers a breath of the fresh air of true sophistication .',\n",
       " 'a thoughtful , provocative , insistently humanizing film .',\n",
       " 'with a cast that includes some of the top actors working in independent film , lovely & amazing involves us because it is so incisive , so bleakly amusing about how we go about our lives .',\n",
       " 'a disturbing and frighteningly evocative assembly of imagery and hypnotic music composed by philip glass .',\n",
       " \"not for everyone , but for those with whom it will connect , it's a nice departure from standard moviegoing fare .\",\n",
       " 'scores a few points for doing what it does with a dedicated and good-hearted professionalism .',\n",
       " \"occasionally melodramatic , it's also extremely effective .\",\n",
       " 'spiderman rocks',\n",
       " 'an idealistic love story that brings out the latent 15-year-old romantic in everyone .',\n",
       " \"at about 95 minutes , treasure planet maintains a brisk pace as it races through the familiar story . however , it lacks grandeur and that epic quality often associated with stevenson's tale as well as with earlier disney efforts .\",\n",
       " 'it helps that lil bow wow . . . tones down his pint-sized gangsta act to play someone who resembles a real kid .',\n",
       " 'guaranteed to move anyone who ever shook , rattled , or rolled .',\n",
       " 'a masterful film from a master filmmaker , unique in its deceptive grimness , compelling in its fatalist worldview .',\n",
       " 'light , cute and forgettable .',\n",
       " \"if there's a way to effectively teach kids about the dangers of drugs , i think it's in projects like the ( unfortunately r-rated ) paid .\",\n",
       " \"while it would be easy to give crush the new title of two weddings and a funeral , it's a far more thoughtful film than any slice of hugh grant whimsy .\",\n",
       " 'though everything might be literate and smart , it never took off and always seemed static .',\n",
       " \"cantet perfectly captures the hotel lobbies , two-lane highways , and roadside cafes that permeate vincent's days\",\n",
       " 'ms . fulford-wierzbicki is almost spooky in her sulky , calculating lolita turn .',\n",
       " 'though it is by no means his best work , laissez-passer is a distinguished and distinctive effort by a bona-fide master , a fascinating film replete with rewards to be had by all willing to make the effort to reap them .',\n",
       " 'like most bond outings in recent years , some of the stunts are so outlandish that they border on being cartoonlike . a heavy reliance on cgi technology is beginning to creep into the series .',\n",
       " 'newton draws our attention like a magnet , and acts circles around her better known co-star , mark wahlberg .',\n",
       " \"the story loses its bite in a last-minute happy ending that's even less plausible than the rest of the picture . much of the way , though , this is a refreshingly novel ride .\",\n",
       " 'fuller would surely have called this gutsy and at times exhilarating movie a great yarn .',\n",
       " \"'compleja e intelectualmente retadora , el ladrón de orquídeas es uno de esos filmes que vale la pena ver precisamente por su originalidad . '\",\n",
       " 'the film makes a strong case for the importance of the musicians in creating the motown sound .',\n",
       " 'karmen moves like rhythm itself , her lips chanting to the beat , her long , braided hair doing little to wipe away the jeweled beads of sweat .',\n",
       " 'gosling provides an amazing performance that dwarfs everything else in the film .',\n",
       " \"a real movie , about real people , that gives us a rare glimpse into a culture most of us don't know .\",\n",
       " 'tender yet lacerating and darkly funny fable .',\n",
       " \"may be spoofing an easy target -- those old '50's giant creature features -- but . . . it acknowledges and celebrates their cheesiness as the reason why people get a kick out of watching them today .\",\n",
       " \"an engaging overview of johnson's eccentric career .\",\n",
       " 'in its ragged , cheap and unassuming way , the movie works .',\n",
       " \"some actors have so much charisma that you'd be happy to listen to them reading the phone book . hugh grant and sandra bullock are two such likeable actors .\",\n",
       " 'sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .',\n",
       " 'behind the snow games and lovable siberian huskies ( plus one sheep dog ) , the picture hosts a parka-wrapped dose of heart .',\n",
       " 'everytime you think undercover brother has run out of steam , it finds a new way to surprise and amuse .',\n",
       " 'manages to be original , even though it rips off many of its ideas .',\n",
       " 'singer/composer bryan adams contributes a slew of songs \\x97 a few potential hits , a few more simply intrusive to the story \\x97 but the whole package certainly captures the intended , er , spirit of the piece .',\n",
       " \"you'd think by now america would have had enough of plucky british eccentrics with hearts of gold . yet the act is still charming here .\",\n",
       " 'whether or not you\\'re enlightened by any of derrida\\'s lectures on \" the other \" and \" the self , \" derrida is an undeniably fascinating and playful fellow .',\n",
       " 'a pleasant enough movie , held together by skilled ensemble actors .',\n",
       " \"this is the best american movie about troubled teens since 1998's whatever .\",\n",
       " \"disney has always been hit-or-miss when bringing beloved kids' books to the screen . . . tuck everlasting is a little of both .\",\n",
       " 'just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .',\n",
       " 'the animated subplot keenly depicts the inner struggles of our adolescent heroes - insecure , uncontrolled , and intense .',\n",
       " 'the invincible werner herzog is alive and well and living in la',\n",
       " 'morton is a great actress portraying a complex character , but morvern callar grows less compelling the farther it meanders from its shocking start .',\n",
       " 'part of the charm of satin rouge is that it avoids the obvious with humour and lightness .',\n",
       " 'son of the bride may be a good half-hour too long but comes replete with a flattering sense of mystery and quietness .',\n",
       " 'a simmering psychological drama in which the bursts of sudden violence are all the more startling for the slow buildup that has preceded them .',\n",
       " 'a taut , intelligent psychological drama .',\n",
       " \"a compelling coming-of-age drama about the arduous journey of a sensitive young girl through a series of foster homes and a fierce struggle to pull free from her dangerous and domineering mother's hold over her .\",\n",
       " 'a truly moving experience , and a perfect example of how art -- when done right -- can help heal , clarify , and comfort .',\n",
       " 'this delicately observed story , deeply felt and masterfully stylized , is a triumph for its maverick director .',\n",
       " 'at heart the movie is a deftly wrought suspense yarn whose richer shadings work as coloring rather than substance .',\n",
       " \"the appearance of treebeard and gollum's expanded role will either have you loving what you're seeing , or rolling your eyes . i loved it ! gollum's 'performance' is incredible !\",\n",
       " 'a screenplay more ingeniously constructed than \" memento \"',\n",
       " \"if this movie were a book , it would be a page-turner , you can't wait to see what happens next .\",\n",
       " 'haneke challenges us to confront the reality of sexual aberration .',\n",
       " 'absorbing and disturbing -- perhaps more disturbing than originally intended -- but a little clarity would have gone a long way .',\n",
       " \"it's the best film of the year so far , the benchmark against which all other best picture contenders should be measured .\",\n",
       " \"painful to watch , but viewers willing to take a chance will be rewarded with two of the year's most accomplished and riveting film performances .\",\n",
       " 'this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border .',\n",
       " 'an imaginative comedy/thriller .',\n",
       " 'a few artsy flourishes aside , narc is as gritty as a movie gets these days .',\n",
       " 'while the isle is both preposterous and thoroughly misogynistic , its vistas are incredibly beautiful to look at .',\n",
       " \"together , tok and o orchestrate a buoyant , darkly funny dance of death . in the process , they demonstrate that there's still a lot of life in hong kong cinema .\",\n",
       " 'director kapur is a filmmaker with a real flair for epic landscapes and adventure , and this is a better film than his earlier english-language movie , the overpraised elizabeth .',\n",
       " 'the movie is a blast of educational energy , as bouncy animation and catchy songs escort you through the entire 85 minutes .',\n",
       " \"a sports movie with action that's exciting on the field and a story you care about off it .\",\n",
       " \"doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie's spycraft and uses damon's ability to be focused and sincere .\",\n",
       " 'the tenderness of the piece is still intact .',\n",
       " 'katz uses archival footage , horrifying documents of lynchings , still photographs and charming old reel-to-reel recordings of meeropol entertaining his children to create his song history , but most powerful of all is the song itself',\n",
       " \"like the film's almost anthropologically detailed realization of early-'80s suburbia , it's significant without being overstated .\",\n",
       " \"while mcfarlane's animation lifts the film firmly above the level of other coming-of-age films . . . it's also so jarring that it's hard to get back into the boys' story .\",\n",
       " 'if nothing else , this movie introduces a promising , unusual kind of psychological horror .',\n",
       " 'in a normal screen process , these bromides would be barely enough to sustain an interstitial program on the discovery channel . but in imax 3-d , the clichés disappear into the vertiginous perspectives opened up by the photography .',\n",
       " 'writer-director burger imaginatively fans the embers of a dormant national grief and curiosity that has calcified into chronic cynicism and fear .',\n",
       " '. . . a roller-coaster ride of a movie',\n",
       " 'i enjoyed time of favor while i was watching it , but i was surprised at how quickly it faded from my memory .',\n",
       " 'chicago is sophisticated , brash , sardonic , completely joyful in its execution .',\n",
       " \"steve irwin's method is ernest hemmingway at accelerated speed and volume .\",\n",
       " 'a refreshing korean film about five female high school friends who face an uphill battle when they try to take their relationships into deeper waters .',\n",
       " \"on the surface , it's a lovers-on-the-run crime flick , but it has a lot in common with piesiewicz's and kieslowski's earlier work , films like the double life of veronique .\",\n",
       " 'the values that have held the enterprise crew together through previous adventures and perils do so again-courage , self-sacrifice and patience under pressure .',\n",
       " \"if it's possible for a sequel to outshine the original , then sl2 does just that .\",\n",
       " 'a romantic comedy that operates by the rules of its own self-contained universe .',\n",
       " \"4 friends , 2 couples , 2000 miles , and all the pabst blue ribbon beer they can drink - it's the ultimate redneck road-trip .\",\n",
       " \"the film is often filled with a sense of pure wonderment and excitement not often seen in today's cinema du sarcasm\",\n",
       " \"it might be tempting to regard mr . andrew and his collaborators as oddballs , but mr . earnhart's quizzical , charming movie allows us to see them , finally , as artists .\",\n",
       " 'a feel-good picture in the best sense of the term .',\n",
       " 'edited and shot with a syncopated style mimicking the work of his subjects , pray turns the idea of the documentary on its head , making it rousing , invigorating fun lacking any mtv puffery .',\n",
       " 'a mostly intelligent , engrossing and psychologically resonant suspenser .',\n",
       " \"it's this memory-as-identity obviation that gives secret life its intermittent unease , reaffirming that long-held illusions are indeed reality , and that erasing them recasts the self .\",\n",
       " \"hip-hop has a history , and it's a metaphor for this love story .\",\n",
       " \"in scope , ambition and accomplishment , children of the century . . . takes kurys' career to a whole new level .\",\n",
       " \"this may not have the dramatic gut-wrenching impact of other holocaust films , but it's a compelling story , mainly because of the way it's told by the people who were there .\",\n",
       " 'between the drama of cube ? s personal revelations regarding what the shop means in the big picture , iconic characters gambol fluidly through the story , with charming results .',\n",
       " 'a gentle , compassionate drama about grief and healing .',\n",
       " 'somewhere short of tremors on the modern b-scene : neither as funny nor as clever , though an agreeably unpretentious way to spend ninety minutes .',\n",
       " 'digital-video documentary about stand-up comedians is a great glimpse into a very different world .',\n",
       " 'unlike most teen flicks , swimming takes its time to tell its story , casts mostly little-known performers in key roles , and introduces some intriguing ambiguity .',\n",
       " \"an enthralling , playful film that constantly frustrates our desire to know the 'truth' about this man , while deconstructing the very format of the biography in a manner that derrida would doubtless give his blessing to .\",\n",
       " '\" extreme ops \" exceeds expectations . good fun , good action , good acting , good dialogue , good pace , good cinematography .',\n",
       " 'you should pay nine bucks for this : because you can hear about suffering afghan refugees on the news and still be unaffected . dramas like this make it human .',\n",
       " 'a thunderous ride at first , quiet cadences of pure finesse are few and far between ; their shortage dilutes the potency of otherwise respectable action . still , this flick is fun , and host to some truly excellent sequences .',\n",
       " \"it's obviously struck a responsive chord with many south koreans , and should work its magic in other parts of the world .\",\n",
       " \"run , don't walk , to see this barbed and bracing comedy on the big screen .\",\n",
       " 'a classy item by a legend who may have nothing left to prove but still has the chops and drive to show how its done .',\n",
       " \"it is nature against progress . in fessenden's horror trilogy , this theme has proved important to him and is especially so in the finale .\",\n",
       " \"it's not exactly a gourmet meal but the fare is fair , even coming from the drive-thru .\",\n",
       " 'this is what imax was made for : strap on a pair of 3-d goggles , shut out the real world , and take a vicarious voyage to the last frontier -- space .',\n",
       " 'merely as a technical , logistical feat , russian ark marks a cinematic milestone .',\n",
       " '[schweiger is] talented and terribly charismatic , qualities essential to both movie stars and social anarchists .',\n",
       " \"it's a great deal of sizzle and very little steak . but what spectacular sizzle it is ! . . . in this incarnation its fizz is infectious .\",\n",
       " 'an original gem about an obsession with time .',\n",
       " 'it will delight newcomers to the story and those who know it from bygone days .',\n",
       " 'gloriously goofy ( and gory ) midnight movie stuff .',\n",
       " 'the film overcomes the regular minefield of coming-of-age cliches with potent doses of honesty and sensitivity .',\n",
       " \"if your senses haven't been dulled by slasher films and gorefests , if you're a connoisseur of psychological horror , this is your ticket .\",\n",
       " \"it's a minor comedy that tries to balance sweetness with coarseness , while it paints a sad picture of the singles scene .\",\n",
       " 'it is intensely personal and yet -- unlike quills -- deftly shows us the temper of the times .',\n",
       " 'as lo-fi as the special effects are , the folks who cobbled nemesis together indulge the force of humanity over hardware in a way that george lucas has long forgotten .',\n",
       " \"like mike doesn't win any points for originality . it does succeed by following a feel-good formula with a winning style , and by offering its target audience of urban kids some welcome role models and optimism .\",\n",
       " \"it's a hoot and a half , and a great way for the american people to see what a candidate is like when he's not giving the same 15-cent stump speech .\",\n",
       " 'far from perfect , but its heart is in the right place . . . innocent and well-meaning .',\n",
       " 'a sad , superior human comedy played out on the back roads of life .',\n",
       " 'waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality .',\n",
       " 'tim allen is great in his role but never hogs the scenes from his fellow cast , as there are plenty of laughs and good lines for everyone in this comedy .',\n",
       " 'more a load of enjoyable , conan-esque claptrap than the punishing , special-effects soul assaults the mummy pictures represent .',\n",
       " 'enormously likable , partly because it is aware of its own grasp of the absurd .',\n",
       " \"here's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\",\n",
       " \"it's an old story , but a lively script , sharp acting and partially animated interludes make just a kiss seem minty fresh .\",\n",
       " 'must be seen to be believed .',\n",
       " \"ray liotta and jason patric do some of their best work in their underwritten roles , but don't be fooled : nobody deserves any prizes here .\",\n",
       " \"everything that has to do with yvan and charlotte , and everything that has to do with yvan's rambunctious , jewish sister and her non-jew husband , feels funny and true .\",\n",
       " 'sweet home alabama \" is what it is \\x96 a nice , harmless date film . . .',\n",
       " \"the year's happiest surprise , a movie that deals with a real subject in an always surprising way .\",\n",
       " \"fans of behan's work and of irish movies in general will be rewarded by borstal boy .\",\n",
       " \"its mysteries are transparently obvious , and it's too slowly paced to be a thriller . [but it's] worth recommending because of two marvelous performances by michael caine and brendan fraser .\",\n",
       " \"the film is faithful to what one presumes are the book's twin premises -- that we become who we are on the backs of our parents , but we have no idea who they were at our age ; and that time is a fleeting and precious commodity no matter how old you are .\",\n",
       " \"stephen earnhart's homespun documentary mule skinner blues has nothing but love for its posse of trailer park denizens .\",\n",
       " 'a solidly seaworthy chiller .',\n",
       " 'if you can get past the fantastical aspects and harsh realities of \" the isle \" you\\'ll get a sock-you-in-the-eye flick that is a visual tour-de-force and a story that is unlike any you will likely see anywhere else .',\n",
       " \"there are as many misses as hits , but ultimately , it finds humor in the foibles of human behavior , and it's a welcome return to the roots of a genre that should depend on surprises .\",\n",
       " 'a well-made thriller with a certain level of intelligence and non-reactionary morality .',\n",
       " \"there's enough science to make it count as educational , and enough beauty to make it unforgettable .\",\n",
       " 'remains a solid , if somewhat heavy-handed , account of the near-disaster . . . done up by howard with a steady , if not very imaginative , hand .',\n",
       " \"makmalbaf follows a resolutely realistic path in this uncompromising insight into the harsh existence of the kurdish refugees of iran's borderlands .\",\n",
       " 'for a good chunk of its running time , trapped is an effective and claustrophobic thriller .',\n",
       " 'most of crush is a clever and captivating romantic comedy with a welcome pinch of tartness .',\n",
       " 'nair does capture the complexity of a big family and its trials and tribulations . . .',\n",
       " 'the seaside splendor and shallow , beautiful people are nice to look at while you wait for the story to get going .',\n",
       " \"rare is the 'urban comedy' that even attempts the insight and honesty of this disarming indie .\",\n",
       " \"ranks among willams' best screen work .\",\n",
       " 'engagingly captures the maddening and magnetic ebb and flow of friendship .',\n",
       " 'an experience so engrossing it is like being buried in a new environment .',\n",
       " \"it's traditional moviemaking all the way , but it's done with a lot of careful period attention as well as some very welcome wit .\",\n",
       " 'thoroughly enjoyable .',\n",
       " \"maybe it's just because this past year has seen the release of some of the worst film comedies in decades . . . but honestly , analyze that really isn't all that bad .\",\n",
       " 'a droll , well-acted , character-driven comedy with unexpected deposits of feeling .',\n",
       " \"this is simply the most fun you'll ever have with a documentary !\",\n",
       " 'a very funny movie .',\n",
       " \"watching haneke's film is , aptly enough , a challenge and a punishment . but watching huppert , a great actress tearing into a landmark role , is riveting .\",\n",
       " 'a cop story that understands the medium amazingly well .',\n",
       " \"one of the best , most understated performances of [jack nicholson's] career .\",\n",
       " 'britney has been delivered to the big screen safe and sound , the way we like our 20-year-old superstar girls to travel on the fame freeway .',\n",
       " \"those outside show business will enjoy a close look at people they don't really want to know .\",\n",
       " 'the kind of nervous film that will either give you a mild headache or exhilarate you .',\n",
       " 'watching beanie and his gang put together his slasher video from spare parts and borrowed materials is as much fun as it must have been for them to make it .',\n",
       " \"children may not understand everything that happens -- i'm not sure even miyazaki himself does -- but they will almost certainly be fascinated , and undoubtedly delighted .\",\n",
       " 'a fascinating and fun film .',\n",
       " 'tadpole is a sophisticated , funny and good-natured treat , slight but a pleasure .',\n",
       " 'this insightful , oscar-nominated documentary , in which children on both sides of the ever-escalating conflict have their say away from watchful parental eyes , gives peace yet another chance .',\n",
       " 'i admired this work a lot .',\n",
       " \"whether you're moved and love it , or bored or frustrated by the film , you'll still feel something .\",\n",
       " '. . . there are enough moments of heartbreaking honesty to keep one glued to the screen .',\n",
       " 'my goodness , queen latifah has a lot to offer and she seemed to have no problem flaunting her natural gifts . she must have a very strong back .',\n",
       " 'a smart , sweet and playful romantic comedy .',\n",
       " 'australian actor/director john polson and award-winning english cinematographer giles nuttgens make a terrific effort at disguising the obvious with energy and innovation .',\n",
       " 'without heavy-handedness , dong provides perspective with his intelligent grasp of human foibles and contradictions .',\n",
       " 'solid , lump-in-the-throat family entertainment that derives its power by sticking to the facts .',\n",
       " 'as an entertainment , the movie keeps you diverted and best of all , it lightens your wallet without leaving a sting .',\n",
       " 'it is interesting and fun to see goodall and her chimpanzees on the bigger-than-life screen .',\n",
       " \"it won't bust your gut -- and it's not intended to -- it's merely a blandly cinematic surgical examination of what makes a joke a joke .\",\n",
       " 'a somewhat crudely constructed but gripping , questing look at a person so racked with self-loathing , he becomes an enemy to his own race .',\n",
       " 'it extends the writings of jean genet and john rechy , the films of fassbinder , perhaps even the nocturnal works of goya .',\n",
       " \"narc may not get an 'a' for originality , but it wears its b-movie heritage like a badge of honor .\",\n",
       " \"with the film's striking ending , one realizes that we have a long way to go before we fully understand all the sexual permutations involved .\",\n",
       " \"[drumline] is entertaining for what it does , and admirable for what it doesn't do .\",\n",
       " 'at its best early on as it plays the culture clashes between the brothers .',\n",
       " '[a] rare , beautiful film .',\n",
       " 'an unabashedly schmaltzy and thoroughly enjoyable true story .',\n",
       " 'a thoughtful look at a painful incident that made headlines in 1995 .',\n",
       " 'you walk out of the good girl with mixed emotions \\x97 disapproval of justine combined with a tinge of understanding for her actions .',\n",
       " 'tsai ming-liang has taken his trademark style and refined it to a crystalline point .',\n",
       " 'purely propaganda , a work of unabashed hero worship , it is nonetheless -- and likely inadvertently -- a timely and invaluable implicit reminder of the role that u . s . foreign policy has played in the rise of castro .',\n",
       " 'now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism .',\n",
       " \"we get some truly unique character studies and a cross-section of americana that hollywood couldn't possibly fictionalize and be believed .\",\n",
       " 'though this film can be clumsy , its ambitions are equally -- and admirably -- uncommercial .',\n",
       " 'daring , mesmerizing and exceedingly hard to forget .',\n",
       " \"the dangerous lives of altar boys' take on adolescence feels painfully true .\",\n",
       " \"moore's performance impresses almost as much as her work with haynes in 1995's safe .\",\n",
       " \"visits spy-movie territory like a novel you can't put down , examines a footnote to history seldom brought to light on the screen , and keeps you guessing from first frame to last .\",\n",
       " 'an absorbing , slice-of-depression life that touches nerves and rings true .',\n",
       " 'mr . parker has brilliantly updated his source and grasped its essence , composing a sorrowful and hilarious tone poem about alienated labor , or an absurdist workplace sitcom .',\n",
       " 'the result is something quite fresh and delightful .',\n",
       " 'all but the most persnickety preteens should enjoy this nonthreatening but thrilling adventure .',\n",
       " \"despite its many infuriating flaws -- not the least of which is amy's self-absorbed personality -- amy's o's honesty will win you over .\",\n",
       " \"this is one of polanski's best films .\",\n",
       " 'day is not a great bond movie , but it is a good bond movie , which still makes it much better than your typical bond knock-offs .',\n",
       " 'polished korean political-action film is just as good -- and bad -- as hollywood action epics . is this progress ?',\n",
       " 'elling , portrayed with quiet fastidiousness by per christian ellefsen , is a truly singular character , one whose frailties are only slightly magnified versions of the ones that vex nearly everyone .',\n",
       " \"denis and co-writer michele petin's impeccable screenplay penetrates with a rawness that that is both unflinching and tantalizing . lead provocatuers testud and parmentier give superlative performances\",\n",
       " 'an absorbing trip into the minds and motivations of people under stress as well as a keen , unsentimental look at variations on the theme of motherhood .',\n",
       " 'i admired it , particularly that unexpected downer of an ending .',\n",
       " 'the passions aroused by the discord between old and new cultures are set against the strange , stark beauty of the mideast desert , so lovingly and perceptively filmed that you can almost taste the desiccated air .',\n",
       " 'remarkably accessible and affecting .',\n",
       " 'never mind whether you buy the stuff about barris being a cia hit man . the kooky yet shadowy vision clooney sustains throughout is daring , inventive and impressive .',\n",
       " 'a triumph of art direction over narrative , but what art direction !',\n",
       " \"behan himself knew how to spin a tale and one can't help but think he'd appreciate this attempt to turn his life into art .\",\n",
       " \"jirí hubac's script is a gem . his characters are engaging , intimate and the dialogue is realistic and greatly moving . the scope of the silberstein family is large and we grow attached to their lives , full of strength , warmth and vitality . .\",\n",
       " \"moore's complex and important film is also , believe it or not , immensely entertaining , a david and goliath story that's still very much playing itself out .\",\n",
       " \"the additional storyline is interesting and entertaining , but it doesn't have the same magical quality as the beginning of the story . i like the new footage and still love the old stuff .\",\n",
       " 'though mama takes a bit too long to find its rhythm and a third-act plot development is somewhat melodramatic , its ribald humor and touching nostalgia are sure to please anyone in search of a jules and jim for the new millennium .',\n",
       " \"you might not buy the ideas . but you'll definitely want the t-shirt .\",\n",
       " 'provides an intriguing window into the imagination and hermetic analysis of todd solondz .',\n",
       " 'windtalkers is shapelessly gratifying , the kind of movie that invites you to pick apart its faults even as you have to admit that somehow it hit you where you live .',\n",
       " 'presents an astute appraisal of middle american musical torpor and the desperate struggle to escape it .',\n",
       " 'just what makes us happy , anyway ?',\n",
       " 'a thoughtful , moving piece that faces difficult issues with honesty and beauty .',\n",
       " 'one of the greatest romantic comedies of the past decade .',\n",
       " \"you wouldn't call the good girl a date movie ( an anti-date movie is more like it ) , but when it's good , it's good and horrid .\",\n",
       " \"benefits from a strong performance from zhao , but it's dong jie's face you remember at the end .\",\n",
       " 'this is a film brimming with detail and nuance and one that speaks volumes about the ability of the human spirit to find solace in events that could easily crush it forever .',\n",
       " 'the director , steven shainberg , has succeeded by focusing intently on his characters , making them quirky individuals rather than figures of fun .',\n",
       " \"it ultimately stands forth as an important chronicle of the abuses of one of latin america's most oppressive regimes .\",\n",
       " 'the movie has a soft , percolating magic , a deadpan suspense .',\n",
       " 'a well-made and often lovely depiction of the mysteries of friendship .',\n",
       " 'using his audience as a figurative port-of-call , dong pulls his even-handed ideological ship to their dock for unloading , before he continues his longer journey still ahead .',\n",
       " '. . . understands that a generation defines its music as much as the music defines a generation .',\n",
       " 'the transporter is as lively and as fun as it is unapologetically dumb',\n",
       " \"as a witness to several greek-american weddings -- but , happily , a victim of none -- i can testify to the comparative accuracy of ms . vardalos' memories and insights .\",\n",
       " 'has it ever been possible to say that williams has truly inhabited a character ? it is now .',\n",
       " \"by presenting an impossible romance in an impossible world , pumpkin dares us to say why either is impossible -- which forces us to confront what's possible and what we might do to make it so .\",\n",
       " 'an impressive debut for first-time writer-director mark romanek , especially considering his background is in music video .',\n",
       " 'an incendiary , deeply thought-provoking look at one of the most peculiar ( and peculiarly venomous ) bigotries in our increasingly frightening theocracy',\n",
       " 'all the performances are top notch and , once you get through the accents , all or nothing becomes an emotional , though still positive , wrench of a sit .',\n",
       " '\" its successes are also tempered with elements which prove the direct antithesis of what it gets right . \"',\n",
       " \"it's solid and affecting and exactly as thought-provoking as it should be .\",\n",
       " \"this is such a dazzlingly self-assured directorial debut that it's hard to know what to praise first .\",\n",
       " \"parker holds true to wilde's own vision of a pure comedy with absolutely no meaning , and no desire to be anything but a polished , sophisticated entertainment that is in love with its own cleverness .\",\n",
       " \"münch's genuine insight makes the film's occasional overindulgence forgivable .\",\n",
       " 'thankfully , the film , which skirts that rapidly deteriorating line between fantasy and reality . . . takes a tongue-in-cheek attitude even as it pushes the croc hunter agenda .',\n",
       " \"ultimately , the message of trouble every day seems to be that all sexual desire disrupts life's stasis .\",\n",
       " \"if you're like me , a sucker for a good old fashion romance and someone who shamelessly loves to eat , then mostly martha offers all the perfect ingredients to more than satisfy your appetite .\",\n",
       " 'the film has just enough of everything -- re-enactments , archival footage , talking-head interviews -- and the music is simply sublime .',\n",
       " 'there are a few stabs at absurdist comedy . . . but mostly the humor is of the sweet , gentle and occasionally cloying kind that has become an iranian specialty .',\n",
       " 'a wonderful character-based comedy .',\n",
       " 'it would be interesting to hear from the other side , but in talk to her , the women are down for the count .',\n",
       " 'an endearingly offbeat romantic comedy with a great meet-cute gimmick .',\n",
       " 'the unique tug-of-war with viewer expectations is undeniable , if not a pleasure in its own right .',\n",
       " \"it uses an old-time formula , it's not terribly original and it's rather messy -- but you just have to love the big , dumb , happy movie my big fat greek wedding .\",\n",
       " \"it's almost impossible not to be moved by the movie's depiction of sacrifice and its stirring epilogue in post-soviet russia .\",\n",
       " \"who knows what exactly godard is on about in this film , but his words and images don't have to add up to mesmerize you .\",\n",
       " 'the tone is balanced , reflective and reasonable .',\n",
       " 'the principals in this cast are all fine , but bishop and stevenson are standouts .',\n",
       " 'it could change america , not only because it is full of necessary discussion points , but because it is so accessible that it makes complex politics understandable to viewers looking for nothing but energetic entertainment .',\n",
       " \"what's most striking about this largely celebratory film . . . is the sense of isolation that permeates these bastions of individuality in an ikea world .\",\n",
       " \". . . if you're in a mind set for goofy comedy , the troopers will entertain with their gross outs , bawdy comedy and head games .\",\n",
       " \"somewhat blurred , but kinnear's performance is razor sharp .\",\n",
       " 'as a director , mr . ratliff wisely rejects the temptation to make fun of his subjects .',\n",
       " \"for anyone who remembers the '60s or is interested in one man's response to stroke , ram dass : fierce grace is worth seeking out .\",\n",
       " 'intriguing and beautiful film , but those of you who read the book are likely to be disappointed .',\n",
       " 'the new guy does have a heart . now , if it only had a brain .',\n",
       " \"a savvy exploration of paranoia and insecurity in america's culture of fear .\",\n",
       " \"legendary irish writer brendan behan's memoir , borstal boy , has been given a loving screen transferral .\",\n",
       " \"the film's greatest asset is how much it's not just another connect-the-dots , spy-on-the-run picture .\",\n",
       " 'this clever caper movie has twists worthy of david mamet and is enormous fun for thinking audiences .',\n",
       " \"it's one of the saddest films i have ever seen that still manages to be uplifting but not overly sentimental .\",\n",
       " 'morton is , as usual , brilliant .',\n",
       " 'even with all those rough edges safely sanded down , the american insomnia is still pretty darned good .',\n",
       " \"i don't know precisely what to make of steven soderbergh's full frontal , though that didn't stop me from enjoying much of it .\",\n",
       " 'the tug of war that ensues is as much a snapshot of modern china in microcosm as it is a crash course in movie mythology .',\n",
       " \"nearly surreal , dabbling in french , this is no simple movie , and you'll be taking a risk if you choose to see it . i enjoyed the ride ( bumps and all ) , creamy depth , and ultimate theme .\",\n",
       " \"you could say that it's slow at times , you could say that a few of the characters act in ways that real people wouldn't , but one thing you couldn't say is that alias betty is predictable .\",\n",
       " 'asia authors herself as anna battista , an italian superstar and aspiring directress who just happens to be her own worst enemy .',\n",
       " \"roman coppola may never become the filmmaker his dad was , but heck \\x96 few filmmakers will . but based on cq , i'll certainly be keeping an eye out for his next project .\",\n",
       " 'an amusing , breezily apolitical documentary about life on the campaign trail .',\n",
       " \"high on melodrama . but it's emotionally engrossing , too , thanks to strong , credible performances from the whole cast .\",\n",
       " 'finally , a genre movie that delivers -- in a couple of genres , no less .',\n",
       " \"it's not so much enjoyable to watch as it is enlightening to listen to new sides of a previous reality , and to visit with some of the people who were able to make an impact in the theater world .\",\n",
       " 'spielberg is the rare director who does not want to invite viewers to gawk at or applaud his special effects . he just wants them to be part of the action , the wallpaper of his chosen reality . here , thankfully , they are .',\n",
       " 'post 9/11 the philosophical message of \" personal freedom first \" might not be as palatable as intended .',\n",
       " 'hu and liu offer natural , matter-of-fact performances that glint with sorrow , longing and love .',\n",
       " 'this bold and lyrical first feature from raja amari expands the pat notion that middle-aged women just wanna have fun into a rousing treatise of sensual empowerment .',\n",
       " \"easier to respect than enthuse over , andersson's rigorous personal vision is not only distanced but distancing .\",\n",
       " 'girls gone wild and gone civil again',\n",
       " '. . . tunney is allowed to build an uncommonly human character , an almost real-live girl complete with trouble and hope .',\n",
       " 'while this film is not in the least surprising , it is still ultimately very satisfying . think of it as a sort of comfort food for the mind .',\n",
       " 'clever , brutal and strangely soulful movie .',\n",
       " '. . . always remains movingly genuine .',\n",
       " 'an intelligent fiction about learning through cultural clash .',\n",
       " 'will grab your children by the imagination and amaze them and amuse them .',\n",
       " 'a remarkable 179-minute meditation on the nature of revolution .',\n",
       " 'those who would follow haneke on his creepy explorations . . . are rewarded by brutal , committed performances from huppert and magimel .',\n",
       " 'an involving true story of a chinese actor who takes up drugs and winds up in an institution--acted mostly by the actual people involved .',\n",
       " \"hands down the year's most thought-provoking film . but it pays a price for its intricate intellectual gamesmanship .\",\n",
       " \"it's a terrific american sports movie and dennis quaid is its athletic heart .\",\n",
       " \"this is such a high-energy movie where the drumming and the marching are so excellent , who cares if the story's a little weak .\",\n",
       " 'compelling revenge thriller , though somewhat weakened by a miscast leading lady .',\n",
       " \"it's amazingly perceptive in its subtle , supportive but unsentimental look at the marks family .\",\n",
       " 'a whole lot foul , freaky and funny .',\n",
       " 'family fare .',\n",
       " 'attal mixes comedy with a serious exploration of ego and jealousy within a seemingly serene marriage .',\n",
       " 'the diversity of the artists represented , both in terms of style and ethnicity , prevents the proceedings from feeling repetitious , as does the appropriately brief 40-minute running time .',\n",
       " 'the pianist is a fine valedictory work for polanski , made richer by his own experiences , making his other movies somehow richer in the bargain .',\n",
       " 'foster nails the role , giving a tight , focused performance illuminated by shards of feeling .',\n",
       " 'even if you can\\'t pronounce \" gyro \" correctly , you\\'ll appreciate much of vardalos\\' humor , which transcends ethnic boundaries .',\n",
       " \"is office work really as alienating as 'bartleby' so effectively makes it ?\",\n",
       " \"farrell . . . thankfully manages to outshine the role and successfully plays the foil to willis's world-weary colonel .\",\n",
       " \"audiences conditioned to getting weepy over saucer-eyed , downy-cheeked moppets and their empathetic caretakers will probably feel emotionally cheated by the film's tart , sugar-free wit .\",\n",
       " \"bennett's dramatization of her personal descent into post-breakup perdition has a morbid appeal that's tough to shake .\",\n",
       " 'an intriguing and entertaining introduction to johnson .',\n",
       " \"as expected , sayles' smart wordplay and clever plot contrivances are as sharp as ever , though they may be overshadowed by some strong performances .\",\n",
       " 'a model of what films like this should be like .',\n",
       " \"as weber and weissman demonstrate with such insight and celebratory verve , the cockettes weren't as much about gender , sexual preference or political agitprop as they were simply a triumph of the indomitable human will to rebel , connect and create .\",\n",
       " \"yeah , these flicks are just that damn good . isn't it great ?\",\n",
       " 'an unbelievably fun film just a leading man away from perfection .',\n",
       " \"over-the-top and a bit ostentatious , this is a movie that's got oodles of style and substance .\",\n",
       " '. . . a poignant and powerful narrative that reveals that reading writing and arithmetic are not the only subjects to learn in life .',\n",
       " 'nicely serves as an examination of a society in transition .',\n",
       " 'boisterous , heartfelt comedy .',\n",
       " \"a tender and touching drama , based on the true story of a troubled african-american's quest to come to terms with his origins , reveals the yearning we all have in our hearts for acceptance within the family circle .\",\n",
       " 'as a randy film about sexy people in gorgeous places being pushed and pulled ( literally and figuratively ) by desire . . . [sex and lucía] makes for an arousing good time .',\n",
       " 'absorbing character study by andré turpin .',\n",
       " \"celebrated at sundance , this slight comedy of manners has winning performances and a glossy , glib charm that's hard to beat .\",\n",
       " \"renner's performance as dahmer is unforgettable , deeply absorbing .\",\n",
       " \"if no one singles out any of these performances as award-worthy , it's only because we would expect nothing less from this bunch .\",\n",
       " \"if you love reading and/or poetry , then by all means check it out . you'll probably love it .\",\n",
       " 'though of particular interest to students and enthusiast of international dance and world music , the film is designed to make viewers of all ages , cultural backgrounds and rhythmic ability want to get up and dance .',\n",
       " 'energetic and boldly provocative .',\n",
       " 'star wars is back in a major way .',\n",
       " \"it's a movie -- and an album -- you won't want to miss .\",\n",
       " \"it's rare to find a film that dazzles the eye , challenges the brain , and satisfies our lust for fast-paced action , but minority report delivers all that and a whole lot more .\",\n",
       " \"while not all transitions to adulthood are so fraught , there's much truth and no small amount of poetry in girls can't swim .\",\n",
       " \"if there's nothing fresh about wannabes , which was written by mr . demeo , who produced and directed the film with charles a . addessi , much of the time the movie feels authentic .\",\n",
       " \"jacquot's tosca is a treat .\",\n",
       " 'by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .',\n",
       " \"if you liked such movies as notting hill , four weddings and a funeral , bridget jones' diary or high fidelity , then you won't want to miss about a boy .\",\n",
       " '. . . the gentle melding of drama and comedy makes \" what time is it there ? \" something the true film buff will enjoy .',\n",
       " \"romanek keeps the film constantly taut . . . reflecting the character's instability with a metaphorical visual style and an unnerving , heartbeat-like score .\",\n",
       " 'i whole-heartedly recommend that everyone see this movie-- for its historical significance alone .',\n",
       " 'hey , who else needs a shower ?',\n",
       " 'longley has constructed a remarkably coherent , horrifically vivid snapshot of those turbulent days .',\n",
       " \"although it bangs a very cliched drum at times , this crowd-pleaser's fresh dialogue , energetic music , and good-natured spunk are often infectious .\",\n",
       " \"often gruelling and heartbreaking to witness , but seldahl and wollter's sterling performances raise this far above the level of the usual maudlin disease movie .\",\n",
       " 'go see it and enjoy .',\n",
       " 'the stunning , dreamlike visuals will impress even those viewers who have little patience for euro-film pretension .',\n",
       " \"george clooney proves he's quite a talented director and sam rockwell shows us he's a world-class actor with confessions of a dangerous mind .\",\n",
       " \"there's a vastness implied in metropolis that is just breathtaking .\",\n",
       " 'murderous maids may well be the most comprehensive of these films and also strike closest to the truth .',\n",
       " \"the people in dogtown and z-boys are so funny , aggressive and alive , you have to watch them because you can't wait to see what they do next .\",\n",
       " \"as green-guts monster movies go , it's a beaut .\",\n",
       " 'as bundy , michael reilly burke ( octopus 2 : river of fear ) has just the right amount of charisma and menace .',\n",
       " 'a deceivingly simple film , one that grows in power in retrospect .',\n",
       " \"ana is a vivid , vibrant individual and the movie's focus upon her makes it successful and accessible .\",\n",
       " 'a slick , skillful little horror film .',\n",
       " 'a very witty take on change , risk and romance , and the film uses humour to make its points about acceptance and growth .',\n",
       " '[anderson] uses a hit-or-miss aesthetic that hits often enough to keep the film entertaining even if none of it makes a lick of sense .',\n",
       " \"bubba ho-tep is a wonderful film with a bravura lead performance by bruce campbell that doesn't deserve to leave the building until everyone is aware of it .\",\n",
       " 'despite the long running time , the pace never feels slack -- there\\'s no scene that screams \" bathroom break ! \"',\n",
       " 'bullock does a good job here of working against her natural likability .',\n",
       " 'a film of precious increments artfully camouflaged as everyday activities .',\n",
       " 'kinnear gives a tremendous performance .',\n",
       " \"the best movie of its kind since 'brazil . ' lucas , take notes . this is how you use special effects .\",\n",
       " '\" frailty \" has been written so well , that even a simple \" goddammit ! \" near the end takes on a whole other meaning .',\n",
       " \"one hour photo is an intriguing snapshot of one man and his delusions ; it's just too bad it doesn't have more flashes of insight .\",\n",
       " 'kaufman creates an eerie sense of not only being there at the time of these events but the very night matthew was killed .',\n",
       " 'chalk it up to my adoration for both de niro and murphy , but i had a pretty good time with this movie - despite its myriad flaws .',\n",
       " 'its scenes and sensibility are all more than familiar , but it exudes a kind of nostalgic spy-movie charm and , at the same time , is so fresh and free of the usual thriller nonsense that it all seems to be happening for the first time .',\n",
       " \"it represents better-than-average movie-making that doesn't demand a dumb , distracted audience .\",\n",
       " 'a charming yet poignant tale of the irrevocable ties that bind .',\n",
       " 'an enchanting spectacular for potter fans anxious to ride the hogwarts express toward a new year of magic and mischief .',\n",
       " 'the talents of the actors helps \" moonlight mile \" rise above its heart-on-its-sleeve writing .',\n",
       " \"it's a humble effort , but spiced with wry humor and genuine pathos , especially between morgan and redgrave .\",\n",
       " 'this examination of aquatic life off the shores of the baja california peninsula of mexico offers an engrossing way to demonstrate the virtues of the imax format .',\n",
       " 'dark and disturbing , but also surprisingly funny .',\n",
       " 'the movie has an avalanche of eye-popping visual effects .',\n",
       " \"starts off with a bang , but then fizzles like a wet stick of dynamite at the very end . it's still worth a look .\",\n",
       " \"most impressive , though , is the film's open-ended finale that refuses to entirely close its characters' emotional wounds .\",\n",
       " 'a hip ride into hyper-time , clockstoppers is a lively and enjoyable adventure for all ages at any time .',\n",
       " \"grenier is terrific , bringing an unforced , rapid-fire delivery to toback's heidegger- and nietzsche-referencing dialogue .\",\n",
       " '. . . a polished and relatively sincere piece of escapism .',\n",
       " \"the story wraps back around on itself in the kind of elegant symmetry that's rare in film today , but be warned : it's a slow slog to get there .\",\n",
       " 'the whole cast looks to be having so much fun with the slapstick antics and silly street patois , tossing around obscure expressions like bellini and mullinski , that the compact 86 minutes breezes by .',\n",
       " '. . . has freaky scenes where the crew wonder if they\\'re ghosts imagining themselves as alive . it\\'s a sly wink to the others without becoming a postmodern joke , made creepy by its \" men in a sardine can \" warped logic .',\n",
       " \"long after you leave justine , you'll be wondering what will happen to her and wishing her the best -- whatever that might mean .\",\n",
       " \"still pretentious and filled with subtext , but entertaining enough at 'face value' to recommend to anyone looking for something different .\",\n",
       " 'call me a wimp , but i cried , not once , but three times in this animated sweet film .',\n",
       " 'notorious c . h . o . has oodles of vulgar highlights .',\n",
       " 'an inspiring and heart-affecting film about the desperate attempts of vietnamese refugees living in u . s . relocation camps to keep their hopes alive in 1975 .',\n",
       " 'the level of maturity displayed by this 33-year-old first-time feature director is astonishing , considering her inexperience and her subject matter .',\n",
       " 'a splendid entertainment , young in spirit but accomplished in all aspects with the fullness of spirit and sense of ease that comes only with experience .',\n",
       " \"disney's live-action division has a history of releasing cinematic flotsam , but this is one occasion when they have unearthed a rare gem .\",\n",
       " 'if the message seems more facile than the earlier films , the images have such a terrible beauty you may not care .',\n",
       " 'whether kiss is a future cult classic or destined to be completely forgotten is open to question , but the risk-takers in the crowd should check it out and form their own opinion .',\n",
       " \"there are moments in this account of the life of artist frida kahlo that are among cinema's finest this year . unfortunately , they're sandwiched in between the most impossibly dry account of kahlo's life imaginable .\",\n",
       " 'there are moments it can be heart-rending in an honest and unaffected ( and gentle ) way .',\n",
       " 'stay clear of reminding yourself that it\\'s a \" true story \" and you\\'re likely to have one helluva time at the movies .',\n",
       " 'there are just enough twists in the tale to make it far more satisfying than almost any horror film in recent memory .',\n",
       " \"the sundance film festival has become so buzz-obsessed that fans and producers descend upon utah each january to ferret out the next great thing . 'tadpole' was one of the films so declared this year , but it's really more of the next pretty good thing .\",\n",
       " \"working from elliott's memoir , rohmer fashions the sort of delicate , articulate character- and- relationship study he's favored for decades .\",\n",
       " 'the story feels more like a serious read , filled with heavy doses of always enticing sayles dialogue .',\n",
       " 'when it really counts . . . bloody sunday connects on a visceral level that transcends language .',\n",
       " 'the crime matters less than the characters , although the filmmakers supply enough complications , close calls and double-crosses to satisfy us .',\n",
       " '[an] hilarious romantic comedy .',\n",
       " 'the actors are fantastic . they are what makes it worth the trip to the theatre .',\n",
       " \"ranging from funny to shattering and featuring some of the year's best acting , personal velocity gathers plenty of dramatic momentum .\",\n",
       " 'i complain all the time about seeing the same ideas repeated in films over and over again , but the bourne identity proves that a fresh take is always possible .',\n",
       " \"recalls quiet freak-outs like l'avventura and repulsion .\",\n",
       " \"only an epic documentary could get it all down , and spike lee's jim brown : all american at long last gives its subject a movie worthy of his talents .\",\n",
       " '. . . as the story congeals you feel the pieces of the star wars saga falling into place in a way that makes your spine tingle with revelation and excitement .',\n",
       " \"a great comedy filmmaker knows great comedy needn't always make us laugh . tim story's not there yet - but 'barbershop' shows he's on his way .\",\n",
       " 'the movie is one of the best examples of artful large format filmmaking you are likely to see anytime soon .',\n",
       " 'lends itself to the narcotizing bland ( sinister , though not nearly so sinister as the biennial disney girl movie ) machinations of the biennial disney boy movie .',\n",
       " 'well-written , nicely acted and beautifully shot and scored , the film works on several levels , openly questioning social mores while ensnaring the audience with its emotional pull .',\n",
       " 'jason x has cheesy effects and a hoary plot , but its macabre , self-deprecating sense of humor makes up for a lot .',\n",
       " \"[taymor] utilizes the idea of making kahlo's art a living , breathing part of the movie , often catapulting the artist into her own work . this isn't a new idea . it's been done before but never so vividly or with so much passion .\",\n",
       " 'an impressive if flawed effort that indicates real talent .',\n",
       " 'two generations within one family test boundaries in this intelligent and restrained coming-of-age drama .',\n",
       " \"it sounds sick and twisted , but the miracle of shainberg's film is that it truly is romance\",\n",
       " 'disturbing and brilliant documentary .',\n",
       " '. . . mesmerizing , an eye-opening tour of modern beijing culture in a journey of rebellion , retreat into oblivion and return .',\n",
       " \"one of the best examples of how to treat a subject , you're not fully aware is being examined , much like a photo of yourself you didn't know was being taken .\",\n",
       " \"not too far below the gloss you can still feel director denis villeneuve's beating heart and the fondness he has for his characters .\",\n",
       " 'as if to prove a female director can make a movie with no soft edges , kathryn bigelow offers no sugar-coating or interludes of lightness . her film is unrelentingly claustrophobic and unpleasant .',\n",
       " '[villeneuve] seems to realize intuitively that even morality is reduced to an option by the ultimate mysteries of life and death .',\n",
       " 'the result is mesmerizing -- filled with menace and squalor .',\n",
       " 'fisher has bared his soul and confronted his own shortcomings here in a way . . . that feels very human and very true to life .',\n",
       " \"it's fun , but the code-talk will fly right over everyone's head\",\n",
       " \"bourne , jason bourne . he can scale a building like a super hero , he can out-stealth any agent , he'll get the girl . he's super spy !\",\n",
       " 'what makes the movie a comedy is the way it avoids the more serious emotions involved .',\n",
       " 'an exhilarating experience .',\n",
       " 'this cuddly sequel to the 1999 hit is a little more visually polished , a little funnier , and a little more madcap .',\n",
       " \"the pleasures of super troopers may be fleeting , but they'll register strongly with anybody who still retains a soft spot for precollegiate humor .\",\n",
       " 'the film is exhilarating to watch because sandler , liberated from the constraints of formula , reveals unexpected depths as an actor .',\n",
       " \"a distant , even sterile , yet compulsively watchable look at the sordid life of hogan's heroes star bob crane .\",\n",
       " \"the film delivers not just the full assault of reno's immense wit and insight , but a time travel back to what it felt like during those unforgettably uncertain days .\",\n",
       " 'what might have been a predictably heartwarming tale is suffused with complexity .',\n",
       " \"sound the trumpets : for the first time since desperately seeking susan , madonna doesn't suck as an actress .\",\n",
       " \"although very much like the first movie based on j . k . rowling's phenomenal fantasy best sellers , this second go-round possesses a quite pleasing , headlong thrust and a likably delinquent attitude .\",\n",
       " '[ \" take care of my cat \" ] is an honestly nice little film that takes us on an examination of young adult life in urban south korea through the hearts and minds of the five principals .',\n",
       " 'as the story moves inexorably through its seven day timeframe , the picture becomes increasingly mesmerizing .',\n",
       " 'maguire is a surprisingly effective peter/spider-man .',\n",
       " \"not a cozy or ingratiating work , but it's challenging , sometimes clever , and always interesting , and those are reasons enough to see it .\",\n",
       " 'the film runs on equal parts of innocence and wisdom -- wisdom that comes with experience . it has fun being grown up .',\n",
       " 'like old myths and wonder tales spun afresh .',\n",
       " 'rarely do films come along that are as intelligent , exuberant , and moving as monsoon wedding .',\n",
       " 'one scarcely needs the subtitles to enjoy this colorful action farce .',\n",
       " 'quite funny for the type of movie it is . . .',\n",
       " \"it's often infuriatingly glib and posturing , and yet it has been made with great evident care and manages to deliver up the man in a way to arouse further curiosity in even the most unknowing viewer .\",\n",
       " \"one of [herzog's] least inspired works .\",\n",
       " 'this boisterous comedy serves up a cruel reminder of the fate of hundreds of thousands of chinese , one which can only qualify as a terrible tragedy .',\n",
       " \"elling really is about a couple of crazy guys , and it's therapeutic to laugh along with them .\",\n",
       " 'never [sinks] into exploitation .',\n",
       " 'an irresistible combination of a rousing good story set on a truly grand scale .',\n",
       " \"there's no denying the physically spectacular qualities of the film . . . or the emotional integrity of the performances .\",\n",
       " 'few films this year have been as resolute in their emotional nakedness .',\n",
       " 'exquisitely acted and masterfully if preciously interwoven\\x85 [the film] addresses in a fascinating , intelligent manner the intermingling of race , politics and local commerce .',\n",
       " \"stevenson's performance is at once clueless and fiercely committed , a volatile combination .\",\n",
       " 'this is a very fine movie -- go see it .',\n",
       " \"as shaky as the plot is , kaufman's script is still memorable for some great one-liners .\",\n",
       " 'despite its flaws , secretary stays in your head and makes you question your own firmly held positions .',\n",
       " 'one of those rare , exhilarating cinematic delights that gets even better in hindsight , as you mull over its every nuance in your mind .',\n",
       " 'not everything works , but the average is higher than in mary and most other recent comedies .',\n",
       " 'a byzantine melodrama that stimulates the higher brain functions as well as the libido .',\n",
       " \"a sensitive and expertly acted crowd-pleaser that isn't above a little broad comedy and a few unabashedly sentimental tears .\",\n",
       " \"the film's sharp , often mischievous sense of humor will catch some off guard . . .\",\n",
       " 'does what a fine documentary does best : it extends a warm invitation into an unfamiliar world , then illuminates it fully and allows the larger implications of the journey to sink in unobtrusively .',\n",
       " 'almost every scene in this film is a gem that could stand alone , a perfectly realized observation of mood , behavior and intent .',\n",
       " 'a psychologically rich and suspenseful moral thriller with a stellar performance by al pacino .',\n",
       " \"you won't believe much of it , but you will laugh at the audacity , at the who's who casting and the sheer insanity of it all .\",\n",
       " \"this version's no classic like its predecessor , but its pleasures are still plentiful .\",\n",
       " 'the bourne identity is what summer screen escapism used to be in the decades when it was geared more to grownups .',\n",
       " 'provide[s] nail-biting suspense and credible characters without relying on technology-of-the-moment technique or pretentious dialogue .',\n",
       " 'if it tried to do anything more , it would fail and perhaps explode , but at this level of manic whimsy , it is just about right .',\n",
       " 'too sincere to exploit its subjects and too honest to manipulate its audience .',\n",
       " \"the saturation bombing of reggio's images and glass' evocative music . . . ultimately leaves viewers with the task of divining meaning .\",\n",
       " 'for all its serious sense of purpose . . . [it] finds a way to lay bare the tragedies of its setting with a good deal of warmth and humor .',\n",
       " 'a depressing confirmation of everything those of us who don\\'t object to the description \" unelected \" have suspected all along : george w . bush is an incurious , uncharismatic , overgrown frat boy with a mean streak a mile wide .',\n",
       " \"this road movie gives you emotional whiplash , and you'll be glad you went along for the ride .\",\n",
       " \"sure , it's more of the same , but as the film proves , that's not always a bad thing .\",\n",
       " 'a lighthearted , feel-good film that embraces the time-honored truth that the most powerful thing in life is love .',\n",
       " 'a bowel-curdling , heart-stopping recipe for terror .',\n",
       " \"daughter from danang is a film that should be seen by all , especially those who aren't aware of , or have forgotten about the unmentioned victims of war .\",\n",
       " \"zhang yimou delivers warm , genuine characters who lie not through dishonesty , but because they genuinely believe it's the only way to bring happiness to their loved ones .\",\n",
       " '. . . breathes surprising new life into the familiar by amalgamating genres and adding true human complexity to its not-so-stock characters .',\n",
       " \"' . . . both hokey and super-cool , and definitely not in a hurry , so sit back , relax and have a few laughs while the little ones get a fuzzy treat . '\",\n",
       " 'a pleasant romantic comedy .',\n",
       " \"it's a count for our times .\",\n",
       " 'greengrass has delivered an undoubted stylistic tour-de-force , and has managed elements such as sound and cinematography with skill',\n",
       " \"smith's point is simple and obvious -- people's homes are extensions of themselves , and particularly eccentric people have particularly eccentric living spaces -- but his subjects are charmers .\",\n",
       " 'a romantic comedy , yes , but one with characters who think and talk about their goals , and are working on hard decisions .',\n",
       " 'vividly conveys both the pitfalls and the pleasures of over-the-top love .',\n",
       " '. . . a weak , manipulative , pencil-thin story that is miraculously able to entertain anyway .',\n",
       " 'a pro-fat farce that overcomes much of its excessive moral baggage thanks to two appealing lead performances .',\n",
       " 'for the first two-thirds of this sparklingly inventive and artful , always fast and furious tale , kids will go happily along for the ride .',\n",
       " \"majidi's poetic love story is a ravishing consciousness-raiser , if a bit draggy at times .\",\n",
       " 'the smartest bonehead comedy of the summer .',\n",
       " 'effectively feeds our senses with the chilling sights and sounds from within the camp to create a completely numbing experience .',\n",
       " 'i love the way that it took chances and really asks you to take these great leaps of faith and pays off .',\n",
       " 'in his debut as a film director , denzel washington delivers a lean and engaging work .',\n",
       " 'only two words will tell you what you know when deciding to see it : anthony . hopkins .',\n",
       " \"the movie's quiet affirmation of neighborhood values gives it an honest , lived-in glow .\",\n",
       " 'a teasing drama whose relentless good-deed/bad-deed reversals are just interesting enough to make a sinner like me pray for an even more interesting , less symmetrical , less obviously cross-shaped creation .',\n",
       " 'hayek is stunning as frida and . . . a star-making project .',\n",
       " \"it's both a necessary political work and a fascinating documentary . . .\",\n",
       " 'hilarious , acidic brit comedy .',\n",
       " \"as a revenge thriller , the movie is serviceable , but it doesn't really deliver the delicious guilty pleasure of the better film versions .\",\n",
       " 'an ironic speculation on democracy in a culture unaccustomed to it .',\n",
       " \"it's not life-affirming \\x97 its vulgar and mean , but i liked it .\",\n",
       " 'several degrees shy of the gross-out contests one expects from current teen fare .',\n",
       " 'the inherent strength of the material as well as the integrity of the filmmakers gives this coming-of-age story restraint as well as warmth .',\n",
       " \"led by griffin's smartly nuanced performance and enthusiasm , the cast has a lot of fun with the material .\",\n",
       " 'tuck everlasting achieves a delicate balance of romantic innocence and philosophical depth .',\n",
       " 'a gentle blend of present day testimonials , surviving footage of burstein and his family performing , historical archives , and telling stills .',\n",
       " 'a generation x artifact , capturing a brief era of insanity in the sports arena that surely cannot last .',\n",
       " \"possession is elizabeth barrett browning meets nancy drew , and it's directed by . . . neil labute . hmm .\",\n",
       " 'an uneven but intriguing drama that is part homage and part remake of the italian masterpiece .',\n",
       " 'windtalkers celebrates the human spirit and packs an emotional wallop .',\n",
       " \"having never been a huge fan of dickens' 800-page novel , it surprised me how much pleasure i had watching mcgrath's version .\",\n",
       " 'the best thing the film does is to show us not only what that mind looks like , but how the creative process itself operates .',\n",
       " 'for all its failed connections , divine secrets of the ya-ya sisterhood is nurturing , in a gauzy , dithering way .',\n",
       " 'this is pretty dicey material . but some unexpected zigs and zags help .',\n",
       " 'compellingly watchable .',\n",
       " 'the filmmakers skillfully evoke the sense of menace that nature holds for many urban dwellers .',\n",
       " 'the laser-projected paintings provide a spell-casting beauty , while russell and dreyfus are a romantic pairing of hearts , preciously exposed as history corners them .',\n",
       " \"you don't have to be an especially tough grader to give a charitable b-minus to the emperor's club .\",\n",
       " 'this romantic thriller is steeped in the atmosphere of wartime england , and ably captures the speech patterns , moral codes and ideals of the 1940s .',\n",
       " \"divine secrets of the ya-ya sisterhood may not be exactly divine , but it's definitely -- defiantly -- ya ya , what with all of those terrific songs and spirited performances .\",\n",
       " \"viewed on its own terms , treasure planet is better-than-average family entertainment , but true fans of the stevenson's novel will likely prefer disney's more faithful 1950 live-action swashbuckling classic .\",\n",
       " 'a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance .',\n",
       " 'resourceful and ingenious entertainment .',\n",
       " '\" antwone fisher \" is an earnest , by-the-numbers effort by washington . it won\\'t rock any boats but is solid meat-and-potatoes filmmaking .',\n",
       " 'a historical epic with the courage of its convictions about both scope and detail .',\n",
       " \"we need [moore's] noisy , cocky energy , his passion and class consciousness ; we need his shticks , we need his stones .\",\n",
       " 'although the editing might have been tighter , hush ! sympathetically captures the often futile lifestyle of young people in modern japan .',\n",
       " '[gai] comes closer to any actress i can remember to personifying independence in its purest and , yes , most intimidating form .',\n",
       " 'these are lives worth watching , paths worth following .',\n",
       " \"it's rather like a lifetime special -- pleasant , sweet and forgettable .\",\n",
       " \"a moody horror/thriller elevated by deft staging and the director's well-known narrative gamesmanship .\",\n",
       " \"as a singular character study , it's perfect . it's also the year's sweetest movie .\",\n",
       " 'a graceful , contemplative film that gradually and artfully draws us into a world where the personal and the political get fatally intertwined .',\n",
       " 'while not as aggressively impressive as its american counterpart , \" in the bedroom , \" moretti\\'s film makes its own , quieter observations',\n",
       " 'the experience of watching blobby old-school cgi animation in this superlarge format is just surreal enough to be diverting .',\n",
       " 'time changer may not be the most memorable cinema session but its profound self-evaluation message about our fragile existence and the absence of spiritual guidance should at least invade an abundance of mindsets',\n",
       " '\" the emperor\\'s new clothes \" begins with a simple plan . . . . well , at least that\\'s the plan .',\n",
       " \"haynes has so fanatically fetishized every bizarre old-movie idiosyncrasy with such monastic devotion you're not sure if you should applaud or look into having him committed .\",\n",
       " \"[director peter] jackson and his crew have so steeped themselves in the majesty of tolkien's writing that every frame produces new joys , whether you're a fan of the books or not .\",\n",
       " \"while the glass slipper doesn't quite fit , pumpkin is definitely a unique modern fairytale .\",\n",
       " 'the drama is played out with such aching beauty and truth that it brings tears to your eyes .',\n",
       " 'an exciting and involving rock music doc , a smart and satisfying look inside that tumultuous world .',\n",
       " 'an offbeat , sometimes gross and surprisingly appealing animated film about the true meaning of the holidays .',\n",
       " 'this version incarnates the prophetic book in a way even its exacting author might admire .',\n",
       " 'sometimes , nothing satisfies like old-fashioned swashbuckling . and in this regard , on guard delivers .',\n",
       " \". . . ambition is in short supply in the cinema , and egoyan tackles his themes and explores his characters' crises with seriousness and compassion .\",\n",
       " 'an impossible romance , but we root for the patronized iranian lad .',\n",
       " 'like dickens with his passages , mcgrath crafts quite moving scenes throughout his resolutely dramatic variation on the novel .',\n",
       " \"there's a disreputable air about the whole thing , and that's what makes it irresistible .\",\n",
       " \"an exceedingly clever piece of cinema . another great \\x91what you don't see' is much more terrifying than what you do see thriller , coupled with some arresting effects , incandescent tones and stupendous performances\",\n",
       " 'a carefully structured scream of consciousness that is tortured and unsettling--but unquestionably alive .',\n",
       " \"a quietly reflective and melancholy new zealand film about an eventful summer in a 13-year-old girl's life .\",\n",
       " 'cute , funny , heartwarming digitally animated feature film with plenty of slapstick humor for the kids , lots of in-jokes for the adults and heart enough for everyone .',\n",
       " 'very solid , very watchable first feature for director peter sheridan',\n",
       " 'a budget affair that exposes the generally sad existence of the bedouins while providing a precious twinkle of insight into their lives .',\n",
       " 'it suggests the wide-ranging effects of media manipulation , from the kind of reporting that is done by the supposedly liberal media . . . to the intimate and ultimately tragic heartache of maverick individuals like hatfield and hicks .',\n",
       " 'workmanlike , maybe , but still a film with all the elements that made the other three great , scary times at the movies .',\n",
       " 'a pleasant enough comedy that should have found a summer place .',\n",
       " 'branagh , in his most forceful non-shakespeare screen performance , grounds even the softest moments in the angry revolt of his wit .',\n",
       " 'though the violence is far less sadistic than usual , the film is typical miike : fast , furious and full of off-the-cuff imaginative flourishes .',\n",
       " 'compelling as it is exotic , fast runner has a plot that rivals shakespeare for intrigue , treachery and murder .',\n",
       " 'what it lacks in originality it makes up for in intelligence and b-grade stylishness .',\n",
       " 'the warm presence of zhao benshan makes the preposterous lying hero into something more than he reasonably should be .',\n",
       " \"this is as powerful a set of evidence as you'll ever find of why art matters , and how it can resonate far beyond museum walls and through to the most painfully marginal lives .\",\n",
       " 'director rob marshall went out gunning to make a great one .',\n",
       " 'skip work to see it at the first opportunity .',\n",
       " \"bow's best moments are when he's getting busy on the basketball court because that's when he really scores .\",\n",
       " 'offers enough playful fun to entertain the preschool set while embracing a wholesome attitude .',\n",
       " \"in the end , punch-drunk love is one of those films that i wanted to like much more than i actually did . sometimes , that's enough .\",\n",
       " 'an intimate , good-humored ethnic comedy like numerous others but cuts deeper than expected .',\n",
       " 'ice cube holds the film together with an engaging and warm performance . . .',\n",
       " 'both deeply weird and charmingly dear .',\n",
       " 'as blunt as it is in depicting child abuse , el bola is a movie steeped in an ambiguity that lends its conflicts a symbolic resonance .',\n",
       " 'despite a story predictable enough to make the sound of music play like a nail-biting thriller , its heart is so much in the right place it is difficult to get really peeved at it .',\n",
       " \"it's a masterpiece .\",\n",
       " 'an incredibly low-rent danish film , it brings a group of people together in a sweet and charming way , if a little convenient',\n",
       " \"it's the cinematic equivalent of a good page-turner , and even if it's nonsense , its claws dig surprisingly deep .\",\n",
       " \"director nalin pan doesn't do much to weigh any arguments one way or the other . he simply presents his point of view that ayurveda works . no question .\",\n",
       " 'what \" empire \" lacks in depth it makes up for with its heart .',\n",
       " 'claude miller airs out a tight plot with an easy pace and a focus on character drama over crime-film complications .',\n",
       " \"what full frontal lacks in thematic coherence it largely makes up for as loosey-goosey , experimental entertainment . still , i'm not quite sure what the point is\",\n",
       " 'rich in detail , gorgeously shot and beautifully acted , les destinees is , in its quiet , epic way , daring , inventive and refreshingly unusual .',\n",
       " \"( a ) hollywood sheen bedevils the film from the very beginning . . . ( but ) lohman's moist , deeply emotional eyes shine through this bogus veneer . . .\",\n",
       " \"do we really need a 77-minute film to tell us exactly why a romantic relationship between a 15-year-old boy and a 40-year-old woman doesn't work ?\",\n",
       " 'ford deserves to be remembered at oscar time for crafting this wonderful portrait of a conflicted soldier .',\n",
       " \"the film's 45-minute running time stops shy of overkill , though viewers may be more exhausted than the athletes onscreen .\",\n",
       " \"don't expect any surprises in this checklist of teamwork cliches . . .\",\n",
       " 'as adapted by kevin molony from simon leys\\' novel \" the death of napoleon \" and directed by alan taylor , napoleon\\'s journey is interesting but his parisian rebirth is stillborn',\n",
       " \"the movie addresses a hungry need for pg-rated , nonthreatening family movies , but it doesn't go too much further .\",\n",
       " 'this warm and gentle romantic comedy has enough interesting characters to fill several movies , and its ample charms should win over the most hard-hearted cynics .',\n",
       " 'a yarn that respects the marvel version without becoming ensnared by it .',\n",
       " \"this is a happy throwback to the time when cartoons were cinema's most idiosyncratic form instead of one of its most predictable .\",\n",
       " 'complex , affecting and uniquely almodóvar , the film evokes strong emotions and pushes viewers to question their deepest notions of moral right and wrong .',\n",
       " \"good ol' urban legend stuff .\",\n",
       " \"not so much a movie as a picture book for the big screen . this isn't my favorite in the series , still i enjoyed it enough to recommend .\",\n",
       " \"it's one of the most honest films ever made about hollywood .\",\n",
       " 'it is a film that will have people walking out halfway through , will encourage others to stand up and applaud , and will , undoubtedly , leave both camps engaged in a ferocious debate for years to come .',\n",
       " 'on its own cinematic terms , it successfully showcases the passions of both the director and novelist byatt .',\n",
       " 'light , silly , photographed with colour and depth , and rather a good time .',\n",
       " \"pray's film works well and will appeal even to those who aren't too familiar with turntablism .\",\n",
       " 'troubling and powerful .',\n",
       " 'good movie . good actress . but if you expect light romantic comedy , good gosh , will you be shocked .',\n",
       " 'it has the courage to wonder about big questions with sincerity and devotion . it risks seeming slow and pretentious , because it thinks the gamble is worth the promise .',\n",
       " \"with youthful high spirits , tautou remains captivating throughout michele's religious and romantic quests , and she is backed by a likable cast .\",\n",
       " \"it's an example of sophisticated , challenging filmmaking that stands , despite its noticeable lack of emotional heft , in welcome contrast to the indulgent dead-end experimentation of the director's previous full frontal .\",\n",
       " 'a very funny look at how another culture handles the process of courting and marriage .',\n",
       " 'but tongue-in-cheek preposterousness has always been part of for the most part wilde\\'s droll whimsy helps \" being earnest \" overcome its weaknesses and parker\\'s creative interference . . .',\n",
       " \"much of the movie's charm lies in the utter cuteness of stuart and margolo . their computer-animated faces are very expressive .\",\n",
       " \"the path ice age follows most closely , though , is the one established by warner bros . giant chuck jones , who died a matter of weeks before the movie's release .\",\n",
       " \"anchored by a terrific performance by abbass , satin rouge shows that the idea of women's self-actualization knows few continental divides .\",\n",
       " 'awkward but sincere and , ultimately , it wins you over .',\n",
       " \"smith profiles five extraordinary american homes , and because the owners seem fully aware of the uses and abuses of fame , it's a pleasure to enjoy their eccentricities .\",\n",
       " 'though the plot is predictable , the movie never feels formulaic , because the attention is on the nuances of the emotional development of the delicate characters .',\n",
       " \"sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune .\",\n",
       " \"goyer's screenplay and direction are thankfully understated , and he has drawn excellent performances from his cast .\",\n",
       " 'binoche and magimel are perfect in these roles .',\n",
       " 'when your leading ladies are a couple of screen-eating dominatrixes like goldie hawn and susan sarandon at their raunchy best , even hokum goes down easily .',\n",
       " \"while undercover brother is definitely one for the masses , it's also full of sharp , smart satire .\",\n",
       " 'gets under the skin of a man who has just lost his wife .',\n",
       " 'it may not be \" last tango in paris \" but . . .',\n",
       " 'no wonder they\\'re talking about \" talk to her . \" it\\'s astonishing .',\n",
       " 'for its seriousness , high literary aspirations and stunning acting , the film can only be applauded .',\n",
       " 'look , this is a terrific flick replete with dazzling camera-work , dancing and music .',\n",
       " 'it is inspirational in characterizing how people from such diverse cultures share the same human and spiritual needs .',\n",
       " \"it's fairly self-aware in its dumbness .\",\n",
       " 'a triumph , relentless and beautiful in its downbeat darkness .',\n",
       " 'tailored to entertain !',\n",
       " 'a compelling , moving film that respects its audience and its source material .',\n",
       " 'has a plot full of twists upon knots . . . and a nonstop parade of mock-tarantino scuzbag types that starts out clever but veers into overkill .',\n",
       " 'a work of astonishing delicacy and force .',\n",
       " 'the film benefits greatly from a less manic tone than its predecessor , as cho appears to have settled comfortably into her skin .',\n",
       " \"for the first time in several years , mr . allen has surpassed himself with the magic he's spun with the hollywood empress of ms . leoni's ellie .\",\n",
       " \"isn't quite the equal of woo's best earlier work , but it's easily his finest american film . . . comes close to recapturing the brilliance of his hong kong films .\",\n",
       " 'the film hinges on its performances , and both leads are up to the task .',\n",
       " \"an intelligent , earnest , intimate film that drops the ball only when it pauses for blunt exposition to make sure you're getting its metaphysical point .\",\n",
       " 'a modest pleasure that accomplishes its goals with ease and confidence .',\n",
       " 'a breezy , diverting , conventional , well-acted tale of two men locked in an ongoing game of cat-and-cat .',\n",
       " 'what jackson has accomplished here is amazing on a technical level .',\n",
       " 'as teen movies go , \" orange county \" is a refreshing change',\n",
       " 'makes s&m seem very romantic , and maggie gyllenhaal is a delight .',\n",
       " 'a deliciously mordant , bitter black comedy .',\n",
       " \"although life or something like it is very much in the mold of feel-good movies , the cast and director stephen herek's polished direction pour delightfully piquant wine from aged bottles .\",\n",
       " 'it is risky , intelligent , romantic and rapturous from start to finish .',\n",
       " \"the movie sticks much closer to hornby's drop-dead confessional tone than the film version of high fidelity did .\",\n",
       " 'a pleasant ramble through the sort of idoosyncratic terrain that errol morris has often dealt with . . . it does possess a loose , lackadaisical charm .',\n",
       " \". . . spiced with humor ( 'i speak fluent flatula , ' advises denlopp after a rather , er , bubbly exchange with an alien deckhand ) and witty updatings ( silver's parrot has been replaced with morph , a cute alien creature who mimics everyone and everything around )\",\n",
       " \"this is a raw and disturbing tale that took five years to make , and the trio's absorbing narrative is a heart-wrenching showcase indeed .\",\n",
       " 'a beautiful and haunting examination of the stories we tell ourselves to make sense of the mundane horrors of the world .',\n",
       " \"aside from being the funniest movie of the year , simone , andrew niccol's brilliant anti-hollywood satire , has a wickedly eccentric enchantment to it .\",\n",
       " \"watstein handily directs and edits around his screenplay's sappier elements . . . and sustains off the hook's buildup with remarkable assuredness for a first-timer .\",\n",
       " 'just another fish-out-of-water story that barely stays afloat .',\n",
       " \"there's an energy to y tu mamá también . much of it comes from the brave , uninhibited performances by its lead actors .\",\n",
       " \"it's the kind of pigeonhole-resisting romp that hollywood too rarely provides .\",\n",
       " \"reinforces the often forgotten fact of the world's remarkably varying human population and mindset , and its capacity to heal using creative , natural and ancient antidotes .\",\n",
       " 'you can feel the heat that ignites this gripping tale , and the humor and humanity that root it in feeling .',\n",
       " \"it's hard not to be seduced by [witherspoon's] charisma , even in this run-of-the-mill vehicle , because this girl knows how to drive it to the max .\",\n",
       " \"a movie for 11-year-old boys with sports dreams of their own and the preteen girls who worship lil' bow wow .\",\n",
       " 'a refreshingly authentic coming-of-age tale .',\n",
       " \"if you're not into the pokemon franchise , this fourth animated movie in four years won't convert you -- or even keep your eyes open . but fans should have fun meeting a brand-new pokemon called celebi .\",\n",
       " \"from the big giant titles of the opening credits to elmer bernstein's perfectly melodic score , haynes gets just about everything right .\",\n",
       " 'whether seen on a 10-inch television screen or at your local multiplex , the edge-of-your-seat , educational antics of steve irwin are priceless entertainment .',\n",
       " 'has a shambling charm . . . a cheerfully inconsequential diversion .',\n",
       " \"ferrara directs the entire film with the kind of detachment that makes any given frame look like a family's custom-made christmas card .\",\n",
       " 'the movie has lots of dancing and fabulous music . there are slow and repetitive parts , but it has just enough spice to keep it interesting .',\n",
       " 'an incredibly clever and superbly paced caper filled with scams within scams within scams .',\n",
       " \"there's not much more to this adaptation of the nick hornby novel than charm -- effortless , pleasurable , featherweight charm .\",\n",
       " 'as a belated nod to some neglected all-stars , standing in the shadows of motown is cultural history of the best kind : informative , revealing and richly entertaining .',\n",
       " \"even if the ride's a little bumpy , with a final lap that's all too suspiciously smooth , you gotta give director roger michell , best known for the superfluous notting hill , credit for trying .\",\n",
       " 'not as distinctive or even as humorous as its needs to be to stand out , but it has clearly been made with affection and care .',\n",
       " \"this is carion's debut feature but his script and direction hums with a confidence that many spend entire careers trying to reach .\",\n",
       " 'an intelligent , moving and invigorating film .',\n",
       " 'ofrece una buena oportunidad de cultura ( aunque sea condensada ) que bien vale la pena aprovechar .',\n",
       " \". . . one of the most ingenious and entertaining thrillers i've seen in quite a long time .\",\n",
       " 'a clever blend of fact and fiction .',\n",
       " 'a vivid cinematic portrait .',\n",
       " 'hilarious , touching and wonderfully dyspeptic .',\n",
       " 'es divertida , visualmente espectacular y muy entretenida . simple y sencillamente te sorprenderá .',\n",
       " 'theirs is a simple and heart-warming story , full of mirth that should charm all but the most cynical .',\n",
       " 'the film is an enjoyable family film -- pretty much aimed at any youngster who loves horses .',\n",
       " 'a frisky and fresh romantic comedy exporing sexual politics and the challenges of friendships between women .',\n",
       " \"it's a good film -- not a classic , but odd , entertaining and authentic .\",\n",
       " 'flavorful and romantic , you could call this how martha got her groove back -- assuming , that is , she ever had one to begin with .',\n",
       " 'happily for mr . chin -- though unhappily for his subjects -- the invisible hand of the marketplace wrote a script that no human screenwriter could have hoped to match .',\n",
       " 'thurman and lewis are hilarious throughout .',\n",
       " 'the plot is so amusingly contrived and outlandish in its coincidences that no one could ever mistake it for anything resembling reality',\n",
       " \"hits one out of the park for the 'they don't make 'em like that anymore' department .\",\n",
       " 'it dares to be a little different , and that shading is what makes it worthwhile .',\n",
       " '[fessenden] is much more into ambiguity and creating mood than he is for on screen thrills',\n",
       " \"the comic performances are all spot on , especially lee ross's turn as ken .\",\n",
       " 'a compelling journey . . . and \" his best friend remembers \" is up there with the finest of specials .',\n",
       " 'at nearly three hours , the whole of safe conduct is less than the sum of its parts .',\n",
       " 'the hours makes you examine your own life in much the same way its characters do , and the experience is profound . the hours is what movies are supposed to be . . .',\n",
       " 'a bold and subversive film that cuts across the grain of what is popular and powerful in this high-tech age , speaking its truths with spellbinding imagery and the entrancing music of philip glass .',\n",
       " 'pretty darn good , despite its smarty-pants aura .',\n",
       " 'so young , so smart , such talent , such a wise * * * .',\n",
       " \"woo's fights have a distinct flair . his warriors collide in balletic explosion that implies an underlying order throughout the chaos .\",\n",
       " 'barney has created a tour de force that is weird , wacky and wonderful .',\n",
       " 'the ending does leave you unfulfilled , but these are performances to enjoy in a memorable ensemble piece .',\n",
       " \". . . an agreeable time-wasting device -- but george pal's low-tech 1960 version still rules the epochs .\",\n",
       " \"it's a brave attempt to tap into the heartbeat of the world , a salute to the universal language of rhythm and a zippy sampling of sounds .\",\n",
       " 'offers an unusual opportunity to observe the inequities in the death penalty , not just the inherent immorality but also the haphazard administration of it and public misperception of how the whole thing works .',\n",
       " \"i don't think i've been as entranced and appalled by an asian film since shinya tsukamoto's iron man .\",\n",
       " \"it is so refreshing to see robin williams turn 180 degrees from the string of insultingly innocuous and sappy fiascoes he's been making for the last several years .\",\n",
       " \"director benoit jacquot , making his first opera-to-film translation with tosca , conveys the heaving passion of puccini's famous love-jealousy- murder-suicide fandango with great cinematic innovation .\",\n",
       " \"lilia's transformation from strict mother to sensual siren is superficially preposterous , but abbas infuses the role with an unimpeachable core of emotional truth .\",\n",
       " \"frida's artistic brilliance is undeniable -- it's among the most breathtakingly designed films i've ever seen .\",\n",
       " 'the perfect film for those who like sick comedies that can be snide .',\n",
       " \"'charly' will divide its audience in two separate groups , those reaching for more tissues and those begging for mercy . . .\",\n",
       " 'nervy and sensitive , it taps into genuine artistic befuddlement , and at the same time presents a scathing indictment of what drives hollywood .',\n",
       " 'a marvellous journey from childhood idealism to adolescent self-absorption .',\n",
       " 'the film is just a big , gorgeous , mind-blowing , breath-taking mess .',\n",
       " 'sharp , lively , funny and ultimately sobering film .',\n",
       " \"though the film's scenario is certainly not earthshaking , this depiction of fluctuating female sexuality has two winning lead performances and charm to spare .\",\n",
       " \"a worthy tribute to a great humanitarian and her vibrant 'co-stars . '\",\n",
       " \"a recent favourite at sundance , this white-trash satire will inspire the affection of even those unlucky people who never owned a cassette of def leppard's pyromania .\",\n",
       " 'the recording session is the only part of the film that is enlightening -- and how appreciative you are of this depends on your level of fandom .',\n",
       " 'occasionally funny and consistently odd , and it works reasonably well as a star vehicle for zhao .',\n",
       " \"bright seems alternately amused and disgusted with this material , and he can't help throwing in a few of his own touches .\",\n",
       " \"the 3d images only enhance the film's otherworldly quality , giving it a strange combo of you-are-there closeness with the disorienting unreality of the seemingly broken-down fourth wall of the movie screen .\",\n",
       " \"andersson creates a world that's at once surreal and disturbingly familiar ; absurd , yet tremendously sad .\",\n",
       " \"it's predictable , but it jumps through the expected hoops with style and even some depth .\",\n",
       " 'often hilarious , well-shot and , importantly , entertaining , hell house is a fascinating document of an event that has to be seen to be believed .',\n",
       " 'de oliveira creates an emotionally rich , poetically plump and visually fulsome , but never showy , film whose bittersweet themes are reinforced and brilliantly personified by michel piccoli .',\n",
       " '. . . an inviting piece of film .',\n",
       " \"the film's real appeal won't be to clooney fans or adventure buffs , but to moviegoers who enjoy thinking about compelling questions with no easy answers .\",\n",
       " 'the fact that the rookie is a nearly impeccable cinematic experience -- and a wonderful all-ages triumph besides -- is a miracle akin to the story the film portrays .',\n",
       " 'a deviant topical comedy which is funny from start to finish .',\n",
       " 'a startling and fresh examination of how the bike still remains an ambiguous icon in chinese society .',\n",
       " \"a highly intriguing thriller , coupled with some ingenious plot devices and some lavishly built settings . . it's a worthwhile tutorial in quantum physics and slash-dash\",\n",
       " \"as hugh grant says repeatedly throughout the movie , 'lovely ! brilliant ! '\",\n",
       " \"cho's fearless in picking apart human foibles , not afraid to lay her life bare in front of an audience . her delivery and timing are flawless .\",\n",
       " 'works because , for the most part , it avoids the stupid cliches and formulaic potholes that befall its brethren .',\n",
       " 'at its best , the good girl is a refreshingly adult take on adultery . . .',\n",
       " 'an amazing and incendiary movie that dives straight into the rough waters of contradiction .',\n",
       " 'about nowhere kids who appropriated turfs as they found them and become self-made celebrity athletes -- a low-down version of the american dream .',\n",
       " 'occasionally , in the course of reviewing art-house obscurities and slam-bam action flicks , a jaded critic smacks into something truly new .',\n",
       " 'a miniscule little bleep on the film radar , but one that many more people should check out',\n",
       " 'desta vez , columbus capturou o pomo de ouro .',\n",
       " '\" 13 conversations \" holds its goodwill close , but is relatively slow to come to the point .',\n",
       " 'a slick , well-oiled machine , exquisitely polished and upholstered .',\n",
       " \"don't plan on the perfect ending , but sweet home alabama hits the mark with critics who escaped from a small town life .\",\n",
       " \"it has a subtle way of getting under your skin and sticking with you long after it's over .\",\n",
       " 'the movie stays afloat thanks to its hallucinatory production design .',\n",
       " 'it helps that the central performers are experienced actors , and that they know their roles so well .',\n",
       " 'a provocative movie about loss , anger , greed , jealousy , sickness and love .',\n",
       " 'worth the effort to watch .',\n",
       " 'that rara avis : the intelligent romantic comedy with actual ideas on its mind .',\n",
       " 'boisterous and daft documentary .',\n",
       " \"hawke draws out the best from his large cast in beautifully articulated portrayals that are subtle and so expressive they can sustain the poetic flights in burdette's dialogue .\",\n",
       " 'a work of the utmost subtlety and perception , it marks the outstanding feature debut of writer-director eric byler , who understands the power of the implicit and the virtues of simplicity and economy .',\n",
       " \"full frontal is the antidote for soderbergh fans who think he's gone too commercial since his two oscar nominated films in 2000\",\n",
       " 'it turns out to be a cut above the norm , thanks to some clever writing and sprightly acting .',\n",
       " \"you might not want to hang out with samantha , but you'll probably see a bit of yourself in her unfinished story .\",\n",
       " 'a work of intricate elegance , literary lyricism and profound common sense .',\n",
       " \"it's as close as we'll ever come to looking through a photographer's viewfinder as he works .\",\n",
       " 'thoughtful , provocative and entertaining .',\n",
       " 'witty , touching and well paced .',\n",
       " \"lee jeong-hyang tells it so lovingly and films it so beautifully that i couldn't help being captivated by it .\",\n",
       " \"you have to pay attention to follow all the stories , but they're each interesting . the movie is well shot and very tragic , and one to ponder after the credits roll .\",\n",
       " 'enjoy it for what it is ; you can hate yourself later .',\n",
       " \"a map of the inner rhythms of love and jealousy and sacrifice drawn with a master's steady stroke .\",\n",
       " 'más sarcástica , divertida y demencial que su predecesora , es un buen ejemplo de lo que es el cine de entretenimiento puro y sin complejos .',\n",
       " \"a psychological thriller with a smart script and an obsessive-compulsive's attention to detail .\",\n",
       " 'often hilarious .',\n",
       " 'grant gets to display his cadness to perfection , but also to show acting range that may surprise some who thought light-hearted comedy was his forte .',\n",
       " \"at times funny and at other times candidly revealing , it's an intriguing look at two performers who put themselves out there because they love what they do .\",\n",
       " \"westfeldt and juergensen exude a chemistry and comfort level that's both saucy and endearing .\",\n",
       " 'harsh , effective documentary on life in the israeli-occupied palestinian territories .',\n",
       " \"the film is all a little lit crit 101 , but it's extremely well played and often very funny .\",\n",
       " \"earns its laughs from stock redneck 'types' and from the many , many moments when we recognize even without the elizabethan prose , the play behind the thing .\",\n",
       " 'a real story about real people living their lives concerned about the future of an elderly , mentally handicapped family member .',\n",
       " \"it's absolutely spooky how lillard channels the shagster right down to the original casey kasem-furnished voice .\",\n",
       " \"a dream cast of solid female talent who build a seamless ensemble . there isn't a weak or careless performance amongst them .\",\n",
       " 'smart science fiction for grown-ups , with only a few false steps along the way .',\n",
       " \"it's a refreshing change from the self-interest and paranoia that shape most american representations of castro .\",\n",
       " \"often moving and explores the discomfort inherent in the contacts between the american 'hosts' and their 'guests . '\",\n",
       " \"though the controversial korean filmmaker's latest effort is not for all tastes , it offers gorgeous imagery , effective performances , and an increasingly unsettling sense of foreboding .\",\n",
       " 'lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .',\n",
       " 'the story may not be new , but australian director john polson , making his american feature debut , jazzes it up adroitly .',\n",
       " \"it's endearing to hear madame d . refer to her husband as 'jackie' -- and he does make for excellent company , not least as a self-conscious performer .\",\n",
       " 'the film often achieves a mesmerizing poetry .',\n",
       " 'more than makes up for its mawkish posing by offering rousing spates of genuine feeling .',\n",
       " \"it's neither as romantic nor as thrilling as it should be . but it offers plenty to ponder and chew on as its unusual relationship slowly unfolds .\",\n",
       " 'occasionally funny , always very colorful and enjoyably overblown in the traditional almodóvar style .',\n",
       " \"merchant effectively translates naipaul's lively mix of characters from the page to screen .\",\n",
       " \"some movies are like a tasty hors-d'oeuvre ; this one is a feast .\",\n",
       " 'what could have become just another cautionary fable is allowed to play out as a clever , charming tale \\x96 as pleasantly in its own way as its self-dramatizing characters .',\n",
       " 'davis has filled out his cast with appealing fresh faces .',\n",
       " 'achieves a sort of filmic epiphany that revels in the true potential of the medium .',\n",
       " 'once you get into its rhythm . . . the movie becomes a heady experience .',\n",
       " '\" auto focus \" works as an unusual biopic and document of male swingers in the playboy era',\n",
       " \"if mr . zhang's subject matter is , to some degree at least , quintessentially american , his approach to storytelling might be called iranian .\",\n",
       " 'a fast-moving and remarkable film that appears destined to become a landmark in japanese animation .',\n",
       " '. . . a sour little movie at its core ; an exploration of the emptiness that underlay the relentless gaiety of the 1920\\'s . . . the film\\'s ending has a \" what was it all for ? \" feeling to it , but like the 1920\\'s , the trip there is a great deal of fun .',\n",
       " 'a worthy entry into a very difficult genre .',\n",
       " '[broomfield] uncovers a story powerful enough to leave the screen sizzling with intrigue .',\n",
       " \"eight crazy nights is a showcase for sandler's many talents .\",\n",
       " \"a sweet-natured reconsideration of one of san francisco's most vital , if least widely recognized , creative fountainheads .\",\n",
       " \"this is one of the most visually stunning and thematically moving epics in recent memory , and in spite of numerous minor flaws , scorsese's best in more than a decade .\",\n",
       " 'everywhere the camera looks there is something worth seeing .',\n",
       " 'a richly imagined and admirably mature work from a gifted director who definitely has something on his mind .',\n",
       " \"it's a nicely detailed world of pawns , bishops and kings , of wagers in dingy backrooms or pristine forests .\",\n",
       " \"a charming , quirky and leisurely paced scottish comedy -- except with an outrageous central gimmick that could have been a reject from monty python's meaning of life .\",\n",
       " 'it never fails to engage us .',\n",
       " \"its direction , its script , and weaver's performance as a vaguely discontented woman of substance make for a mildly entertaining 77 minutes , if that's what you're in the mood for .\",\n",
       " 'a charming romantic comedy that is by far the lightest dogme film and among the most enjoyable .',\n",
       " 'this is the kind of movie that used to be right at home at the saturday matinee , and it still is .',\n",
       " 'the spark of special anime magic here is unmistakable and hard to resist .',\n",
       " \"like its two predecessors , 1983's koyaanisqatsi and 1988's powaqqatsi , the cinematic collage naqoyqatsi could be the most navel-gazing film ever .\",\n",
       " \"baran isn't the most transporting or gripping film from iran -- or , indeed , by its director -- but it's a worthy companion to the many fine , focused films emerging from that most surprising of nations .\",\n",
       " 'the visuals alone make metropolis worth seeing .',\n",
       " 'dark , resonant , inventively detailed and packed with fleet turns of plot and a feast of visual amazement .',\n",
       " 'a picture that extols the virtues of comradeship and community in a spunky , spirited fashion .',\n",
       " 'a resonant tale of racism , revenge and retribution .',\n",
       " \"noyce's film is contemplative and mournfully reflective .\",\n",
       " 'here , adrian lyne comes as close to profundity as he is likely to get .',\n",
       " 'evokes a little of the fear that parents have for the possible futures of their children--and the sometimes bad choices mothers and fathers make in the interests of doing them good .',\n",
       " 'uno de los policiales más interesantes de los últimos tiempos .',\n",
       " 'rain is a small treasure , enveloping the viewer in a literal and spiritual torpor that is anything but cathartic .',\n",
       " 'an elegant , exquisitely modulated psychological thriller .',\n",
       " 'this concoction , so bizarre to the adult mind , is actually a charming triumph where its intended under-12 audience is concerned .',\n",
       " 'droll caper-comedy remake of \" big deal on madonna street \" that\\'s a sly , amusing , laugh-filled little gem in which the ultimate \" bellini \" begins to look like a \" real kaputschnik . \"',\n",
       " \"it's a beautifully accomplished lyrical meditation on a bunch of despondent and vulnerable characters living in the renown chelsea hotel . . .\",\n",
       " 'is it a total success ? no . is it something any true film addict will want to check out ? you bet .',\n",
       " 'zany , exuberantly irreverent animated space adventure .',\n",
       " 'dolgin and franco fashion a fascinating portrait of a vietnamese-born youngster who eagerly and easily assimilated as an all-american girl with a brand new name in southern tennessee .',\n",
       " 'the disarming cornball atmosphere has a way of infecting the entire crowd as the film rolls on .',\n",
       " 'a refreshingly honest and ultimately touching tale of the sort of people usually ignored in contemporary american film . search it out .',\n",
       " 'engrossing and affecting , if ultimately not quite satisfying .',\n",
       " 'the story , like life , refuses to be simple , and the result is a compelling slice of awkward emotions .',\n",
       " \"a sly game of cat and mouse that's intense and thrilling at times , but occasionally stretches believability to its limits and relies on predictable plot contrivances .\",\n",
       " 'funny and , at times , poignant , the film from director george hickenlooper all takes place in pasadena , \" a city where people still read . \"',\n",
       " \"this horror-comedy doesn't go for the usual obvious laughs at the expense of cheap-looking monsters -- unless you count elvira's hooters .\",\n",
       " \"the movie's eventual success should be credited to dennis quaid , in fighting trim shape as an athlete as well as an actor\",\n",
       " 'not a bad journey at all .',\n",
       " 'sits uneasily as a horror picture . . . but finds surprising depth in its look at the binds of a small family .',\n",
       " \"windtalkers blows this way and that , but there's no mistaking the filmmaker in the tall grass , true to himself .\",\n",
       " 'there is a refreshing absence of cynicism in stuart little 2--quite a rarity , even in the family film market . eventually , it wins you over .',\n",
       " 'noyce films it more as a shocking history lesson than as drama .',\n",
       " 'like a south-of-the-border melrose place .',\n",
       " 'those with an interest in new or singular sorts of film experiences will find what time is it there ? well worth the time .',\n",
       " 'a wildly funny prison caper .',\n",
       " \"huppert gives erika a persona that is so intriguing that you find yourself staring hypnotically at her , trying to understand her and wondering if she'll crack .\",\n",
       " \"despite what anyone believes about the goal of its makers , the show . . . represents a spectacular piece of theater , and there's no denying the talent of the creative forces behind it .\",\n",
       " \"you'll be left with the sensation of having just witnessed a great performance and , perhaps , give in to the urge to get on your feet and shake it .\",\n",
       " 'the actors are so terrific at conveying their young angst , we do indeed feel for them .',\n",
       " \"the reason this picture works better than its predecessors is that myers is no longer simply spoofing the mini-mod-madness of '60s spy movies .\",\n",
       " \"it is a kickass , dense sci-fi action thriller hybrid that delivers and then some . i haven't seen one in so long , no wonder i didn't recognize it at first .\",\n",
       " 'a compelling portrait of moral emptiness',\n",
       " 'in adobo , ethnicity is not just the spice , but at the heart of more universal concerns .',\n",
       " 'it is ridiculous , of course . . . but it is also refreshing , disarming , and just outright enjoyable despite its ridiculousness .',\n",
       " '. . . blade ii is more enjoyable than the original .',\n",
       " 'a film that takes you inside the rhythms of its subject : you experience it as you watch .',\n",
       " 'the movie exists for its soccer action and its fine acting .',\n",
       " 'the movie is saved from unbearable lightness by the simplicity of the storytelling and the authenticity of the performances .',\n",
       " 'the film starts out as competent but unremarkable . . . and gradually grows into something of considerable power .',\n",
       " 'nothing denis has made before , like beau travil and nenette et boni , could prepare us for this gory , perverted , sex-soaked riff on the cannibal genre .',\n",
       " 'reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich .',\n",
       " 'greene delivers a typically solid performance in a role that is a bit of a departure from the noble characters he has played in the past , and he is matched by schweig , who carries the film on his broad , handsome shoulders .',\n",
       " 'finds a way to tell a simple story , perhaps the simplest story of all , in a way that seems compelling and even original .',\n",
       " \"a stunning piece of visual poetry that will , hopefully , be remembered as one of the most important stories to be told in australia's film history .\",\n",
       " 'this is art paying homage to art .',\n",
       " '. . . a joke at once flaky and resonant , lightweight and bizarrely original .',\n",
       " 'invincible is a wonderful movie .',\n",
       " '. . . a cute and sometimes side-splittingly funny blend of legally blonde and drop dead gorgeous , starring piper perabo in what could be her breakthrough role .',\n",
       " 'dazzling and sugar-sweet , a blast of shallow magnificence that only sex , scandal , and a chorus line of dangerous damsels can deliver .',\n",
       " 'occasionally amateurishly made but a winsome cast and nice dialogue keeps it going .',\n",
       " \"japan's premier stylist of sex and blood hits audiences with what may be his most demented film to date .\",\n",
       " \"culkin , who's in virtually every scene , shines as a young man who uses sarcastic lies like a shield .\",\n",
       " 'cuts right through the b . s . giving a big middle-fingered \" shut up \" to those who talk up what is nothing more than two guys beating the hell outta one another .',\n",
       " 'the am-radio soundtrack and game cast -- tierney and the inimitable walken especially -- keep this unusual comedy from choking on its own conceit .',\n",
       " \". . . does such a fine job of engulfing you in its world and allying you with its characters' choices , good and ill , that its shortcomings are remembered only as an afterthought .\",\n",
       " 'marvelous , merry and , yes , melancholy film .',\n",
       " \"from spiritual rebirth to bruising defeat , vincent's odyssey resonates in a profound way , comparable to the classic films of jean renoir .\",\n",
       " 'novak manages to capture a cruelly hilarious vein of black comedy in the situation with his cast of non-actors and a gritty , no-budget approach .',\n",
       " 'insomnia is involving . still , i thought it could have been more .',\n",
       " \"there was time on that second round to see the subtleties of ramsay's portrait of grief .\",\n",
       " 'we can see the wheels turning , and we might resent it sometimes , but this is still a nice little picture , made by bright and friendly souls with a lot of good cheer .',\n",
       " 'a comprehensive and provocative film -- one that pushes the boundaries of biography , and challenges its audience .',\n",
       " 'the way coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining .',\n",
       " 'a modest masterpiece .',\n",
       " 'a worthwhile way to spend two hours .',\n",
       " \"francophiles will snicker knowingly and you'll want to slap them .\",\n",
       " 'sensitive , insightful and beautifully rendered film . one of the best of the year .',\n",
       " 'a love for films shines through each frame and the era is recreated with obvious affection , scored to perfection with some tasty boogaloo beats .',\n",
       " 'throwing caution to the wind with an invitation to the hedonist in us all , nair has constructed this motion picture in such a way that even the most cynical curmudgeon with find himself or herself smiling at one time or another .',\n",
       " \"makes an aborbing if arguable case for the man's greatness .\",\n",
       " 'an endlessly fascinating , landmark movie that is as bold as anything the cinema has seen in years .',\n",
       " '. . . a haunting vision , with images that seem more like disturbing hallucinations .',\n",
       " \"they crush each other under cars , throw each other out windows , electrocute and dismember their victims in full consciousness . and we don't avert our eyes for a moment .\",\n",
       " 'it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .',\n",
       " \"frailty isn't as gory or explicit . but in its child-centered , claustrophobic context , it can be just as frightening and disturbing -- even punishing .\",\n",
       " 'mixes likeable personalities , inventive photography and cutting , and wall-to-wall toe-tapping music to paint a picture of a subculture that is at once exhilarating , silly , perverse , hopeful and always fun .',\n",
       " 'the long-range appeal of \" minority report \" should transcend any awards it bags . this is one for the ages .',\n",
       " \"[a] superbly controlled , passionate adaptation of graham greene's 1955 novel .\",\n",
       " 'much monkeyfun for all .',\n",
       " \"an enchanting film that presents an audacious tour of the past and takes within its warm embrace the bounties of cultural artifacts inside st . petersburg's hermitage museum .\",\n",
       " \"[hawn's character]is so bluntly written , without a trace of sentimentality , and so blisteringly defined , that every other character seems overlooked and underwritten .\",\n",
       " 'the heightened symmetry of this new/old cinema paradiso makes the film a fuller experience , like an old friend haunted by the exigencies of time .',\n",
       " \"the powers team has fashioned a comedy with more laughs than many , no question . but this time there's some mold on the gold .\",\n",
       " 'while surprisingly sincere , this average little story is adorned with some awesome action photography and surfing .',\n",
       " 'it is far from the worst , thanks to the topical issues it raises , the performances of stewart and hardy , and that essential feature -- a decent full-on space battle .',\n",
       " 'a film that is a portrait of grace in an imperfect world .',\n",
       " 'a pleasurably jacked-up piece of action moviemaking .',\n",
       " 'nicolas philibert observes life inside a one-room schoolhouse in northern france in his documentary to be and to have , easily one of the best films of the year .',\n",
       " 'a perverse little truffle , dainty psychological terror on the outside with a creamy filling of familial jealousy and unrepentant domestic psychopathy .',\n",
       " 'this ecologically minded , wildlife friendly film teaches good ethics while entertaining with its unconventionally wacky but loving family',\n",
       " 'an enjoyably half-wit remake of the venerable italian comedy big deal on madonna street .',\n",
       " 'it takes this never-ending confusion and hatred , puts a human face on it , evokes shame among all who are party to it and even promotes understanding .',\n",
       " \"reign of fire may be little more than another platter of reheated aliens , but it's still pretty tasty .\",\n",
       " 'there are times when a rumor of angels plays like an extended episode of touched by an angel -- a little too much dancing , a few too many weeping scenes -- but i liked its heart and its spirit .',\n",
       " \"two hours of melodramatic musical married to two hours of underdog sports intrigue , if the picture also shares the weaknesses of both genres , more's the pity .\",\n",
       " 'this cheery , down-to-earth film is warm with the cozy feeling of relaxing around old friends .',\n",
       " 'thrilling , provocative and darkly funny , this timely sci-fi mystery works on so many different levels that it not only invites , it demands repeated viewings .',\n",
       " \"a tale of horror and revenge that is nearly perfect in its relentless descent to the depths of one man's tortured soul .\",\n",
       " \"an epic of grandeur and scale that's been decades gone from the popcorn pushing sound stages of hollywood .\",\n",
       " \"genuinely touching because it's realistic about all kinds of love .\",\n",
       " 'lauren ambrose comes alive under the attention from two strangers in town - with honest performances and realistic interaction between the characters , this is a coming-of-age story with a twist .',\n",
       " 'there has been much puzzlement among critics about what the election symbolizes . i believe the message is in the messenger : the agent is a woman .',\n",
       " 'an enjoyable film for the family , amusing and cute for both adults and kids .',\n",
       " '\" the mothman prophecies \" is a difficult film to shake from your conscience when night falls .',\n",
       " 'the second chapter of the harry potter series is even more magical than the first and simply the best family film of the year .',\n",
       " \"more honest about alzheimer's disease , i think , than iris .\",\n",
       " 'the acting alone is worth the price of admission .',\n",
       " 'an excellent romp that boasts both a heart and a mind .',\n",
       " \"interacting eyeball-to-eyeball and toe-to-toe , hopkins and norton are a winning combination -- but fiennes steals 'red dragon' right from under their noses .\",\n",
       " 'this is a terrific character study , a probe into the life of a complex man .',\n",
       " 'impresses you with its open-endedness and surprises .',\n",
       " \"this isn't a narrative film -- i don't know if it's possible to make a narrative film about september 11th , though i'm sure some will try -- but it's as close as anyone has dared to come .\",\n",
       " 'my oh my , is this an invigorating , electric movie .',\n",
       " 'the two leads chomp considerably more scenery with their acting than fire-breathing monsters barbecue with their breath . . .',\n",
       " 'cedar takes a very open-minded approach to this sensitive material , showing impressive control , both visually and in the writing .',\n",
       " 'never once predictable .',\n",
       " 'biggie and tupac is so single-mindedly daring , it puts far more polished documentaries to shame .',\n",
       " 'so many documentaries like this presuppose religious bigotry or zealous nuttiness of its antagonists , but family fundamentals displays a rare gift for unflinching impartiality .',\n",
       " 'the cast is uniformly excellent and relaxed .',\n",
       " \"after making several adaptations of other writers' work , armenian-canadian director atom egoyan broached an original treatment of a deeply personal subject .\",\n",
       " 'the film is painfully authentic , and the performances of the young players are utterly convincing .',\n",
       " \"if it seems like a minor miracle that its septuagenarian star is young enough to be the nonagenarian filmmaker's son , more incredible still are the clear-eyed boldness and quiet irony with which actor and director take on life's urgent questions .\",\n",
       " 'a candid and often fascinating documentary about a pentecostal church in dallas that assembles an elaborate haunted house each year to scare teenagers into attending services .',\n",
       " 'fans of the animated wildlife adventure show will be in warthog heaven ; others need not necessarily apply .',\n",
       " 'without resorting to hyperbole , i can state that kissing jessica stein may be the best same-sex romance i have seen .',\n",
       " 'nolan bravely treads where few american films dare to delve -- into the world of ambivalence and ambiguity . . .',\n",
       " \"unlike the nauseating fictions peddled by such 'have-yourself-a-happy-little-holocaust' movies as life is beautiful and jakob the liar , the grey zone is honest enough to deny the possibility of hope in auschwitz .\",\n",
       " 'a potent allegorical love story .',\n",
       " 'even those who would like to dismiss the film outright should find much to mull and debate .',\n",
       " 'this is cool , slick stuff , ready to quench the thirst of an audience that misses the summer blockbusters .',\n",
       " 'the movie is full of fine performances , led by josef bierbichler as brecht and monica bleibtreu as helene weigel , his wife .',\n",
       " 'a captivating cross-cultural comedy of manners .',\n",
       " 'andy garcia enjoys one of his richest roles in years and mick jagger gives his best movie performance since , well , performance .',\n",
       " \"the movie isn't always easy to look at . but if it is indeed a duty of art to reflect life , than leigh has created a masterful piece of artistry right here .\",\n",
       " \"it's [ricci's] best work yet , this girl-woman who sincerely believes she can thwart the world's misery with blind good will .\",\n",
       " 'highlights are the terrific performances by christopher plummer , as the prime villain , and nathan lane as vincent crummles , the eccentric theater company manager .',\n",
       " '[howard] so good as leon barlow . . . that he hardly seems to be acting .',\n",
       " 'an uplifting , near-masterpiece .',\n",
       " 'superior genre storytelling , which gets under our skin simply by crossing the nuclear line .',\n",
       " 'by taking entertainment tonight subject matter and giving it humor and poignancy , auto focus becomes both gut-bustingly funny and crushingly depressing .',\n",
       " \"it's a bittersweet and lyrical mix of elements .\",\n",
       " 'subversive , meditative , clinical and poetic , the piano teacher is a daring work of genius .',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8530"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4265"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_dataset['label'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
