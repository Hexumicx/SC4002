{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets\n",
    "# %pip install datasets\n",
    "# %pip install sklearn\n",
    "# %pip install numpy\n",
    "# %pip install torch==2.0.1\n",
    "# %pip install gensim\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 0: Dataset Preparation\n",
    "# Loading the Rotten Tomatoes movie review dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, val_data, test_data = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation function\n",
    "def prepare_data(data):\n",
    "    sentences = [item['text'] for item in data]\n",
    "    labels = [item['label'] for item in data]\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = prepare_data(train_data)\n",
    "val_sentences, val_labels = prepare_data(val_data)\n",
    "test_sentences, test_labels = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18951\n",
      "Number of OOV words: 3036\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Preparing Word Embeddings\n",
    "# Load pre-trained word vectors (e.g., GloVe or Word2Vec)\n",
    "embedding_dim = 100  # You can choose other dimensions if available\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Build vocabulary from training data\n",
    "def build_vocab(sentences, vectors):\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            vocab.add(word)\n",
    "    oov_words = [word for word in vocab if word not in vectors]\n",
    "    return vocab, oov_words\n",
    "\n",
    "vocab, oov_words = build_vocab(train_sentences, glove_vectors)\n",
    "\n",
    "# Q1(a) Answer: Size of vocabulary\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "# Q1(b) Answer: Number of OOV words\n",
    "print(\"Number of OOV words:\", len(oov_words))\n",
    "\n",
    "# Q1(c) Handling OOV words by using random embeddings for missing words\n",
    "def get_embedding_matrix(vocab, vectors, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    word_to_idx = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "        word_to_idx[word] = idx\n",
    "        if word in vectors:  # gensim uses `in` to check if a word exists in the vocabulary\n",
    "            embedding_matrix[idx] = vectors[word]\n",
    "        else:\n",
    "            # For OOV words, initialize random embeddings\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32), word_to_idx\n",
    "\n",
    "embedding_matrix, word_to_idx = get_embedding_matrix(vocab, glove_vectors, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 65.85%\n",
      "Epoch 2: Validation Accuracy = 63.41%\n",
      "Epoch 3: Validation Accuracy = 70.17%\n",
      "Epoch 4: Validation Accuracy = 72.42%\n",
      "Epoch 5: Validation Accuracy = 71.48%\n",
      "Epoch 6: Validation Accuracy = 72.70%\n",
      "Epoch 7: Validation Accuracy = 73.45%\n",
      "Epoch 8: Validation Accuracy = 73.92%\n",
      "Epoch 9: Validation Accuracy = 73.83%\n",
      "Epoch 10: Validation Accuracy = 74.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Part 2: Model Training & Evaluation (RNN Model)\n",
    "# Define a simple RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix, freeze=True\n",
    "        )\n",
    "        self.rnn = nn.RNN(embedding_dim, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 2)  # 2 output classes for sentiment\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        # Using mean pooling over the sequence for sentence representation\n",
    "        sentence_representation = torch.mean(output, dim=1)\n",
    "        return self.fc(sentence_representation)\n",
    "\n",
    "# Convert sentences to index sequences\n",
    "def sentence_to_idx(sentence, word_to_idx):\n",
    "    return [word_to_idx.get(word, 0) for word in sentence.split()]\n",
    "\n",
    "train_indices = [sentence_to_idx(sentence, word_to_idx) for sentence in train_sentences]\n",
    "val_indices = [sentence_to_idx(sentence, word_to_idx) for sentence in val_sentences]\n",
    "test_indices = [sentence_to_idx(sentence, word_to_idx) for sentence in test_sentences]\n",
    "\n",
    "# Convert data into PyTorch datasets with padding for consistent sequence length\n",
    "def pad_sequences(sequences, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    return [seq + [padding_value] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "# Pad and convert to tensors\n",
    "train_indices = pad_sequences(train_indices)\n",
    "val_indices = pad_sequences(val_indices, max_len=len(train_indices[0]))  # same max_len as train\n",
    "test_indices = pad_sequences(test_indices, max_len=len(train_indices[0]))\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_indices), torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(torch.tensor(val_indices), torch.tensor(val_labels))\n",
    "test_dataset = TensorDataset(torch.tensor(test_indices), torch.tensor(test_labels))\n",
    "\n",
    "# DataLoader Preparation\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model initialization, loss, and optimizer\n",
    "model = RNNModel(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the RNN model\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sentences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation accuracy after each epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for sentences, labels in val_loader:\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "                outputs = model(sentences)\n",
    "                preds = outputs.argmax(dim=1).cpu().tolist()  # Detach and move to CPU\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().tolist())  # Detach and move to CPU\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}: Validation Accuracy = {val_acc * 100:.2f}%\")\n",
    "        model.train()  # Set model back to training mode\n",
    "\n",
    "train(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Enhancement (Using BiLSTM, BiGRU, and CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BiLSTM model\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, 64, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(64 * 2, 2)  # 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, _) = self.bilstm(x)\n",
    "        sentence_representation = torch.mean(output, dim=1)\n",
    "        return self.fc(sentence_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.bigru = nn.GRU(embedding_dim, 64, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(64 * 2, 2)  # 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.bigru(x)\n",
    "        sentence_representation = torch.mean(output, dim=1)\n",
    "        return self.fc(sentence_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.conv1 = nn.Conv2d(1, 100, (3, embedding_dim))  # 3-gram filter\n",
    "        self.conv2 = nn.Conv2d(1, 100, (4, embedding_dim))  # 4-gram filter\n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embedding_dim))  # 5-gram filter\n",
    "        self.fc = nn.Linear(100 * 3, 2)  # 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # Add channel dimension\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x2 = torch.relu(self.conv2(x)).squeeze(3)\n",
    "        x3 = torch.relu(self.conv3(x)).squeeze(3)\n",
    "\n",
    "        # Apply max pooling over time\n",
    "        x1 = torch.max(x1, dim=2)[0]\n",
    "        x2 = torch.max(x2, dim=2)[0]\n",
    "        x3 = torch.max(x3, dim=2)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM Model\n",
      "Epoch 1: Validation Accuracy = 70.36%\n",
      "Epoch 2: Validation Accuracy = 73.64%\n",
      "Epoch 3: Validation Accuracy = 75.05%\n",
      "Epoch 4: Validation Accuracy = 76.74%\n",
      "Epoch 5: Validation Accuracy = 76.08%\n",
      "Epoch 6: Validation Accuracy = 77.49%\n",
      "Epoch 7: Validation Accuracy = 75.98%\n",
      "Epoch 8: Validation Accuracy = 75.14%\n",
      "Epoch 9: Validation Accuracy = 74.77%\n",
      "Epoch 10: Validation Accuracy = 74.39%\n",
      "Training biGRU Model\n",
      "Epoch 1: Validation Accuracy = 76.83%\n",
      "Epoch 2: Validation Accuracy = 76.55%\n",
      "Epoch 3: Validation Accuracy = 75.98%\n",
      "Epoch 4: Validation Accuracy = 75.70%\n",
      "Epoch 5: Validation Accuracy = 75.52%\n",
      "Epoch 6: Validation Accuracy = 75.42%\n",
      "Epoch 7: Validation Accuracy = 75.14%\n",
      "Epoch 8: Validation Accuracy = 75.42%\n",
      "Epoch 9: Validation Accuracy = 75.52%\n",
      "Epoch 10: Validation Accuracy = 75.52%\n",
      "Training CNN Model\n",
      "Epoch 1: Validation Accuracy = 75.33%\n",
      "Epoch 2: Validation Accuracy = 75.61%\n",
      "Epoch 3: Validation Accuracy = 75.14%\n",
      "Epoch 4: Validation Accuracy = 75.14%\n",
      "Epoch 5: Validation Accuracy = 75.05%\n",
      "Epoch 6: Validation Accuracy = 75.05%\n",
      "Epoch 7: Validation Accuracy = 75.42%\n",
      "Epoch 8: Validation Accuracy = 75.14%\n",
      "Epoch 9: Validation Accuracy = 75.23%\n",
      "Epoch 10: Validation Accuracy = 75.14%\n"
     ]
    }
   ],
   "source": [
    "# Training function for different models\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sentences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation accuracy after each epoch\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for sentences, labels in val_loader:\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "                outputs = model(sentences)\n",
    "                preds = outputs.argmax(dim=1).cpu().tolist()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}: Validation Accuracy = {val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "embedding_dim = 100  # Ensure this matches your embedding matrix dimensions\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)  # Ensure embedding matrix is a tensor\n",
    "bilstm_model = BiLSTMModel(embedding_matrix)\n",
    "bigru_model = BiGRUModel(embedding_matrix)\n",
    "cnn_model = CNNModel(embedding_matrix)\n",
    "\n",
    "# Prepare loss function and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bilstm = optim.Adam(bilstm_model.parameters(), lr=0.001)\n",
    "optimizer_bigru = optim.Adam(bigru_model.parameters(), lr=0.001)\n",
    "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate each model (example for BiLSTM)\n",
    "print(\"Training BiLSTM Model\")\n",
    "train_and_evaluate(bilstm_model, train_loader, val_loader, criterion, optimizer_bilstm)\n",
    "\n",
    "print(\"Training biGRU Model\")\n",
    "train_and_evaluate(bigru_model, train_loader, val_loader, criterion, optimizer_bigru)\n",
    "\n",
    "print(\"Training CNN Model\")\n",
    "train_and_evaluate(cnn_model, train_loader, val_loader, criterion, optimizer_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3(e)\n",
    "# BiLSTM with Attention Model\n",
    "class BiLSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(BiLSTMAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, 64, bidirectional=True, batch_first=True)\n",
    "        self.attention = nn.Linear(64 * 2, 1)  # Attention layer\n",
    "        self.fc = nn.Linear(64 * 2, 2)  # 2 output classes for sentiment\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_output, (hidden, _) = self.bilstm(x)  # lstm_output: [batch_size, seq_len, hidden_dim*2]\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)  # [batch_size, seq_len, 1]\n",
    "        weighted_output = torch.sum(lstm_output * attention_weights, dim=1)  # [batch_size, hidden_dim*2]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        return self.fc(weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan Wong\\AppData\\Local\\Temp\\ipykernel_31452\\2456702298.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding_matrix_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 76.92%\n",
      "Epoch 2: Validation Accuracy = 75.98%\n",
      "Epoch 3: Validation Accuracy = 76.64%\n",
      "Epoch 4: Validation Accuracy = 76.36%\n",
      "Epoch 5: Validation Accuracy = 75.98%\n",
      "Epoch 6: Validation Accuracy = 76.08%\n",
      "Epoch 7: Validation Accuracy = 76.08%\n",
      "Epoch 8: Validation Accuracy = 75.89%\n",
      "Epoch 9: Validation Accuracy = 75.89%\n",
      "Epoch 10: Validation Accuracy = 75.89%\n",
      "Epoch 11: Validation Accuracy = 75.98%\n",
      "Epoch 12: Validation Accuracy = 75.98%\n",
      "Epoch 13: Validation Accuracy = 75.89%\n",
      "Epoch 14: Validation Accuracy = 75.80%\n",
      "Epoch 15: Validation Accuracy = 75.42%\n",
      "Epoch 16: Validation Accuracy = 75.52%\n",
      "Epoch 17: Validation Accuracy = 75.61%\n",
      "Epoch 18: Validation Accuracy = 75.52%\n",
      "Epoch 19: Validation Accuracy = 75.52%\n",
      "Epoch 20: Validation Accuracy = 74.77%\n",
      "Epoch 21: Validation Accuracy = 75.52%\n",
      "Epoch 22: Validation Accuracy = 75.61%\n",
      "Epoch 23: Validation Accuracy = 75.33%\n",
      "Epoch 24: Validation Accuracy = 75.05%\n",
      "Epoch 25: Validation Accuracy = 75.23%\n",
      "Epoch 26: Validation Accuracy = 74.86%\n",
      "Epoch 27: Validation Accuracy = 75.42%\n",
      "Epoch 28: Validation Accuracy = 75.05%\n",
      "Epoch 29: Validation Accuracy = 74.95%\n",
      "Epoch 30: Validation Accuracy = 74.95%\n",
      "Epoch 31: Validation Accuracy = 74.67%\n",
      "Epoch 32: Validation Accuracy = 74.67%\n",
      "Epoch 33: Validation Accuracy = 74.58%\n",
      "Epoch 34: Validation Accuracy = 74.86%\n",
      "Epoch 35: Validation Accuracy = 74.39%\n",
      "Epoch 36: Validation Accuracy = 74.67%\n",
      "Epoch 37: Validation Accuracy = 74.67%\n",
      "Epoch 38: Validation Accuracy = 74.67%\n",
      "Epoch 39: Validation Accuracy = 74.39%\n",
      "Epoch 40: Validation Accuracy = 74.58%\n",
      "Epoch 41: Validation Accuracy = 74.67%\n",
      "Epoch 42: Validation Accuracy = 74.58%\n",
      "Epoch 43: Validation Accuracy = 74.39%\n",
      "Epoch 44: Validation Accuracy = 74.67%\n",
      "Epoch 45: Validation Accuracy = 74.39%\n",
      "Epoch 46: Validation Accuracy = 74.48%\n",
      "Epoch 47: Validation Accuracy = 74.30%\n",
      "Epoch 48: Validation Accuracy = 74.58%\n",
      "Epoch 49: Validation Accuracy = 74.48%\n",
      "Epoch 50: Validation Accuracy = 74.58%\n",
      "Epoch 51: Validation Accuracy = 74.67%\n",
      "Epoch 52: Validation Accuracy = 74.58%\n",
      "Epoch 53: Validation Accuracy = 74.58%\n",
      "Epoch 54: Validation Accuracy = 74.48%\n",
      "Epoch 55: Validation Accuracy = 74.20%\n",
      "Epoch 56: Validation Accuracy = 74.48%\n",
      "Epoch 57: Validation Accuracy = 74.48%\n",
      "Epoch 58: Validation Accuracy = 74.39%\n",
      "Epoch 59: Validation Accuracy = 74.39%\n",
      "Epoch 60: Validation Accuracy = 74.58%\n",
      "Epoch 61: Validation Accuracy = 74.48%\n",
      "Epoch 62: Validation Accuracy = 74.39%\n",
      "Epoch 63: Validation Accuracy = 74.30%\n",
      "Epoch 64: Validation Accuracy = 74.39%\n",
      "Epoch 65: Validation Accuracy = 74.48%\n",
      "Epoch 66: Validation Accuracy = 74.30%\n",
      "Epoch 67: Validation Accuracy = 74.48%\n",
      "Epoch 68: Validation Accuracy = 74.39%\n",
      "Epoch 69: Validation Accuracy = 74.39%\n",
      "Epoch 70: Validation Accuracy = 74.30%\n",
      "Epoch 71: Validation Accuracy = 74.30%\n",
      "Epoch 72: Validation Accuracy = 74.58%\n",
      "Epoch 73: Validation Accuracy = 74.58%\n",
      "Epoch 74: Validation Accuracy = 74.30%\n",
      "Epoch 75: Validation Accuracy = 74.30%\n",
      "Epoch 76: Validation Accuracy = 74.58%\n",
      "Epoch 77: Validation Accuracy = 74.39%\n",
      "Epoch 78: Validation Accuracy = 74.48%\n",
      "Epoch 79: Validation Accuracy = 74.48%\n",
      "Epoch 80: Validation Accuracy = 74.39%\n",
      "Epoch 81: Validation Accuracy = 74.20%\n",
      "Epoch 82: Validation Accuracy = 74.30%\n",
      "Epoch 83: Validation Accuracy = 74.39%\n",
      "Epoch 84: Validation Accuracy = 74.20%\n",
      "Epoch 85: Validation Accuracy = 74.39%\n",
      "Epoch 86: Validation Accuracy = 74.48%\n",
      "Epoch 87: Validation Accuracy = 74.48%\n",
      "Epoch 88: Validation Accuracy = 74.48%\n",
      "Epoch 89: Validation Accuracy = 74.39%\n",
      "Epoch 90: Validation Accuracy = 74.39%\n",
      "Epoch 91: Validation Accuracy = 74.58%\n",
      "Epoch 92: Validation Accuracy = 74.39%\n",
      "Epoch 93: Validation Accuracy = 74.58%\n",
      "Epoch 94: Validation Accuracy = 74.39%\n",
      "Epoch 95: Validation Accuracy = 74.58%\n",
      "Epoch 96: Validation Accuracy = 74.58%\n",
      "Epoch 97: Validation Accuracy = 74.58%\n",
      "Epoch 98: Validation Accuracy = 74.67%\n",
      "Epoch 99: Validation Accuracy = 74.58%\n",
      "Epoch 100: Validation Accuracy = 74.58%\n",
      "Test Accuracy with Final Improved Model = 76.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7664165103189493"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "attention_model = BiLSTMAttentionModel(embedding_matrix_tensor).to(device)\n",
    "\n",
    "# Prepare loss function and optimizer with specified configurations\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_attention = optim.Adagrad(attention_model.parameters(), lr=0.01)\n",
    "\n",
    "# Updated DataLoader with batch_size=64\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training and evaluation function with specified 100 epochs\n",
    "def train_and_evaluate_final_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=100):\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sentences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation after each epoch\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for sentences, labels in val_loader:\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "                outputs = model(sentences)\n",
    "                preds = outputs.argmax(dim=1).cpu().tolist()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}: Validation Accuracy = {val_acc * 100:.2f}%\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    model.eval()\n",
    "    all_test_preds, all_test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in test_loader:\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            outputs = model(sentences)\n",
    "            preds = outputs.argmax(dim=1).cpu().tolist()\n",
    "            all_test_preds.extend(preds)\n",
    "            all_test_labels.extend(labels.cpu().tolist())\n",
    "    test_acc = accuracy_score(all_test_labels, all_test_preds)\n",
    "    print(f\"Test Accuracy with Final Improved Model = {test_acc * 100:.2f}%\")\n",
    "    return test_acc\n",
    "\n",
    "# Train and evaluate the final model with the specified configurations\n",
    "train_and_evaluate_final_model(attention_model, train_loader, val_loader, test_loader, criterion, optimizer_attention, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
